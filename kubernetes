# Instructions to get gcloud/kubernetes to run in the hermelinkluntjes:thesis docker container in vscode
apt install -y curl
apt-get install -y apt-transport-https ca-certificates gnupg curl sudo

sudo curl -sSL https://sdk.cloud.google.com | bash

#gcloud
curl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-454.0.0-linux-x86_64.tar.gz
tar -xf google-cloud-cli-454.0.0-linux-x86_64.tar.gz
./google-cloud-sdk/install.sh
# New terminal
./google-cloud-sdk/bin/gcloud init

gcloud components install gke-gcloud-auth-plugin

pip install dask-kubernetes --upgrade 

#Kubernetes
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"
echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check
install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

-------------------------- Google K8 Cluster Management -----------------------------

gcloud container clusters create \
  --machine-type n1-standard-2 \
  --num-nodes 1 \
  --zone us-central1-a \
  --cluster-version latest \
  coolercluster

# Name of scheduler nodepool: default-pool
# Create additional nodeluster with different machine-type
gcloud container node-pools create workerpool \
  --machine-type n1-standard-8 \  
  --num-nodes 0 \  
  --cluster coolercluster \
  --zone=us-central1-a
  
kubectl create clusterrolebinding cluster-admin-binding \
--clusterrole=cluster-admin \
--user=adrianbeerka

# Shut down kubernetes cluster...
gcloud container node-pools list --cluster=coolercluster --zone=us-central1-a
gcloud container clusters resize coolercluster --node-pool default-pool --num-nodes 0 --zone=us-central1-a
gcloud container clusters resize coolercluster --node-pool workerpool --num-nodes 0 --zone=us-central1-a


# Scale node pool vertically
gcloud container node-pools update POOL_NAME \
    --cluster CLUSTER_NAME \
    --machine-type MACHINE_TYPE \
    --disk-type DISK_TYPE \
    --disk-size DISK_SIZE

Konfigurieren von unterschiedl. machine-types auf unterschiedl. Nodes,
2 Nodes -> 1 für worker-machine und 1 node für scheduler-machine 

Von https://www.inovex.de/de/blog/data-processing-dask-rapids-kubernetes-cluster-gpu-support-13/:
gcloud container clusters create gpu-support
--accelerator type=nvidia-tesla-t4,count=2
--cluster-version 1.15.11-gke.1
--machine-type n1-highmem-4
--num-nodes 2
--zone europe-west4-b

gcloud container clusters list

gcloud container clusters delete coolercluster --zone us-central1-a

-------------------------- Helm -------------------------------------
helm repo add dask https://helm.dask.org/
helm repo update
helm install --create-namespace -n dask-operator --generate-name dask/dask-kubernetes-operator

helm list
helm delete <>

***HelmCluster is a fixed cluster option, KubeCluster is an ephemeral one.
  
It is not possible to use HelmCluster from the Jupyter session which is deployed as part of the Helm Chart without first copying your ~/.kube/config file to that Jupyter session.

Dask and Kubernetes:
- Workers connect to the scheduler via the scheduler Service and that service can also be exposed to the user in order to connect clients and perform work.
  
  
------------- .YAML File configuration -------------------
1. kubectl apply -f - <<EOF
apiVersion: kubernetes.dask.org/v1
kind: DaskCluster
metadata:
  name: my-dask-cluster
spec:
  ...
2. cluster = KubeCluster(name="my-dask-cluster", image='ghcr.io/dask/dask:latest')

is the same as:
cluster = KubeCluster(custom_cluster_spec="cluster.yml")


------------- KubeCluster ------------------------
*** KubeCluster isn't designed to interface directly with gcloud. It is designed to be run from a pod on a Kubernetes cluster that has permissions to launch other pods.

kubectl apply -f cluster.yaml

kubectl get daskclusters

kubectl get pods -A -l app.kubernetes.io/name=dask-kubernetes-operator

kubectl get svc -l dask.org/cluster-name=simple
kubectl port-forward svc/simple 8786:8786

from dask.distributed import Client
client = Client("localhost:8786")
print(client)

cluster.add_worker_group(name="highmem", n_workers=2, resources={"requests": {"memory": "2Gi"}, "limits": {"memory": "64Gi"}})

cluster.scale(5)
cluster.scale(5, worker_group="highmem")

cluster.delete_worker_group(name="highmem")

kubectl get po -l dask.org/cluster-name=simple


kubectl delete -f cluster.yaml
kubectl delete daskcluster simple


STEPS:
1. Spin up simple kubernetes cluster with GKE
2. Give GKE cluster account permissions
3. Install Helm chart on kubernetes cluster
4. Use external IP address of jupyter service for notebook.





