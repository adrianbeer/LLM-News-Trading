\documentclass[12pt,a4paper]{article}

\usepackage{titling}
\usepackage[english]{babel}
\usepackage[round]{natbib}
\usepackage{amsmath}
%\usepackage{parskip}

\title{Market Efficiency of News Events}
\author{Adrian Beer}
\date{\vspace{-10ex}}
\setlength{\droptitle}{-10em}

\begin{document}
	\maketitle
	\tableofcontents 
	\section{Introduction}	
	
	One possible explanation is that "low-capitalisation, young, unprofitable, low-dividend-paying, high-volatility and highgrowth companies are difficult to arbitrage or value according to traditional financial theory and are therefore very sensitive to investor sentiment".
	
	
	\subsection{Categories}
	\cite{peng_leverage_2015}:
	Bag of Keywords, Polarity Score, Category Tagging...
	Using initial seed words for each category.
	Categories: new-product, acquisition, pricerise, price-drop, law-suit, fiscal-report, investment,
	bankrupt, government, analyst-highlights.
	
	\subsubsection{Labeling}
	\begin{itemize}
		\item{Next Days Close}
		\item{Next Close/Open}
		\item{Trailing Stop-Loss}
	\end{itemize}
	
	\section{Related Work}
	"Following the work of Luss and dâ€™Aspremont (2015), Ding et al. (2015), Xu and Cohen (2018), Ke et al. (2019), we formulate the stock movement prediction as a binary classification task"
	
	\textbf{\cite{salbrechter_financial_2021-1}} investigates the impact of financial news on the daily returns of S\&P 500 constituents using a self-fine-tuned BERT model. 
	Their algorithm produce the CLS Token from FinNewsBert and a Topic vector from Text2Topic and feed these into a standard feed forward NN, which then produces probabilities for the 3 classes.
	They find that the news is priced in within one day, but not instantaneously.
	Their FinNewsBert model is retrained every two years in order to obtain a large time-frame for their out-of-sample study.
	They consider the problem of look-ahead-bias of the base BERT model, due to the data that it is trained on.
	They use a dataset of financial news published by Refinitiv from 1996 to 2020 and from Refinitive Datastream daily price data of 1330 companies, which were at some point in time S\&P consituents.
	As part of the preprocessing they convert news articles to lower case, remove all numbers, punctuation marks and brackets, so that only letters remain.
	Multiple news articles about one company published in quick succession are merged into a single document.
	(Q: How is this handled coupled with the token limitations of BERT?)
	An algorithms is used to discern fresh ans stale news.
	The labels for the training of FinNewsBert are constructed based on the idiosynchratic price movement following the news,
	namely on the following formula: 
	$$I R_{i, t}=R_{i, t}-R_{f, t}-\beta_{i, t} *\left(R_{S \& P 500, t}-R_{f, t}\right)$$
	Afterwards z-scores are calculated for each stock, in order to avoid overweighting price movements in smaller, more volatile stocks.
	$$z_{i, t}=\frac{I R_{i, t}-\mu_{i, t}}{\sigma_{i, t}}$$
	They find improved classification performance, with the topic vector being used as input.
	With the Text2Vec algorithm they differentiate between analyst forecasts, earnings reports, monetary policy and business/strategic.
	
	
	\bigskip
	\textbf{\cite{ke_predicting_2020}} propose a text-mining methodology, named SESTM, to extract sentiment information from text. 
	They used a bag-of-words representation of news articles.
	To distinguish fresh from stale news a measure of novelty was constructed based on the cosine similarity of an articles with all other articles about the same firm five trading days prior.
	$$
	\text { Novelty }_{i, t}=1-\max _{j \in \chi_{i, t}}\left(\frac{d_{i, t} \cdot d_j}{\left\|d_{i, t}\right\|\left\|d_j\right\|}\right)
	$$
	They find that price responses to news are larger for fresh news. 
	They also found larger price responses for smaller and also for more volatile stocks.
	Price movements for more volatile stocks last up to three days on average, while news about low volatility stocks are incorporated after just one day of trading.
	
	
	\bigskip
	\textbf{\cite{liu_intraday_2023}} analyze the predictability of stock returns for different time horizons, ranging from 1-min to 30-min, and market sectors between 2005 and 2016.
	They find high profitability even after transaction costs.
	Intraday predictability decreased with higher time horizons
	
	
	\bigskip
	These results are consistent with the observations of Baker (2007b), who observes that low-capitalisation, young, unprofitable, low-dividend-paying, high-volatility and highgrowth companies are difficult to arbitrage or value according to traditional financial theory and are therefore very sensitive to investor sentiment.
	
	
	\section{Methodology}
	Because pre-news price movements have shown to correlate with the sentiment of the news, we incorporate this information
	into our feature set \cite{ke_predicting_2020}.
	
	\section{Labeling}
	Adjusting returns to the market is important.
	\cite{salbrechter_financial_2021-1}:
	$$
	I R_{i, t}=R_{i, t}-R_{f, t}-\beta_{i, t} *\left(R_{S \& P 500, t}-R_{f, t}\right)
	$$
	\cite{chen_stock_2021}:
	$$
	r_{s, t}=\frac{P_{s, t+\Delta t}}{P_{s, t}}-\frac{P_{m, t+\Delta t}}{P_{m, t}}
	$$
	
	
	Since news can take from 1 day up to 3 days to be fully incorporated in the stock price, ideally we set our label accordingly.
	However the longer this time horizon, the higher the probability of having a second news event in this period.
	How big is this problem??
	
	\bibliographystyle{plainnat}
	\bibliography{References}
	
	
\end{document}




