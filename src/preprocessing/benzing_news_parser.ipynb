{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkovyr-oc4-K"
   },
   "source": [
    "# Benzinga-Nachrichten-Verarbeitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2A8GOctQmMfj"
   },
   "source": [
    "## Setup up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxfEWak8MNGE"
   },
   "source": [
    "### Cluster spin up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8pmVviAjMTaQ"
   },
   "outputs": [],
   "source": [
    "use_colab = False\n",
    "if use_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    cwd=\"/content/drive/MyDrive/NewsTrading/trading_bot\"\n",
    "    %cd /content/drive/MyDrive/NewsTrading/trading_bot\n",
    "    %pip install -r requirements_clean.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gxfs_work/cau/sunms534/trading_bot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gxfs_work/cau/sunms534/.conda/envs/my_pytorch_env/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /gxfs_work/cau/sunms534/trading_bot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "zj0QodzRMNGE",
    "outputId": "88876701-df20-40bc-aab1-3dec09411b9a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from src.config import config \n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.WARNING)\n",
    "\n",
    "from src.utils.dataframes import parallelize_dataframe, block_apply_factory\n",
    "from src.preprocessing.news_parser import infer_author, filter_body, body_formatter\n",
    "\n",
    "pd.set_option(\n",
    "    'display.max_colwidth', 2000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/SlurmTMP/sunms534.10454313/ipykernel_1091601/2219788147.py:1: DtypeWarning: Columns (3,5,6,7,8,9,10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  news_csv = pd.read_csv(\"data/news/nasdaq_exteral_data.csv\")\n"
     ]
    }
   ],
   "source": [
    "news_csv = pd.read_csv(\"data/news/nasdaq_exteral_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_csv.rename(columns={\n",
    "    \"Date\": \"time\",\n",
    "    \"Stock_symbol\": \"stocks\",\n",
    "    \"Author\": \"author\",\n",
    "    \"Article_title\": \"title\",\n",
    "    \"Article\":\"body\"\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_0 = news_csv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_csv = news_csv.loc[:, ['time', 'stocks', 'author', 'title', 'body']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many time stamps don't contain intra-day information. Hence we need to set the intra-day time of these entries to the end of the day and mark them as special rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_csv.dropna(subset=['time', 'stocks', 'body'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_1 = news_csv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 13057521 entries due to NaNs.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dropped {N_0[0] - N_1[0]} entries due to NaNs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_csv['time'] = pd.to_datetime(news_csv['time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRqa0EPy_lpx"
   },
   "source": [
    "## Grobes HTML-Parsing\n",
    "Als erstes müssen wir die HTML-Dokumente zu normalem Text umwandeln, ansonsten sind die Text-Zellen zu groß und führen zu Problemen mit PyArrow/Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOM after starting second loop... something isn't being properly being garbage collected... maybe the child processes of multiprocessing in parallelize_dataframe?\n",
    "%%time\n",
    "for year in range(2023, 2024):\n",
    "    print(f\"{year}\")\n",
    "    df = pd.read_parquet(config.data.benzinga.raw + f\"/story_df_raw_{year}.parquet\")\n",
    "    df[\"html_body\"] = parallelize_dataframe(df[\"html_body\"], block_apply_factory(body_formatter), n_cores=os.cpu_count())\n",
    "    df = df.rename(columns={\"html_body\":\"body\"})\n",
    "    df.to_parquet(config.data.benzinga.raw_html_parsed + f\"/story_df_html_parsed{year}.parquet\")\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFnIFQW2Nag7"
   },
   "source": [
    "## Author-Inferenz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHFpynyiq3O4"
   },
   "outputs": [],
   "source": [
    "ddf = pd.read_parquet(config.data.benzinga.raw_html_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.memory_usage(deep=True).sum() / 1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ylwarwpXLegE"
   },
   "outputs": [],
   "source": [
    "# Remove rows for which noo stock ticker is recorded\n",
    "ddf = ddf[ddf.stocks != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eu1EhTqrVpBG"
   },
   "source": [
    "Untersuche als nächstes die Behauptung, dass **PRNewswire** und **Businesswire** den gesamten Markt für Pressemeldungen in den USA kontrollieren. Wenn dem so ist, und sie nicht noch weitere, unwichtige Meldungen veröffentlichen, dann können wir einfach die Newsartikel nach diesen Autoren filtern und uns viel Arbeit ersparen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5mO091MfO6e"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf[\"inferred_author\"] = None\n",
    "ddf[\"inferred_author\"] = ddf.body.progress_apply(infer_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTcPjnU0VZZu"
   },
   "outputs": [],
   "source": [
    "# value_counts for authors\n",
    "auhtor_value_counts = pd.concat([ddf.author.value_counts().head(10), ddf.inferred_author.value_counts().head(10)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNesN9jX1IP0"
   },
   "outputs": [],
   "source": [
    "auhtor_value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKT6tusBp13y"
   },
   "outputs": [],
   "source": [
    "auhtor_value_counts.sum().diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wk6p6woeqJqP"
   },
   "source": [
    "Ungefähr 650k Nachrichten werden ausgelassen, wenn nur die vier Hauptvertreiber von Pressemeldungen berücksichtigt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8X8rHo-CsDCQ"
   },
   "outputs": [],
   "source": [
    "ddf = ddf[~ddf.inferred_author.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHvzym8sFcX_"
   },
   "outputs": [],
   "source": [
    "ddf[\"inferred_author\"] = ddf[\"inferred_author\"].astype(\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsEmViTzNvgs"
   },
   "outputs": [],
   "source": [
    "ddf.inferred_author.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfNNyD_31sly"
   },
   "outputs": [],
   "source": [
    "ddf.inferred_author.value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pmMI9DsEhrn2"
   },
   "outputs": [],
   "source": [
    "ddf = ddf.drop(columns=[\"author\"]).rename(columns={\"inferred_author\":\"author\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLdnwdyxNFiH"
   },
   "outputs": [],
   "source": [
    "# Legacy code... channels only present in benzinga news and only after 2017 aswell\n",
    "# Contains 100k rows\n",
    "# earnings_ddf = ddf[ddf.channels.apply(lambda x: \"Earnings\" in x)]\n",
    "# value counts for authors of earnings reports (contrast to value counts of all news articles)\n",
    "# earnings_ddf.author.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-zs5SSiVbyp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dVD6j-G9nie"
   },
   "source": [
    "Hier sehen wir, dass es keine einzige Pressemeldung von **Business Wire** gibt, die mit *Earnings* gekennzeichnet sind. Trotzdem gibt es relevante *Earnings* reports von Business Wire. Dies habe ich kurz verifiziert..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jafqZ87tXDKn"
   },
   "source": [
    "Wie viele Nachrichten bleiben, wenn wir auf relevante Ticker filtern? Wir wollen nicht(!) - so ist es momentan - auf die momentane Russell 3k-Zusammensetzung filtern, denn wir wollen auch ungelistete bzw. ehemalige Russell-Aktien beachten.\n",
    "\n",
    "\n",
    "**1. Full-Name-Discovery:**\n",
    "\n",
    "Herausfinden des vollen Namens des Unternehmens für jeden Ticker, damit 1. der Text richtig geparst werden kann und 2. damit wir einen Anhaltspunkt für das Ticker-Grouping haben.\n",
    "\n",
    "\n",
    "**2. Ticker-Filtering:**\n",
    "\n",
    "Alle Ticker herausfiltern, die wir nicht brauchen. Wenn wir aber ein großes Aktienuniversum (mit inzwischen ungelisteten Aktien) benutzen, werden wir fast alle Nachrichten behalten können. Allerdings lassen sich so Fehlerhafte Nachrichten/Ticker etc. herausfiltern.\n",
    "\n",
    "\n",
    "**3. Ticker-Grouping:**\n",
    "\n",
    "Was machen wir, wenn wir mehrerer Aktiengattungen für ein Unternehmen haben? Z.B. Vorzugs- und Stammaktien. Wir können i.A. die Stammaktie nehmen, da diese normalerweise ein höheres Handelsvolumen aufweist. D.h. wir bilden alle Ticker der Benzinga-Nachrichten auf den Ticker der Stammaktie ab.\n",
    "\n",
    "\n",
    "**4. Firmennamen-Nachrichtenkörper-Verifikation:**\n",
    "\n",
    "Da Ticker wiederverwendet werden können bzw. sich verändern können wollen wir sicherstellen, dass der Unternehmensname im Nachrichtenkörper vorkommt. Bzw. generell ist das eine gute Datensäuberungs-Maßnahme. Einerseits verhindern wir,\n",
    "dass später Aktienkurse einer falschen Aktie zugeordnet wird. Andererseits werden dadurch evtl. auch weniger seriöse Nachrichten herausgefiltert, die nicht\n",
    "die Kontaktadresszeile des Unternehmens am Ende besitzen, in dem der vollständige Unternehmensname vorkommt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = pd.read_parquet(config.data.shared.ticker_name_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70POMUC-PN-n"
   },
   "outputs": [],
   "source": [
    "ddf = ddf[ddf.stocks.isin(mapper.index.to_list())]\n",
    "ddf[\"company_name\"] = ddf.stocks.progress_map(lambda x: mapper.company_names.loc[x]).astype(str)\n",
    "ddf[\"short_name\"] = ddf.stocks.progress_map(lambda x: mapper.short_name.loc[x]).astype(str)\n",
    "print(f\"Es verbleiben {ddf.shape[0]} Nachrichten, für die wir den Ticker zu einem Firmennamen aufgelösen konnten.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.company_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Xw1JSiI_0JH"
   },
   "source": [
    "### Duplikate Entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cEzrJqEN_yuJ"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "samples_before = ddf.shape[0]\n",
    "ddf = ddf.drop_duplicates()\n",
    "samples_after = ddf.shape[0]\n",
    "print(f\"{samples_before=}, {samples_after=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3F3dOc35YN8k"
   },
   "source": [
    "### Firmennamen-Nachrichtenkörper-Verifikation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_company_name_in_body(df):\n",
    "    return df.apply(lambda x: bool(re.search(x[\"short_name\"],\n",
    "                                             x.title + x[\"body\"].replace(\"( )*\\n( )*\", \" \"),\n",
    "                                             re.IGNORECASE)),\n",
    "                    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mask = parallelize_dataframe(ddf, verify_company_name_in_body, n_cores=os.cpu_count())\n",
    "\n",
    "print(f\"Around {len(ddf.stocks.unique())} stocks before filtering and {len(ddf[mask].stocks.unique())} after\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNAFvvVmmWm2"
   },
   "outputs": [],
   "source": [
    "# Filter out faulty news\n",
    "ddf = ddf[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bshj1secWQ3P"
   },
   "source": [
    "Bis zu 5k Nachrichten pro Firma, z.B. AT&T, was in 13 Jahren ca. einer Nachricht pro Tag entspricht. Wir wollen nicht das eine Firma mit vielen Junk-Nachrichten das Modell dominiert. Wobei das Modell hoffentlich dann auch die Junk-Nachrichten als solche erkennt und ignoriert. Eher wichtig noch einen `staleness`-Faktor, also wie ähnlich die Nachricht zu Vorhergegangenen ist (i.e. Nachrichten desselben Tages oder derselben Woche).\n",
    "\n",
    "Kategorisieren von Nachrichten (mit Text2Topic, wie Salbrechter?) und eliminieren von Business/Strategic etc.\n",
    "Im Falle von Text2Topic, versuche Estimates des Unternehmens von Dritten zu unterscheiden.\n",
    "\n",
    "Wichtig!!! Unterscheide zwischen LERN-Phase und PRODUKTIONS-Phase.\n",
    "Wir können z.B. CLS-Token in der Produktions-Phase vergleichen, in der Lern-Phase aber noch nicht.\n",
    "\n",
    "Text2Vec -> Business category evtl. entfernen-> Intrastock variance average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make reduced ticker name mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dK5naDT8t1Ap"
   },
   "outputs": [],
   "source": [
    "ticker_name_mapper = pd.read_parquet(config.data.shared.ticker_name_mapper)\n",
    "\n",
    "ticker_name_mapper_reduced = ddf[[\"stocks\", \"company_name\", \"short_name\"]].drop_duplicates(keep=\"first\")\n",
    "\n",
    "ticker_name_mapper_reduced.to_parquet(config.data.shared.ticker_name_mapper_reduced)\n",
    "\n",
    "print(f\"From {ticker_name_mapper.shape[0]} to {ticker_name_mapper_reduced.shape[0]} tickers (reduced)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRABrPVOcaa9"
   },
   "source": [
    "## Parsing News Bodies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qubEijBxMNGt"
   },
   "outputs": [],
   "source": [
    "from src.utils.time import convert_timezone\n",
    "ddf[\"time\"] = ddf[\"time\"].progress_map(lambda x: convert_timezone(pd.to_datetime(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ddf#.iloc[:10000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_apply_factory(func, axis=None):\n",
    "    global _f\n",
    "    def _f(s):\n",
    "        # s can be series or dataframe\n",
    "        print(' ', end='', flush=True)\n",
    "        ret = s.progress_apply(func, axis=axis) if axis else s.progress_apply(func)\n",
    "        return ret\n",
    "    return _f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KGlOJPEvFxuP",
    "outputId": "2e7b693a-72c0-46d8-e93c-11f6feb29d09"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf[\"parsed_body\"] = parallelize_dataframe(sample, block_apply_factory(filter_body, axis=1), n_cores=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.to_parquet(config.data.benzinga.cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.company_name.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[[\"body\", \"parsed_body\"]].tail(10).iloc[0:1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[[\"body\", \"parsed_body\"]].tail(10).iloc[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ihzi516YMNGv"
   },
   "source": [
    "## Filtern von Newstiteln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdbc2yRqMNGv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1DIhhqZMNGv"
   },
   "source": [
    "## Voranstellen von gefilterten Newstiteln an Nachrichtenkörper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFEujkJ-MNGw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaK5pLz4mRHF"
   },
   "source": [
    "## Durschnittlichen Tokenlänge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timedeltas zwischen Nachrichtenmeldungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = pd.read_parquet(config.data.benzinga.cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.sort_values(\"time\")\n",
    "tmp = ddf[[\"time\", \"stocks\"]]\n",
    "#### Adding timedeltas to the data frame\n",
    "news_timedeltas = tmp.groupby(\"stocks\").transform(lambda x: x.diff())\n",
    "ddf.loc[:, \"timedelta\"] = news_timedeltas.time.fillna(pd.Timedelta(days=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_timedeltas = ddf.timedelta\n",
    "news_timedeltas.iloc[0].components\n",
    "timedelta_in_minutes = news_timedeltas.apply(lambda x: x.total_seconds() / 60)\n",
    "px.histogram(timedelta_in_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VG5rnSiNpWVN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mc6lpRCpfVT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
