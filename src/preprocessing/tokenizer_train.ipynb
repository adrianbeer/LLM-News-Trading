{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ccf648-8779-4ea5-a446-1d9bb64fc8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gxfs_work/cau/sunms534/trading_bot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gxfs_work/cau/sunms534/.conda/envs/my_pytorch_env/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /gxfs_work/cau/sunms534/trading_bot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afd9a633-a7ff-4d2c-b608-ff34d8a2013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from tokenizers.processors import RobertaProcessing\n",
    "from src.config import config\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6041493e-9240-46b4-a05f-ac45d825a4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies = pd.read_parquet(config.data.news.stripped, columns=[\"parsed_body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26bdec46-ca16-4fa8-85ea-07a6b76fad6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
    "\n",
    "tokenizer.train_from_iterator(bodies.parsed_body.values, \n",
    "                              vocab_size=30000, \n",
    "                              min_frequency=10, \n",
    "                              special_tokens=[\n",
    "                                \"<s>\",\n",
    "                                \"<pad>\",\n",
    "                                \"</s>\",\n",
    "                                \"<unk>\",\n",
    "                                \"<mask>\",\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd1b4a4f-9851-474c-aea7-ea705f4c08b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = RobertaProcessing(\n",
    "    cls=(\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "    sep=(\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b145a2a1-d2a8-4780-a6c6-eb6b58690969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/models/newstokenizer/tokenizer_config.json',\n",
       " 'data/models/newstokenizer/special_tokens_map.json',\n",
       " 'data/models/newstokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save files to disk\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer._tokenizer, pad_token=\"<pad>\", truncation=True)\n",
    "tokenizer.save_pretrained(\"data/models/newstokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f7303d2-942c-4e55-8e32-2a0737c997b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"data/models/newstokenizer\", max_len=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "899044d8-89e5-425c-9e90-591b28c8fb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = bodies.iloc[10].parsed_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "707c52e2-26f0-4350-9d2a-bf93519bd695",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(\n",
    "    text, \n",
    "    add_special_tokens = True, \n",
    "    truncation = True, \n",
    "    padding = \"max_length\", \n",
    "    max_length = 256,\n",
    "    return_attention_mask = True, \n",
    "    return_tensors = \"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4165cca0-2246-4e60-b4c0-3572e306b584",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mencoding\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'tokens'"
     ]
    }
   ],
   "source": [
    "encoding[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5fcb4f14-f938-4535-8d27-a6878b322b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48ac2a18-bc1c-4217-a21c-dc66e312f802",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_inputs_ids = pd.read_parquet(config.data.news.input_ids)\n",
    "masks = pd.read_parquet(config.data.news.masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "31b1c9ea-e251-4b33-8ed3-6d92719068ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaModel, RobertaForMaskedLM, RobertaTokenizerFast\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "configuration = RobertaConfig(vocab_size = 30000,\n",
    "                              hidden_size = 256,\n",
    "                              num_hidden_layers = 6,\n",
    "                              num_attention_heads = 4,\n",
    "                              intermediate_size = 1556,\n",
    "                              hidden_act = 'gelu',\n",
    "                              hidden_dropout_prob = 0.1,\n",
    "                              attention_probs_dropout_prob = 0.1,\n",
    "                              max_position_embeddings = 258,\n",
    "                              type_vocab_size = 2,\n",
    "                              initializer_range = 0.02,\n",
    "                              layer_norm_eps = 1e-12,\n",
    "                              pad_token_id = 1,\n",
    "                              bos_token_id = 0,\n",
    "                              eos_token_id = 2,\n",
    "                              position_embedding_type = 'absolute',\n",
    "                              use_cache = True,\n",
    "                              classifier_dropout = None)\n",
    "\n",
    "\n",
    "model = RobertaModel(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6e9dd0f9-88b5-443d-a063-f74c832bf7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0\n",
       "1      14924\n",
       "2        262\n",
       "3       1411\n",
       "4        293\n",
       "       ...  \n",
       "251      469\n",
       "252     1831\n",
       "253      629\n",
       "254     3585\n",
       "255        2\n",
       "Name: 0, Length: 256, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_inputs_ids.iloc[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "503933f3-95d2-427a-9aeb-4dc1099d1efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,   91, 5641, ..., 2529,  267,    2],\n",
       "       [   0,  267,  321, ..., 4712,  335,    2],\n",
       "       [   0,  583,  302, ..., 7526,  290,    2],\n",
       "       [   0,   69, 6811, ...,  716,  353,    2]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_inputs_ids.iloc[1:5, :].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8b1fc6c3-5fe0-4040-a78a-44c66a5b8e5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2394, -2.3523,  0.0701,  ...,  1.2033, -0.5012, -0.6043],\n",
       "         [ 0.4904, -0.6112,  0.6721,  ..., -0.0792,  0.1891,  0.0141],\n",
       "         [ 0.2500, -1.8871, -0.2477,  ...,  0.4277,  0.1697,  0.6306],\n",
       "         ...,\n",
       "         [ 1.0402,  0.0129,  0.4128,  ..., -0.3565,  0.5749, -0.9676],\n",
       "         [-0.7853, -0.2002,  0.4063,  ..., -1.2100,  0.4986, -1.1683],\n",
       "         [ 1.0320, -0.8628,  0.8338,  ...,  0.7949,  1.2175, -0.4417]],\n",
       "\n",
       "        [[ 1.4046, -2.3721,  0.3859,  ...,  1.4811, -0.1087, -1.2942],\n",
       "         [-1.5018, -0.3980,  0.1484,  ...,  0.5272, -0.5977, -1.1767],\n",
       "         [ 0.5308, -1.5011,  0.9497,  ...,  0.8222,  1.5338, -0.5177],\n",
       "         ...,\n",
       "         [ 1.8587, -0.0864, -2.1232,  ...,  0.2590, -0.3208, -1.0196],\n",
       "         [ 0.9724, -0.8159,  0.9147,  ..., -0.7498,  0.2683, -0.8722],\n",
       "         [ 1.0463, -0.2195,  0.7432,  ...,  0.6326,  1.1605, -0.5176]],\n",
       "\n",
       "        [[ 1.4023, -2.3850,  0.1718,  ...,  1.2643, -0.4087, -1.5303],\n",
       "         [-0.0243, -0.9546,  0.7980,  ...,  0.3408, -0.3057, -0.2547],\n",
       "         [ 0.6370, -2.2284,  0.7274,  ...,  0.4297,  1.5217, -0.4366],\n",
       "         ...,\n",
       "         [ 1.8132, -1.5455, -0.7896,  ...,  0.9435,  0.3999, -0.4188],\n",
       "         [-0.8256, -1.9106,  0.1009,  ..., -0.5441,  0.2987, -0.8148],\n",
       "         [ 1.0936, -1.2644,  0.4550,  ...,  0.5463,  1.3178, -0.6982]],\n",
       "\n",
       "        [[ 1.3965, -2.5496,  0.8033,  ...,  0.8959, -0.1192, -1.5426],\n",
       "         [-0.2559, -1.9329,  0.2538,  ...,  0.3336, -1.5858, -0.1164],\n",
       "         [-0.0404, -0.8396,  0.4704,  ...,  0.3091,  1.4166, -1.0587],\n",
       "         ...,\n",
       "         [ 2.1516,  0.0774, -1.7228,  ..., -0.3014,  0.9856, -0.0312],\n",
       "         [ 0.1470, -1.3194,  1.1773,  ..., -0.3688, -0.2829,  0.9909],\n",
       "         [ 1.1886, -1.4879,  0.6523,  ...,  0.7458,  1.2785, -0.5999]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.3087,  0.2591, -0.0263,  ..., -0.3007, -0.6516, -0.5680],\n",
       "        [ 0.3487,  0.0355,  0.0664,  ..., -0.2024, -0.4597, -0.5829],\n",
       "        [ 0.3332,  0.1307,  0.1760,  ..., -0.3701, -0.5036, -0.5897],\n",
       "        [ 0.1885, -0.1076,  0.1678,  ..., -0.4107, -0.6982, -0.4893]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids=torch.tensor(title_inputs_ids.iloc[1:5, :].values), attention_mask=torch.tensor(masks.iloc[1:5, :].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f31969-b512-4e15-8b25-78d101eb7945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
