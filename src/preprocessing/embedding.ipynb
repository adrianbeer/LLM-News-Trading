{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_colab = False\n",
    "if google_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    cwd=\"/content/drive/MyDrive/NewsTrading/trading_bot\"\n",
    "    %cd /content/drive/MyDrive/NewsTrading/trading_bot\n",
    "    %pip install -r requirements_clean.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Meine Ablage\\NewsTrading\\trading_bot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizerFast\n",
    "from src.config import config, MODEL_CONFIG\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "import torch\n",
    "import os\n",
    "from torch import Tensor\n",
    "from src.utils.time import timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ENCODING_LENGTH = 512\n",
    "DATASET_PATH = config.data.benzinga.cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_input(text, tokenizer):\n",
    "    # Truncation = True as bert can only take inputs of max 512 tokens.\n",
    "    # return_tensors = \"pt\" makes the funciton return PyTorch tensors\n",
    "    # tokenizer.encode_plus specifically returns a dictionary of values instead of just a list of values\n",
    "    encoding = tokenizer(\n",
    "        text, \n",
    "        add_special_tokens = True, \n",
    "        truncation = True, \n",
    "        padding = \"max_length\", \n",
    "        max_length = MAX_ENCODING_LENGTH,\n",
    "        return_attention_mask = True, \n",
    "        return_tensors = \"pt\"\n",
    "    )\n",
    "    # input_ids: mapping the words to tokens\n",
    "    # attention masks: idicates if index is word or padding\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_masks = encoding['attention_mask']\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "\n",
    "@timing\n",
    "def embed_inputs(texts: list, tokenizer) -> tuple[Tensor, Tensor]:\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    pool_obj = ThreadPoolExecutor(max_workers=os.cpu_count())\n",
    "    ans = pool_obj.map(partial(embed_input, tokenizer=tokenizer), texts)\n",
    "    input_ids, attention_masks = list(zip(*ans))\n",
    "\n",
    "    input_ids: Tensor = torch.cat(input_ids, dim=0)\n",
    "    attention_masks: Tensor = torch.cat(attention_masks, dim=0)\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_and_labels(dat: pd.DataFrame, \n",
    "                        text_col: str = None,\n",
    "                        label_col: str = None) -> tuple[List, List]:\n",
    "    texts = dat.loc[:, text_col].tolist()\n",
    "    labels = dat.loc[:, label_col].tolist()\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_CONFIG.transformer_hugface_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowCapacityError",
     "evalue": "array cannot contain more than 2147483646 bytes, have 2151638076",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowCapacityError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\Meine Ablage\\NewsTrading\\trading_bot\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:509\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, **kwargs)\u001b[0m\n\u001b[0;32m    506\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    507\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\Meine Ablage\\NewsTrading\\trading_bot\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:227\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, use_nullable_dtypes, dtype_backend, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m path_or_handle, handles, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    221\u001b[0m     path,\n\u001b[0;32m    222\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    223\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    224\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m )\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 227\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m     result \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mg:\\Meine Ablage\\NewsTrading\\trading_bot\\.venv\\Lib\\site-packages\\pyarrow\\parquet\\core.py:3003\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[0;32m   2992\u001b[0m         \u001b[38;5;66;03m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[0;32m   2993\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m ParquetFile(\n\u001b[0;32m   2994\u001b[0m             source, metadata\u001b[38;5;241m=\u001b[39mmetadata, read_dictionary\u001b[38;5;241m=\u001b[39mread_dictionary,\n\u001b[0;32m   2995\u001b[0m             memory_map\u001b[38;5;241m=\u001b[39mmemory_map, buffer_size\u001b[38;5;241m=\u001b[39mbuffer_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3000\u001b[0m             thrift_container_size_limit\u001b[38;5;241m=\u001b[39mthrift_container_size_limit,\n\u001b[0;32m   3001\u001b[0m         )\n\u001b[1;32m-> 3003\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3004\u001b[0m \u001b[43m                        \u001b[49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3006\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   3007\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_legacy_dataset=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to get the legacy behaviour is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3008\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated as of pyarrow 8.0.0, and the legacy implementation will \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3009\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe removed in a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3010\u001b[0m     \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m   3012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_prefixes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mg:\\Meine Ablage\\NewsTrading\\trading_bot\\.venv\\Lib\\site-packages\\pyarrow\\parquet\\core.py:2631\u001b[0m, in \u001b[0;36m_ParquetDatasetV2.read\u001b[1;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[0;32m   2623\u001b[0m         index_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   2624\u001b[0m             col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[0;32m   2625\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m   2626\u001b[0m         ]\n\u001b[0;32m   2627\u001b[0m         columns \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2628\u001b[0m             \u001b[38;5;28mlist\u001b[39m(columns) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(index_columns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(columns))\n\u001b[0;32m   2629\u001b[0m         )\n\u001b[1;32m-> 2631\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter_expression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2633\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\n\u001b[0;32m   2634\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2636\u001b[0m \u001b[38;5;66;03m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[0;32m   2637\u001b[0m \u001b[38;5;66;03m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[0;32m   2638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pandas_metadata:\n",
      "File \u001b[1;32mg:\\Meine Ablage\\NewsTrading\\trading_bot\\.venv\\Lib\\site-packages\\pyarrow\\_dataset.pyx:556\u001b[0m, in \u001b[0;36mpyarrow._dataset.Dataset.to_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mg:\\Meine Ablage\\NewsTrading\\trading_bot\\.venv\\Lib\\site-packages\\pyarrow\\_dataset.pyx:3713\u001b[0m, in \u001b[0;36mpyarrow._dataset.Scanner.to_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mg:\\Meine Ablage\\NewsTrading\\trading_bot\\.venv\\Lib\\site-packages\\pyarrow\\error.pxi:154\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mg:\\Meine Ablage\\NewsTrading\\trading_bot\\.venv\\Lib\\site-packages\\pyarrow\\error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowCapacityError\u001b[0m: array cannot contain more than 2147483646 bytes, have 2151638076"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_parquet(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy column\n",
    "dataset[\"text_length\"] = dataset[\"parsed_body\"].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels = get_text_and_labels(dat=dataset, \n",
    "                                    text_col=\"parsed_body\", \n",
    "                                    label_col=\"text_length\")\n",
    "input_ids, masks = embed_inputs(texts, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_matrix = np.ndarray(shape=(input_ids.shape[0], 2*MAX_ENCODING_LENGTH+1), dtype=np.int64)\n",
    "encoding_matrix[:, 0] = dataset.index\n",
    "encoding_matrix[:, 1:(MAX_ENCODING_LENGTH+1)] = Tensor.numpy(input_ids)\n",
    "encoding_matrix[:, (MAX_ENCODING_LENGTH+1):] = Tensor.numpy(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(file=config.data.bzg.encoding_matrix, arr=encoding_matrix.astype(\"int64\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding(encoding_matrix_path: str):\n",
    "    encoding_matrix = np.load(file=encoding_matrix_path)\n",
    "    index = encoding_matrix[:, 0]\n",
    "    input_ids = encoding_matrix[:, 1:(MAX_ENCODING_LENGTH+1)]\n",
    "    masks = encoding_matrix[:, (MAX_ENCODING_LENGTH+1):]\n",
    "    return index, input_ids, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx, ii, msks = get_encoding(config.data.bzg.encoding_matrix)\n",
    "torch.from_numpy(ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
