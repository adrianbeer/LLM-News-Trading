{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pandas_market_calendars as mcal\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import plotly.express as px\n",
    "\n",
    "import pytz\n",
    "eastern = pytz.timezone('US/Eastern')\n",
    "nyse_cal = mcal.get_calendar('NYSE')\n",
    "\n",
    "from dotmap import DotMap\n",
    "import yaml\n",
    "config = DotMap(yaml.safe_load(open(\"src/config.yaml\")), _dynamic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Python numpy: cannot convert datetime64[ns] to datetime64[D] (to use with Numba)](https://stackoverflow.com/a/76139900/9079015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_available_candle(prices: pd.DataFrame, \n",
    "                              time: pd.Timestamp) -> pd.Series:\n",
    "    entry_candle_idx = prices.index.get_indexer(target=[time], \n",
    "                                                method=\"bfill\")\n",
    "    entry_candle = prices.take(entry_candle_idx).iloc[0]\n",
    "    return entry_candle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_appropriate_closing_time(time: pd.Timestamp) -> pd.Timestamp:\n",
    "    if (time.hour < 9) or ((time.hour == 9) and (time.minute < 30)):\n",
    "        return pd.Timestamp(year=time.year, month=time.month, day=time.day)\n",
    "    else:\n",
    "        valid_days = [x.date() for x in nyse_cal.valid_days(start_date=time.date(), end_date=time.date() + pd.DateOffset(days=10))]\n",
    "        i = 1\n",
    "        while True:\n",
    "            new_time = time + pd.DateOffset(days=i)\n",
    "            if new_time.date() in valid_days:\n",
    "                return pd.Timestamp(year=new_time.year, month=new_time.month, day=new_time.day)\n",
    "            if i == 7:\n",
    "                return ValueError()\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_trading_hours(df, time_column):\n",
    "    T = df[time_column].dt\n",
    "    min_mask = (T.hour >= 10) | ((T.hour == 9) & (T.minute >= 31))\n",
    "    max_mask = (T.hour < 16) | ((T.hour == 16) & (T.minute <= 1))\n",
    "    return df.loc[min_mask & max_mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_iq_feed_prices(prices: pd.DataFrame) -> pd.DataFrame: \n",
    "    if \"time\" in prices.columns:\n",
    "        # Intra-day data\n",
    "        prices.loc[:, \"time\"] = prices.loc[:, \"time\"].dt.tz_localize(None)\n",
    "        prices.loc[:, \"time\"] = prices.loc[:, \"time\"].dt.tz_localize(eastern)\n",
    "        prices.drop_duplicates(keep=\"first\", inplace=True)\n",
    "        prices.dropna(inplace=True)\n",
    "        \n",
    "        prices = filter_trading_hours(df=prices, time_column=\"time\")\n",
    "\n",
    "        # Deals with duplicate rows which occurr when not all the digits for volume are \n",
    "        # correctly entered, but only the first 1-3. So keep the largest.\n",
    "        prices = prices.sort_values([\"time\", \"volume\"], ascending=[True, False])\n",
    "        prices = prices.drop_duplicates(subset=[\"time\"], keep=\"first\")\n",
    "\n",
    "        prices.set_index(\"time\", inplace=True)\n",
    "        prices.sort_index(ascending=True, inplace=True)\n",
    "        assert prices.index.is_unique\n",
    "    else:\n",
    "        # Daily data\n",
    "        prices.dropna(inplace=True)\n",
    "        prices[\"date\"] = pd.to_datetime(prices.date)\n",
    "    return prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Preprocess News "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(path=\"data/unraw2_bzg/data-10.parquet\")\n",
    "df[\"time\"] = df.time.dt.tz_convert(eastern)\n",
    "\n",
    "# TODO: This can be *improved* by saying that if we are very close to completing the minute e.g. :55, \n",
    "# then we dont take the next candle (T+1), but the candle after the next(T+2).\n",
    "df[\"entry_time\"] = df[\"time\"].dt.ceil(\"min\")\n",
    "\n",
    "\n",
    "# Necessary to get `us` units, otherwise pandas will always convert back to `ns` for some reason.\n",
    "df[\"nn_exit_time\"] = df[\"time\"].map(get_appropriate_closing_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Preprocess Stock Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy: pd.DataFrame = preprocess_iq_feed_prices(pd.read_parquet(path=f\"{config.data.iqfeed.minute.raw}/SPY_1min.parquet\", \n",
    "                                                              columns=[\"time\", \"close\", \"volume\"]))\n",
    "spy_daily: pd.DataFrame = preprocess_iq_feed_prices(pd.read_parquet(path=f\"{config.data.iqfeed.daily.raw}/SPY_daily.parquet\", \n",
    "                                                                    columns=[\"date\", \"close\", \"volume\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles = [f for f in listdir(config.data.iqfeed.minute.raw) if isfile(join(config.data.iqfeed.minute.raw, f))]\n",
    "tickers = [x.split(\"_\")[0] for x in onlyfiles]\n",
    "ticker = \"GOOGL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices: pd.DataFrame = preprocess_iq_feed_prices(pd.read_parquet(path=f\"{config.data.iqfeed.minute.raw}/{ticker}_1min.parquet\", \n",
    "                                                                 columns=[\"time\", \"close\", \"open\", \"volume\"]))\n",
    "prices_daily: pd.DataFrame = preprocess_iq_feed_prices(pd.read_parquet(path=f\"{config.data.iqfeed.minute.raw}/daily/{ticker}_daily.parquet\", \n",
    "                                                                       columns=[\"date\", \"close\", \"open\", \"volume\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_news = df[df.stocks == ticker]\n",
    "merged = pd.merge_asof(ticker_news, prices, left_on=\"entry_time\", right_on=\"time\", direction=\"forward\")\n",
    "merged = pd.merge(merged, prices_daily, left_on=\"nn_exit_time\", right_on=\"date\", suffixes=(\"_entry\", \"_exit\"))\n",
    "merged[\"r\"] = merged[\"close_exit\"] / merged[\"close_entry\"] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideally we do this for every stock first and then we come back with the complete dataframe... (depends on if it fits in memory)\n",
    "# Merge news and stock prices with spy prices\n",
    "merged = pd.merge_asof(merged, spy, left_on=\"entry_time\", right_on=\"time\", direction=\"forward\")\n",
    "\n",
    "# TODO: Don't use intraday as exit here (closing candle) but the actual closing auction...\n",
    "# But for that we need the daily time series, not with minute frequency\n",
    "merged = pd.merge_asof(merged, spy_daily, left_on=\"nn_exit_time\", right_on=\"date\", suffixes=(\"_spy_entry\", \"_spy_exit\"))\n",
    "\n",
    "merged.loc[:, \"r_spy\"] = merged[\"close_spy_exit\"] / merged[\"close_spy_entry\"] - 1\n",
    "merged.loc[:, \"r_mkt_adj\"] = merged[\"r_spy\"] - merged[\"r\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.loc[:, [\"time\", \"stocks\", \"body\", \"entry_time\", \"r_mkt_adj\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to Files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns: `time|  stocks  |body  |entry_time  |r_mkt_adj`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting training and test set\n",
    "merged_test = merged.loc[merged.time >= config.model.data.cutoff_date]\n",
    "merged_train = merged.loc[merged.time < config.model.data.cutoff_date]\n",
    "assert merged_test.shape[0] + merged_train.shape[0] == merged.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model.data.training\n",
    "config.model.data.testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices[(prices.index.hour == 16) & (prices.index.minute==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
