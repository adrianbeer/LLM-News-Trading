{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import dask.dataframe as dd\n",
    "import pytz\n",
    "eastern = pytz.timezone('US/Eastern')\n",
    "\n",
    "from src.config import config\n",
    "from src.preprocessing.data_merger_util import (get_appropriate_closing_time,\n",
    "                                                get_appropriate_entry_time, \n",
    "                                                get_primary_ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Python numpy: cannot convert datetime64[ns] to datetime64[D] (to use with Numba)](https://stackoverflow.com/a/76139900/9079015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Preprocess News "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = dd.read_parquet(path=config.data.benzinga.cleaned, columns=[\"time\", \"stocks\", \"parsed_body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news[\"time\"] = news.time.dt.tz_convert(eastern).astype('datetime64[ns, US/Eastern]')\n",
    "news.rename(columns={\"time\":\"news_time\"}, inplace=True)\n",
    "\n",
    "# TODO: This can be *improved* by saying that if we are very close to completing the minute e.g. :55, \n",
    "# then we dont take the next candle (T+1), but the candle after the next(T+2).\n",
    "# Watch out, news time is accurate, but candles are right labeled, hence add one minute.\n",
    "news[\"est_entry_time\"] = news[\"news_time\"].map(get_appropriate_entry_time)\n",
    "\n",
    "# Necessary to get `us` units, otherwise pandas will always convert back to `ns` for some reason.\n",
    "news[\"est_exit_time\"] = news[\"news_time\"].map(get_appropriate_closing_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidate Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_mapper_consolidated = pd.read_parquet(\"data_shared/ticker_name_mapper_consolidated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite tickers with consolidated ticker, i.e. the ticker of the time series we use to construct input-output pairs\n",
    "news[\"stocks\"] = news.stocks.map(lambda ticker: get_primary_ticker(ticker, mapper=ticker_mapper_consolidated))\n",
    "# Some tickers don't exist, they will be converted to NaNs\n",
    "news = news.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge News with Price Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Also merge with  non-adjusted prices. We don't trade penny stocks.\n",
    "# If the price is smaller than 1 when the news come out we don't trade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy: pd.DataFrame = pd.read_parquet(path=f\"{config.data.iqfeed.minute.cleaned}/SPY_1min.parquet\")\n",
    "spy.columns = [x.strip(\"adj_\") for x in spy.columns]\n",
    "spy.columns = [f\"SPY_{x}\" for x in spy.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tickers = ticker_mapper_consolidated.stocks[ticker_mapper_consolidated.is_primary_ticker].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in tqdm(unique_tickers):\n",
    "    ticker_news = news.loc[news.stocks == ticker, :].reset_index()\n",
    "\n",
    "    prices: pd.DataFrame = pd.read_parquet(path=f\"{config.data.iqfeed.minute.cleaned}/{ticker}_1min.parquet\")\n",
    "    prices.columns = [x.strip(\"adj_\") for x in prices.columns]\n",
    "    prices = prices.reset_index().sort_values(\"time\")\n",
    "\n",
    "    # We generally neeed to use `merge_asof` here instad of simple `merge`, because\n",
    "    # Sometimes no auction occurred or was recorded at 16:00 or things of this sort.\n",
    "\n",
    "    # Left key must be sorted\n",
    "    ticker_news.sort_values(\"est_entry_time\", inplace=True)\n",
    "    merged = pd.merge_asof(ticker_news, prices.rename(columns=dict(time=\"entry_time\")), left_on=\"est_entry_time\", right_on=\"entry_time\", direction=\"forward\")\n",
    "\n",
    "    merged.sort_values(\"est_exit_time\", inplace=True)\n",
    "    merged = pd.merge_asof(merged, prices.rename(columns=dict(time=\"exit_time\")), left_on=\"est_exit_time\", right_on=\"exit_time\", suffixes=(\"_entry\", \"_exit\"), direction=\"backward\")\n",
    "    # We use the O part of the OHLC for intra day candles here for convenienece as well\n",
    "    merged[\"r\"] = merged[\"open_exit\"] / merged[\"open_entry\"] - 1\n",
    "\n",
    "    # Ideally we do this for every stock first and then we come back with the complete dataframe... (depends on if it fits in memory)\n",
    "    # Merge news and stock prices with spy prices\n",
    "    merged.sort_values(\"entry_time\", inplace=True)\n",
    "    merged.dropna(inplace=True) # NaN can occurr e.g. if there ist not exit_time for an est_exit_time\n",
    "    merged = pd.merge_asof(merged, spy, left_on=\"entry_time\", right_on=\"time\", direction=\"forward\")\n",
    "\n",
    "    # TODO: Don't use intraday as exit here (closing candle) but the actual closing auction...\n",
    "    # But for that we need the daily time series, not with minute frequency\n",
    "    merged.sort_values(\"exit_time\", inplace=True)\n",
    "    merged.dropna(inplace=True)\n",
    "    merged = pd.merge_asof(merged, spy, left_on=\"exit_time\", right_on=\"time\", suffixes=(\"_entry\", \"_exit\"), direction=\"backward\")\n",
    "\n",
    "    # Calculate to potentially filter out penny stocks later on\n",
    "    merged[\"unadj_entry_open\"] = merged[\"open_entry\"] / merged[\"cum_split_ratio_entry\"]\n",
    "\n",
    "    #TODO: shouldnt we use open entry and close exit?\n",
    "    merged[\"r_spy\"] = merged[\"SPY_close_exit\"] / merged[\"SPY_close_entry\"] - 1\n",
    "\n",
    "    merged.set_index(\"index\", inplace=True)\n",
    "    \n",
    "    keep_columns_from_news = [\"staleness\"]\n",
    "    keep_columns = [\"est_entry_time\", \"est_exit_time\", \"r\", \"unadj_entry_open\", \"r_spy\", \"entry_is_too_far_apart\", \"exit_is_too_far_apart\"] + keep_columns_from_news\n",
    "    \n",
    "    # Filter out stocks where estimated entry/exit is further apart than actual entry/exit by more than 1h\n",
    "    merged[\"entry_is_too_far_apart\"] = (merged.entry_time - merged.est_entry_time) > pd.Timedelta(hours=1)\n",
    "    merged[\"exit_is_too_far_apart\"] = (merged.exit_time - merged.est_exit_time) > pd.Timedelta(hours=1)\n",
    "\n",
    "    news.loc[merged.index, keep_columns] = merged.loc[:, keep_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (news[\"entry_is_too_far_apart\"] | news[\"exit_is_too_far_apart\"])\n",
    "news = news[~(news[\"entry_is_too_far_apart\"]|news[\"exit_is_too_far_apart\"])]\n",
    "print(f\"Filtered rows: {mask.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_mapper_consolidated[ticker_mapper_consolidated.stocks == \"ALV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{news.shape[0]} news before. {news.dropna().shape[0]} news after dropping NaNs.\"\n",
    "      f\"NaNs should occurr, when we don't have a price time series when news occurred.\")\n",
    "news = news.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Disk\n",
    "news.to_parquet(config.data.merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## Merge with Daily Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.tickers import get_tickers\n",
    "tickers = get_tickers(config.data.iqfeed.daily.cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_parquet(path=config.data.merged)\n",
    "dataset[[\"std_252\", \"dollar_volume\", 'r_intra_(t-1)', 'unadj_open']] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = [\"std_252\", \"dollar_volume\", 'r_intra_(t-1)', 'unadj_open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = tickers[0]\n",
    "prices = pd.read_parquet(path=f\"{config.data.iqfeed.daily.cleaned}/{ticker}_daily.parquet\")\n",
    "prices.index = prices.index.tz_localize(\"US/Eastern\")\n",
    "ticker_dat = dataset.loc[dataset.stocks == ticker, :].reset_index()\n",
    "merged = pd.merge_asof(ticker_dat, prices[indicators], left_on=\"est_entry_time\", right_on=\"date\", direction=\"backward\")\n",
    "merged.set_index(\"index\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in tqdm(tickers):\n",
    "    prices = pd.read_parquet(path=f\"{config.data.iqfeed.daily.cleaned}/{ticker}_daily.parquet\")\n",
    "    prices.index = prices.index.tz_localize(\"US/Eastern\")\n",
    "    ticker_dat = dataset.loc[dataset.stocks == ticker, :].reset_index().drop(columns=indicators)\n",
    "    ticker_dat.sort_values(\"est_entry_time\", inplace=True)\n",
    "    merged = pd.merge_asof(ticker_dat, prices[indicators], left_on=\"est_entry_time\", right_on=\"date\", direction=\"backward\")\n",
    "    merged.set_index(\"index\", inplace=True)\n",
    "    dataset.loc[merged.index, indicators] = merged[indicators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_parquet(path=config.data.merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat: pd.DataFrame = pd.read_parquet(path=config.data.merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.loc[:, \"r_mkt_adj\"] =  dat[\"r\"] - dat[\"r_spy\"]\n",
    "# std_252 is annualized, but returns arent...\n",
    "# TODO: Dont annualize in indicator_applicator! or make it clear by naming properly\n",
    "#TODO: This needs to be of r_mkt_adj, not of wahtever else std_252 is or?\n",
    "dat.loc[:, \"z_score\"] = dat[\"r_mkt_adj\"] / (dat[\"std_252\"]/(252**0.5)) \n",
    "\n",
    "# TODO: Calculate based on training set split\n",
    "upper_z_quantile = 0.27\n",
    "lower_z_quantile = -0.27\n",
    "dat.loc[:, \"z_score_class\"] = 1\n",
    "# Ordinal labeling\n",
    "dat.loc[dat[\"z_score\"] >= upper_z_quantile, \"z_score_class\"] = 2\n",
    "dat.loc[dat[\"z_score\"] <= lower_z_quantile, \"z_score_class\"] = 0\n",
    "dat[\"z_score_class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.to_parquet(path=config.data.merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
