{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from os.path import isfile, join\n",
    "import plotly.express as px\n",
    "from IPython.display import clear_output\n",
    "import dask.dataframe as dd\n",
    "from os import listdir\n",
    "\n",
    "import pytz\n",
    "eastern = pytz.timezone('US/Eastern')\n",
    "\n",
    "from dotmap import DotMap\n",
    "import yaml\n",
    "config = DotMap(yaml.safe_load(open(\"src/config.yaml\")), _dynamic=False)\n",
    "\n",
    "from src.preprocessing.util import get_appropriate_closing_time,get_appropriate_entry_time, get_time_interval, consolidate_tickers, get_primary_ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig(filename='logs/preprocessing/data_merger.log', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Python numpy: cannot convert datetime64[ns] to datetime64[D] (to use with Numba)](https://stackoverflow.com/a/76139900/9079015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Preprocess News "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = dd.read_parquet(path=\"data/processed_news\", columns=[\"time\", \"stocks\", \"parsed_body\"])\n",
    "# news = pd.read_parquet(path=\"data/processed_news/data-0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "news[\"time\"] = news.time.dt.tz_convert(eastern).astype('datetime64[ns, US/Eastern]')\n",
    "\n",
    "# TODO: This can be *improved* by saying that if we are very close to completing the minute e.g. :55, \n",
    "# then we dont take the next candle (T+1), but the candle after the next(T+2).\n",
    "# Watch out, news time is accurate, but candles are right labeled, hence add one minute.\n",
    "news[\"entry_time\"] = news[\"time\"].map(get_appropriate_entry_time)\n",
    "\n",
    "# Necessary to get `us` units, otherwise pandas will always convert back to `ns` for some reason.\n",
    "news[\"nn_exit_time\"] = news[\"time\"].map(get_appropriate_closing_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidate Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHandling of multiple tickers for a the same company.\\nIf there is only one price time series available for the company, we simply group together the tickers.\\nHowever in some cases we will have multiple price time series for the same company.\\n\\nE.g. in case of Alphabet (Google) we have two different tickers and two different stock prices for the same\\nunderlying company. Here `GOOG` and `GOOGL` describe two different classes of stock for the same company.\\nIn this case we will try to only look at the main class. \\n\\nWe find this class by choosing the Symbol with the longer stock price history, assuming that the history\\nof it includes(!) the history of the other one completely.\\nIf one time series doesn't include the other we merge the two time series. Ideally based on which time series has more liquidity \\nin a given week or but we will simply decide that the newer time series takes precedence for simplicity. \\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Handling of multiple tickers for a the same company.\n",
    "If there is only one price time series available for the company, we simply group together the tickers.\n",
    "However in some cases we will have multiple price time series for the same company.\n",
    "\n",
    "E.g. in case of Alphabet (Google) we have two different tickers and two different stock prices for the same\n",
    "underlying company. Here `GOOG` and `GOOGL` describe two different classes of stock for the same company.\n",
    "In this case we will try to only look at the main class. \n",
    "\n",
    "We find this class by choosing the Symbol with the longer stock price history, assuming that the history\n",
    "of it includes(!) the history of the other one completely.\n",
    "If one time series doesn't include the other we merge the two time series. Ideally based on which time series has more liquidity \n",
    "in a given week or but we will simply decide that the newer time series takes precedence for simplicity. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_consolidate_tickers = False\n",
    "if do_consolidate_tickers:\n",
    "    ticker_mapper = pd.read_parquet(\"data_shared/ticker_name_mapper_reduced.parquet\")\n",
    "    ticker_mapper[[\"first_date\", \"last_date\"]] = np.NaN\n",
    "    for i in ticker_mapper.index:\n",
    "        ticker_mapper.loc[i, [\"first_date\", \"last_date\"]] = get_time_interval(ticker_mapper.loc[i, \"stocks\"])\n",
    "    ticker_mapper.dropna(inplace=True)\n",
    "    ticker_mapper[[\"first_date\", \"last_date\"]] = ticker_mapper[[\"first_date\", \"last_date\"]].apply(pd.to_datetime, axis=0)\n",
    "\n",
    "    ticker_mapper_consolidated = ticker_mapper.copy(deep=True)\n",
    "    ticker_mapper_consolidated[\"is_primary_ticker\"] = False\n",
    "    ticker_mapper_consolidated = ticker_mapper_consolidated.groupby(\"company_name\", as_index=False).apply(consolidate_tickers)\n",
    "    print(f\"{ticker_mapper_consolidated.shape[0]} entries before consolidation. {ticker_mapper_consolidated[ticker_mapper_consolidated.is_primary_ticker].shape} entries after.\")\n",
    "    ticker_mapper_consolidated.to_parquet(\"data_shared/ticker_name_mapper_consolidated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_mapper_consolidated = pd.read_parquet(\"data_shared/ticker_name_mapper_consolidated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite tickers with consolidated ticker, i.e. the ticker of the time series we use to construct input-output pairs\n",
    "news[\"stocks\"] = news.stocks.map(lambda ticker: get_primary_ticker(ticker, mapper=ticker_mapper_consolidated))\n",
    "# Some tickers don't exist, they will be converted to NaNs\n",
    "news = news.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index              3418496\n",
       "time               3418496\n",
       "stocks             3418496\n",
       "parsed_body     1939337930\n",
       "entry_time         3418496\n",
       "nn_exit_time       3418496\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Also merge with  non-adjusted prices. We don't trade penny stocks.\n",
    "# If the price is smaller than 1 when the news come out we don't trade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy: pd.DataFrame = pd.read_parquet(path=f\"{config.data.iqfeed.minute.cleaned}/SPY_1min.parquet\")\n",
    "spy.columns = [x.strip(\"adj_\") for x in spy.columns]\n",
    "spy.columns = [f\"SPY_{x}\" for x in spy.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tickers = ticker_mapper_consolidated.stocks[ticker_mapper_consolidated.is_primary_ticker == True].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, ticker in enumerate(unique_tickers):\n",
    "#     clear_output(wait=True)\n",
    "#     print(f\"{i} - {ticker}\", flush=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = \"ALV\"\n",
    "### \n",
    "ticker_news = news.loc[news.stocks == ticker, :].reset_index()\n",
    "\n",
    "prices: pd.DataFrame = pd.read_parquet(path=f\"{config.data.iqfeed.minute.cleaned}/{ticker}_1min.parquet\")\n",
    "prices.columns = [x.strip(\"adj_\") for x in prices.columns]\n",
    "prices = prices.sort_values(\"time\")\n",
    "\n",
    "merged = pd.merge_asof(ticker_news, prices, left_on=\"entry_time\", right_on=\"time\", direction=\"forward\")\n",
    "# TODO: backfill asof here, since sometimes auction doesnt occurr or ... i dont know was recorded as taking place at 16:00\n",
    "# Weird -> Need to inspect!\n",
    "merged = pd.merge_asof(merged, prices, left_on=\"nn_exit_time\", right_on=\"time\", suffixes=(\"_entry\", \"_exit\"), direction=\"backward\")\n",
    "# We use the O part of the OHLC for intra day candles here for convenienece as well\n",
    "merged[\"r\"] = merged[\"open_exit\"] / merged[\"open_entry\"] - 1\n",
    "\n",
    "# Ideally we do this for every stock first and then we come back with the complete dataframe... (depends on if it fits in memory)\n",
    "# Merge news and stock prices with spy prices\n",
    "# merged = pd.merge_asof(merged, spy, left_on=\"entry_time\", right_on=\"time\", direction=\"forward\")\n",
    "merged = pd.merge(merged, spy, left_on=\"entry_time\", right_on=\"time\", how=\"left\")\n",
    "\n",
    "# TODO: Don't use intraday as exit here (closing candle) but the actual closing auction...\n",
    "# But for that we need the daily time series, not with minute frequency\n",
    "merged = pd.merge(merged, spy, left_on=\"nn_exit_time\", right_on=\"time\", suffixes=(\"_entry\", \"_exit\"), how=\"left\")\n",
    "\n",
    "merged.loc[:, \"r_spy\"] = merged[\"SPY_close_exit\"] / merged[\"SPY_close_entry\"] - 1\n",
    "merged.loc[:, \"r_mkt_adj\"] = merged[\"r_spy\"] - merged[\"r\"]\n",
    "\n",
    "# Calculate to potentially filter out penny stocks later on\n",
    "merged[\"unadj_entry_stock_price\"] = merged.close_entry / merged.cum_split_ratio_entry\n",
    "\n",
    "merged.set_index(\"index\", inplace=True)\n",
    "\n",
    "merged = merged.loc[:, [\"time\", \"stocks\", \"parsed_body\", \"entry_time\", \"nn_exit_time\", \"r\", \"r_spy\", \"r_mkt_adj\"]]\n",
    "news.loc[merged.index, [\"entry_time\", \"r\", \"r_spy\", \"r_mkt_adj\"]] = merged.loc[:, [\"entry_time\", \"r\", \"r_spy\", \"r_mkt_adj\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SPY_open</th>\n",
       "      <th>SPY_high</th>\n",
       "      <th>SPY_low</th>\n",
       "      <th>SPY_close</th>\n",
       "      <th>SPY_volume</th>\n",
       "      <th>SPY_cum_split_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-07-25 09:31:00-04:00</th>\n",
       "      <td>105.469291</td>\n",
       "      <td>105.540500</td>\n",
       "      <td>105.461379</td>\n",
       "      <td>105.524676</td>\n",
       "      <td>1.293347e+06</td>\n",
       "      <td>0.791217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-07-25 09:32:00-04:00</th>\n",
       "      <td>105.520720</td>\n",
       "      <td>105.532667</td>\n",
       "      <td>105.413906</td>\n",
       "      <td>105.413906</td>\n",
       "      <td>1.013519e+06</td>\n",
       "      <td>0.791217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-07-25 09:33:00-04:00</th>\n",
       "      <td>105.413906</td>\n",
       "      <td>105.453466</td>\n",
       "      <td>105.382257</td>\n",
       "      <td>105.437642</td>\n",
       "      <td>8.423487e+05</td>\n",
       "      <td>0.791217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-07-25 09:34:00-04:00</th>\n",
       "      <td>105.437642</td>\n",
       "      <td>105.564237</td>\n",
       "      <td>105.429730</td>\n",
       "      <td>105.556325</td>\n",
       "      <td>1.057097e+06</td>\n",
       "      <td>0.791217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-07-25 09:35:00-04:00</th>\n",
       "      <td>105.562575</td>\n",
       "      <td>105.587973</td>\n",
       "      <td>105.540500</td>\n",
       "      <td>105.572149</td>\n",
       "      <td>8.321808e+05</td>\n",
       "      <td>0.791217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-15 15:57:00-05:00</th>\n",
       "      <td>468.537352</td>\n",
       "      <td>468.565239</td>\n",
       "      <td>468.112076</td>\n",
       "      <td>468.430784</td>\n",
       "      <td>1.384860e+06</td>\n",
       "      <td>0.995962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-15 15:58:00-05:00</th>\n",
       "      <td>468.440744</td>\n",
       "      <td>468.460663</td>\n",
       "      <td>468.122036</td>\n",
       "      <td>468.231592</td>\n",
       "      <td>1.379081e+06</td>\n",
       "      <td>0.995962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-15 15:59:00-05:00</th>\n",
       "      <td>468.241551</td>\n",
       "      <td>468.311269</td>\n",
       "      <td>468.122036</td>\n",
       "      <td>468.122036</td>\n",
       "      <td>1.381041e+06</td>\n",
       "      <td>0.995962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-15 16:00:00-05:00</th>\n",
       "      <td>468.122036</td>\n",
       "      <td>468.141955</td>\n",
       "      <td>467.444782</td>\n",
       "      <td>467.474660</td>\n",
       "      <td>2.882668e+06</td>\n",
       "      <td>0.995962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-15 16:01:00-05:00</th>\n",
       "      <td>467.464701</td>\n",
       "      <td>467.912884</td>\n",
       "      <td>467.424862</td>\n",
       "      <td>467.743570</td>\n",
       "      <td>2.982772e+06</td>\n",
       "      <td>0.995962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1218622 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             SPY_open    SPY_high     SPY_low   SPY_close  \\\n",
       "time                                                                        \n",
       "2011-07-25 09:31:00-04:00  105.469291  105.540500  105.461379  105.524676   \n",
       "2011-07-25 09:32:00-04:00  105.520720  105.532667  105.413906  105.413906   \n",
       "2011-07-25 09:33:00-04:00  105.413906  105.453466  105.382257  105.437642   \n",
       "2011-07-25 09:34:00-04:00  105.437642  105.564237  105.429730  105.556325   \n",
       "2011-07-25 09:35:00-04:00  105.562575  105.587973  105.540500  105.572149   \n",
       "...                               ...         ...         ...         ...   \n",
       "2023-12-15 15:57:00-05:00  468.537352  468.565239  468.112076  468.430784   \n",
       "2023-12-15 15:58:00-05:00  468.440744  468.460663  468.122036  468.231592   \n",
       "2023-12-15 15:59:00-05:00  468.241551  468.311269  468.122036  468.122036   \n",
       "2023-12-15 16:00:00-05:00  468.122036  468.141955  467.444782  467.474660   \n",
       "2023-12-15 16:01:00-05:00  467.464701  467.912884  467.424862  467.743570   \n",
       "\n",
       "                             SPY_volume  SPY_cum_split_ratio  \n",
       "time                                                          \n",
       "2011-07-25 09:31:00-04:00  1.293347e+06             0.791217  \n",
       "2011-07-25 09:32:00-04:00  1.013519e+06             0.791217  \n",
       "2011-07-25 09:33:00-04:00  8.423487e+05             0.791217  \n",
       "2011-07-25 09:34:00-04:00  1.057097e+06             0.791217  \n",
       "2011-07-25 09:35:00-04:00  8.321808e+05             0.791217  \n",
       "...                                 ...                  ...  \n",
       "2023-12-15 15:57:00-05:00  1.384860e+06             0.995962  \n",
       "2023-12-15 15:58:00-05:00  1.379081e+06             0.995962  \n",
       "2023-12-15 15:59:00-05:00  1.381041e+06             0.995962  \n",
       "2023-12-15 16:00:00-05:00  2.882668e+06             0.995962  \n",
       "2023-12-15 16:01:00-05:00  2.982772e+06             0.995962  \n",
       "\n",
       "[1218622 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spy.loc[\"2011-07-24 09:32:00-04:00\t\":]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>stocks</th>\n",
       "      <th>parsed_body</th>\n",
       "      <th>entry_time</th>\n",
       "      <th>nn_exit_time</th>\n",
       "      <th>r</th>\n",
       "      <th>r_spy</th>\n",
       "      <th>r_mkt_adj</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1795622</th>\n",
       "      <td>2011-07-24 06:46:05-04:00</td>\n",
       "      <td>ALV</td>\n",
       "      <td>Regulatory News:  the company  (NYSE: ALV, and...</td>\n",
       "      <td>2011-07-24 09:32:00-04:00</td>\n",
       "      <td>2011-07-25 16:01:00-04:00</td>\n",
       "      <td>-0.009987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3112919</th>\n",
       "      <td>2012-11-22 04:27:07-05:00</td>\n",
       "      <td>ALV</td>\n",
       "      <td>Regulatory News: In addition to the recently a...</td>\n",
       "      <td>2012-11-22 09:32:00-05:00</td>\n",
       "      <td>2012-11-23 16:01:00-05:00</td>\n",
       "      <td>-0.001507</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3536024</th>\n",
       "      <td>2013-04-28 14:01:06-04:00</td>\n",
       "      <td>ALV</td>\n",
       "      <td>Glancy Binkow &amp; Goldberg LLP announces that a ...</td>\n",
       "      <td>2013-04-28 09:32:00-04:00</td>\n",
       "      <td>2013-04-29 16:01:00-04:00</td>\n",
       "      <td>0.005053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3620734</th>\n",
       "      <td>2013-05-25 00:03:31-04:00</td>\n",
       "      <td>ALV</td>\n",
       "      <td>The Law Offices of Todd M.Garber announces tha...</td>\n",
       "      <td>2013-05-25 09:32:00-04:00</td>\n",
       "      <td>2013-05-28 16:01:00-04:00</td>\n",
       "      <td>-0.006215</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4112542</th>\n",
       "      <td>2013-11-27 04:10:30-05:00</td>\n",
       "      <td>ALV</td>\n",
       "      <td>the company  (STO:ALIVSDB) the worldwide leade...</td>\n",
       "      <td>2013-11-27 09:32:00-05:00</td>\n",
       "      <td>2013-11-29 16:01:00-05:00</td>\n",
       "      <td>-0.003990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5244134</th>\n",
       "      <td>2015-02-16 14:34:05-05:00</td>\n",
       "      <td>ALV</td>\n",
       "      <td>the company  (STO:ALIVSDB)  (NYSE: ALV, and SS...</td>\n",
       "      <td>2015-02-16 09:32:00-05:00</td>\n",
       "      <td>2015-02-17 16:01:00-05:00</td>\n",
       "      <td>0.004294</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9069438</th>\n",
       "      <td>2017-02-20 02:36:50-05:00</td>\n",
       "      <td>ALV</td>\n",
       "      <td>the company  , the worldwide leader in automot...</td>\n",
       "      <td>2017-02-20 09:32:00-05:00</td>\n",
       "      <td>2017-02-21 16:01:00-05:00</td>\n",
       "      <td>0.005524</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11221896</th>\n",
       "      <td>2018-02-19 06:42:48-05:00</td>\n",
       "      <td>ALV</td>\n",
       "      <td>the company  , the worldwide leader in automot...</td>\n",
       "      <td>2018-02-19 09:32:00-05:00</td>\n",
       "      <td>2018-02-20 16:01:00-05:00</td>\n",
       "      <td>0.010401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              time stocks  \\\n",
       "index                                       \n",
       "1795622  2011-07-24 06:46:05-04:00    ALV   \n",
       "3112919  2012-11-22 04:27:07-05:00    ALV   \n",
       "3536024  2013-04-28 14:01:06-04:00    ALV   \n",
       "3620734  2013-05-25 00:03:31-04:00    ALV   \n",
       "4112542  2013-11-27 04:10:30-05:00    ALV   \n",
       "5244134  2015-02-16 14:34:05-05:00    ALV   \n",
       "9069438  2017-02-20 02:36:50-05:00    ALV   \n",
       "11221896 2018-02-19 06:42:48-05:00    ALV   \n",
       "\n",
       "                                                parsed_body  \\\n",
       "index                                                         \n",
       "1795622   Regulatory News:  the company  (NYSE: ALV, and...   \n",
       "3112919   Regulatory News: In addition to the recently a...   \n",
       "3536024   Glancy Binkow & Goldberg LLP announces that a ...   \n",
       "3620734   The Law Offices of Todd M.Garber announces tha...   \n",
       "4112542   the company  (STO:ALIVSDB) the worldwide leade...   \n",
       "5244134   the company  (STO:ALIVSDB)  (NYSE: ALV, and SS...   \n",
       "9069438   the company  , the worldwide leader in automot...   \n",
       "11221896  the company  , the worldwide leader in automot...   \n",
       "\n",
       "                        entry_time              nn_exit_time         r  r_spy  \\\n",
       "index                                                                           \n",
       "1795622  2011-07-24 09:32:00-04:00 2011-07-25 16:01:00-04:00 -0.009987    NaN   \n",
       "3112919  2012-11-22 09:32:00-05:00 2012-11-23 16:01:00-05:00 -0.001507    NaN   \n",
       "3536024  2013-04-28 09:32:00-04:00 2013-04-29 16:01:00-04:00  0.005053    NaN   \n",
       "3620734  2013-05-25 09:32:00-04:00 2013-05-28 16:01:00-04:00 -0.006215    NaN   \n",
       "4112542  2013-11-27 09:32:00-05:00 2013-11-29 16:01:00-05:00 -0.003990    NaN   \n",
       "5244134  2015-02-16 09:32:00-05:00 2015-02-17 16:01:00-05:00  0.004294    NaN   \n",
       "9069438  2017-02-20 09:32:00-05:00 2017-02-21 16:01:00-05:00  0.005524    NaN   \n",
       "11221896 2018-02-19 09:32:00-05:00 2018-02-20 16:01:00-05:00  0.010401    NaN   \n",
       "\n",
       "          r_mkt_adj  \n",
       "index                \n",
       "1795622         NaN  \n",
       "3112919         NaN  \n",
       "3536024         NaN  \n",
       "3620734         NaN  \n",
       "4112542         NaN  \n",
       "5244134         NaN  \n",
       "9069438         NaN  \n",
       "11221896        NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged[merged.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>stocks</th>\n",
       "      <th>company_name</th>\n",
       "      <th>short_name</th>\n",
       "      <th>first_date</th>\n",
       "      <th>last_date</th>\n",
       "      <th>is_primary_ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <th>74475</th>\n",
       "      <td>ALV</td>\n",
       "      <td>Autoliv, Inc.</td>\n",
       "      <td>Autoliv</td>\n",
       "      <td>2010-01-04 09:31:00-05:00</td>\n",
       "      <td>2023-12-15 16:01:00-05:00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          stocks   company_name short_name                first_date  \\\n",
       "778 74475    ALV  Autoliv, Inc.    Autoliv 2010-01-04 09:31:00-05:00   \n",
       "\n",
       "                          last_date  is_primary_ticker  \n",
       "778 74475 2023-12-15 16:01:00-05:00               True  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_mapper_consolidated[ticker_mapper_consolidated.stocks == \"ALV\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time', 'stocks', 'parsed_body', 'entry_time', 'nn_exit_time', 'r',\n",
       "       'r_spy', 'r_mkt_adj'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367479 news before. 153507 news after dropping NaNs.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{news.shape[0]} news before. {news.dropna().shape[0]} news after dropping NaNs.\")\n",
    "news = news.dropna()\n",
    "news.to_parquet(config.data.merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_parquet(config.data.merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting training and test set\n",
    "merged_test = merged.loc[merged.time >= config.model.data.cutoff_date]\n",
    "merged_train = merged.loc[merged.time < config.model.data.cutoff_date]\n",
    "assert merged_test.shape[0] + merged_train.shape[0] == merged.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/Data/NN_Training'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.model.data.training\n",
    "config.model.data.testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
