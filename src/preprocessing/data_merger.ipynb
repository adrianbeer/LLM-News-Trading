{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json\n",
    "import pandas_market_calendars as mcal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minutely Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List off functions for modifying the return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import IQFeed Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = \"FITB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_available_candle(prices: pd.DataFrame, \n",
    "                              time: pd.Timestamp) -> pd.Series:\n",
    "    entry_candle_idx = prices.index.get_indexer(target=[time], \n",
    "                                                method=\"bfill\")\n",
    "    entry_candle = prices.take(entry_candle_idx).iloc[0]\n",
    "    return entry_candle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyse_cal = mcal.get_calendar('NYSE')\n",
    "def get_appropriate_closing_time(time: pd.Timestamp) -> pd.Timestamp:\n",
    "    if (time.hour < 9) or ((time.hour == 9) and (time.minute < 30)):\n",
    "        return pd.Timestamp(year=time.year, month=time.month, day=time.day, hour=16, minute=0, tz=time.tz, unit=\"us\")\n",
    "    else:\n",
    "        valid_days = [x.date() for x in nyse_cal.valid_days(start_date=time.date(), end_date=time.date() + pd.DateOffset(days=10))]\n",
    "        i = 1\n",
    "        while True:\n",
    "            new_time = time + pd.DateOffset(days=i)\n",
    "            if new_time.date() in valid_days:\n",
    "                return pd.Timestamp(year=new_time.year, month=new_time.month, day=new_time.day, hour=16, minute=0, tz=time.tz, unit=\"us\")\n",
    "            if i == 7:\n",
    "                return ValueError()\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "def preprocess_iq_feed_prices(prices: pd.DataFrame) -> pd.DataFrame: \n",
    "    eastern = pytz.timezone('US/Eastern')\n",
    "    prices.loc[:, \"time\"] = prices.loc[:, \"time\"].dt.tz_localize(None)\n",
    "    prices.loc[:, \"time\"] = prices.loc[:, \"time\"].dt.tz_localize(eastern)\n",
    "    prices.drop_duplicates(keep=\"first\", inplace=True)\n",
    "    prices.dropna(inplace=True)\n",
    "\n",
    "    # Deals with duplicate rows which occurr when not all the digits for volume are \n",
    "    # correctly entered, but only the first 1-3. So keep the largest.\n",
    "    prices = prices.sort_values([\"time\", \"volume\"], ascending=[True, False])\n",
    "    prices = prices.drop_duplicates(subset=[\"time\"], keep=\"first\")\n",
    "\n",
    "    prices.set_index(\"time\", inplace=True)\n",
    "    prices.sort_index(ascending=True, inplace=True)\n",
    "    assert prices.index.is_unique\n",
    "    return prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Preprocess Stock Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04 08:03:00-05:00</th>\n",
       "      <td>9.75</td>\n",
       "      <td>7799.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 08:06:00-05:00</th>\n",
       "      <td>9.75</td>\n",
       "      <td>67884.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 08:13:00-05:00</th>\n",
       "      <td>9.84</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           close   volume\n",
       "time                                     \n",
       "2010-01-04 08:03:00-05:00   9.75   7799.0\n",
       "2010-01-04 08:06:00-05:00   9.75  67884.0\n",
       "2010-01-04 08:13:00-05:00   9.84    300.0"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices: pd.DataFrame = pd.read_parquet(f\"D:/IQFeedData/{ticker}_1min.parquet\", columns=[\"time\", \"close\", \"volume\"])\n",
    "prices = preprocess_iq_feed_prices(prices)\n",
    "prices.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04 04:08:00-05:00</th>\n",
       "      <td>112.25</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 04:09:00-05:00</th>\n",
       "      <td>112.25</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 04:16:00-05:00</th>\n",
       "      <td>112.22</td>\n",
       "      <td>19030.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            close   volume\n",
       "time                                      \n",
       "2010-01-04 04:08:00-05:00  112.25   1000.0\n",
       "2010-01-04 04:09:00-05:00  112.25   1000.0\n",
       "2010-01-04 04:16:00-05:00  112.22  19030.0"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spy: pd.DataFrame = pd.read_parquet(f\"D:/IQFeedData/SPY_1min.parquet\", columns=[\"time\", \"close\", \"volume\"])\n",
    "spy = preprocess_iq_feed_prices(spy)\n",
    "spy.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Preprocess News "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/unraw2_bzg/data-5.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct assignment doesnt work here, if df.time \n",
    "# already has a tz then it doesnt change on assignment \n",
    "tmp = df.time.dt.tz_convert(eastern)\n",
    "df.loc[:, \"time\"] = 0\n",
    "df.loc[:, \"time\"] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011-04-08 07:00:00-04:00\n",
      "2011-04-08 16:00:00-04:00\n"
     ]
    }
   ],
   "source": [
    "ts = df.time.dt.ceil(\"min\").iloc[5]\n",
    "print(ts)\n",
    "new_ts = get_appropriate_closing_time(ts)\n",
    "print(new_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This can be *improved* by saying that if we are very close to completing the minute e.g. :55, \n",
    "# then we dont take the next candle (T+1), but the candle after the next(T+2).\n",
    "df.loc[:, \"entry_time\"] = df.loc[:, \"time\"].dt.ceil(\"min\")\n",
    "# Necessary to get `us` units, otherwise pandas will always convert back to `ns` for some reason.\n",
    "df.loc[:, \"nn_exit_time\"] = df.loc[:, \"time\"]\n",
    "df.loc[:, \"nn_exit_time\"] = df.loc[:, \"time\"].map(get_appropriate_closing_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_news = df[df.stocks == ticker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_event = ticker_news.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time', 'stocks', 'author', 'title', 'channels', 'body', 'entry_time',\n",
       "       'nn_exit_time'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices.index.is_monotonic_decreasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Input-Output and Merge news and stock prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge_asof(ticker_news, prices, left_on=\"entry_time\", right_on=\"time\", direction=\"forward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(merged, prices, left_on=\"nn_exit_time\", right_on=\"time\", suffixes=(\"_entry\", \"_exit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[\"r\"] = merged[\"close_exit\"] / merged[\"close_entry\"] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideally we do this for every stock and then we come back with the complete dataframe... (depends on if it fits in memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge news and stock prices with spy prices\n",
    "merged = pd.merge_asof(merged, spy, left_on=\"entry_time\", right_on=\"time\", direction=\"forward\")\n",
    "merged = pd.merge_asof(merged, spy, left_on=\"nn_exit_time\", right_on=\"time\", suffixes=(\"_spy_entry\", \"_spy_exit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.loc[:, \"r_spy\"] = merged[\"close_spy_exit\"] / merged[\"close_spy_entry\"] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.loc[:, \"r_mkt_adj\"] = merged[\"r_spy\"] = merged[\"r\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time                datetime64[us, US/Eastern]\n",
       "stocks                          string[python]\n",
       "author                          string[python]\n",
       "title                           string[python]\n",
       "channels                        string[python]\n",
       "body                            string[python]\n",
       "entry_time          datetime64[us, US/Eastern]\n",
       "nn_exit_time        datetime64[us, US/Eastern]\n",
       "close_entry                            float64\n",
       "volume_entry                           float64\n",
       "close_exit                             float64\n",
       "volume_exit                            float64\n",
       "r                                      float64\n",
       "close_spy_entry                        float64\n",
       "volume_spy_entry                       float64\n",
       "close_spy_exit                         float64\n",
       "volume_spy_exit                        float64\n",
       "r_spy                                  float64\n",
       "r_mkt_adj                              float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = pd.read_pickle(\"data/stocks.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_tickers = set(stocks.index.get_level_values(\"ID\").unique())\n",
    "len(stock_tickers)\n",
    "#test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stories = pd.read_pickle(\"data/stories.pkl\")\n",
    "stories = pd.read_parquet(\"data/raw_bzg/story_df_raw_2019.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories_tickers = set(stories.stocks.unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert stories.stocks.dtype == stocks.index.dtypes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_targets(df):\n",
    "    required_columns = [\"Close\", \"High\", \"Low\", \"Open\"]\n",
    "    # df.loc[:, \"IntradayReturn\"] = df[\"Close\"]/df[\"Open\"] - 1\n",
    "    df.loc[:, \"CloseToCloseReturn\"] = df[\"Close\"] / df.shift(1)[\"Close\"] - 1\n",
    "    # df.loc[:, \"NextDayReturn\"] = df.shift(-1)[\"Close\"] / df.shift(-1)[\"Open\"] - 1\n",
    "    # df.loc[:, \"CloseToNextOpen\"] = df.shift(-1)[\"Open\"] / df[\"Close\"] - 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks.index.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks.loc[:, [\"IntradayReturn\", \"NextDayReturn\"]] = np.nan\n",
    "stocks = stocks.swaplevel(0, 1).sort_index(ascending=[True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = stocks.groupby(\"ID\", as_index=False).apply(add_targets)\n",
    "stocks.index = stocks.index.droplevel(None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.offsets import BDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETER \n",
    "typ = \"CloseToCloseReturn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_appropriate_date(timestamp, typ):\n",
    "    if typ == \"CloseToCloseReturn\":\n",
    "        # TODO: Some noise here due to closing auction?\n",
    "        if timestamp.hour < 16: return timestamp.date()\n",
    "        if timestamp.hour >= 16: return timestamp.date() +  BDay(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "get_appropriate_date(stories.NewsTimestamp.iloc[4], typ=\"CloseToCloseReturn\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we use Intraday return then news should only be between 9:40 am and 4pm (us trading hours).\n",
    "# If we use close-to-close return then news for this days CTC should be between yesterday 4pm and today 4pm. \n",
    "stories.loc[:, \"Date\"] = stories.NewsTimestamp.apply(lambda x: get_appropriate_date(x, typ))\n",
    "stories = stories.astype({\"Date\":'datetime64[ns]'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories.rename(columns=dict(stocks=\"ID\"), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = stories[[\"Date\", \"NewsTimestamp\", \"ID\", \"body\"]].\\\n",
    "    merge(stocks[\n",
    "        [\n",
    "            # \"IntradayReturn\", \n",
    "            # \"NextDayReturn\", \n",
    "            \"CloseToCloseReturn\"]\n",
    "         ], on=[\"Date\", \"ID\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset.isna().sum(axis=1) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\tDate\tNewsTimestamp\tID\tbody\tCloseToCloseReturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pickle(\"data/dataset.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train-test-split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(\"data/dataset.pkl\")\n",
    "assert dataset.index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "seed = 420\n",
    "### Train-test split -> Auslagern\n",
    "train_idx, test_idx = train_test_split(dataset.index, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter training set for general stock market events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select S&P\n",
    "SnP = stocks.query(\"ID == 'A0AET0'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SnP = add_targets(SnP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1 # Percentage observations classified as too extreme to be used in the training set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"CloseToCloseReturn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_quantile = SnP.loc[:, target_col].quantile(alpha/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_quantile = SnP.loc[:, target_col].quantile(1-alpha/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Upper Quantile: {upper_quantile:.4f}. Lower Quantile: {lower_quantile:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (SnP.loc[:, target_col] >= lower_quantile) & (SnP.loc[:, target_col] <= upper_quantile)\n",
    "\n",
    "# Only select dates where SnP behaved calmly\n",
    "allowed_dates = SnP.loc[mask, :].index.get_level_values(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now trim  training set\n",
    "train_dat = dataset.loc[train_idx, :]\n",
    "adj_train_idx = train_dat.loc[train_dat.Date.isin(allowed_dates)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_train_idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save train and test indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dataset_train_test_idx.pkl', 'wb') as f:\n",
    "    pickle.dump((adj_train_idx, test_idx), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
