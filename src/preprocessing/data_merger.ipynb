{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import pytz\n",
    "eastern = pytz.timezone('US/Eastern')\n",
    "\n",
    "from src.config import config\n",
    "from src.preprocessing.data_merger_util import get_appropriate_closing_time,get_appropriate_entry_time, get_time_interval, consolidate_tickers, get_primary_ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Python numpy: cannot convert datetime64[ns] to datetime64[D] (to use with Numba)](https://stackoverflow.com/a/76139900/9079015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Preprocess News "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = dd.read_parquet(path=config.data.benzinga.cleaned, columns=[\"time\", \"stocks\", \"parsed_body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "news[\"time\"] = news.time.dt.tz_convert(eastern).astype('datetime64[ns, US/Eastern]')\n",
    "\n",
    "# TODO: This can be *improved* by saying that if we are very close to completing the minute e.g. :55, \n",
    "# then we dont take the next candle (T+1), but the candle after the next(T+2).\n",
    "# Watch out, news time is accurate, but candles are right labeled, hence add one minute.\n",
    "news[\"entry_time\"] = news[\"time\"].map(get_appropriate_entry_time)\n",
    "\n",
    "# Necessary to get `us` units, otherwise pandas will always convert back to `ns` for some reason.\n",
    "news[\"nn_exit_time\"] = news[\"time\"].map(get_appropriate_closing_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidate Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHandling of multiple tickers for a the same company.\\nIf there is only one price time series available for the company, we simply group together the tickers.\\nHowever in some cases we will have multiple price time series for the same company.\\n\\nE.g. in case of Alphabet (Google) we have two different tickers and two different stock prices for the same\\nunderlying company. Here `GOOG` and `GOOGL` describe two different classes of stock for the same company.\\nIn this case we will try to only look at the main class. \\n\\nWe find this class by choosing the Symbol with the longer stock price history, assuming that the history\\nof it includes(!) the history of the other one completely.\\nIf one time series doesn't include the other we merge the two time series. Ideally based on which time series has more liquidity \\nin a given week or but we will simply decide that the newer time series takes precedence for simplicity. \\n\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Handling of multiple tickers for a the same company.\n",
    "If there is only one price time series available for the company, we simply group together the tickers.\n",
    "However in some cases we will have multiple price time series for the same company.\n",
    "\n",
    "E.g. in case of Alphabet (Google) we have two different tickers and two different stock prices for the same\n",
    "underlying company. Here `GOOG` and `GOOGL` describe two different classes of stock for the same company.\n",
    "In this case we will try to only look at the main class. \n",
    "\n",
    "We find this class by choosing the Symbol with the longer stock price history, assuming that the history\n",
    "of it includes(!) the history of the other one completely.\n",
    "If one time series doesn't include the other we merge the two time series. Ideally based on which time series has more liquidity \n",
    "in a given week or but we will simply decide that the newer time series takes precedence for simplicity. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_consolidate_tickers = False\n",
    "if do_consolidate_tickers:\n",
    "    ticker_mapper = pd.read_parquet(\"data_shared/ticker_name_mapper_reduced.parquet\")\n",
    "    ticker_mapper[[\"first_date\", \"last_date\"]] = np.NaN\n",
    "    for i in ticker_mapper.index:\n",
    "        ticker_mapper.loc[i, [\"first_date\", \"last_date\"]] = get_time_interval(ticker_mapper.loc[i, \"stocks\"])\n",
    "    ticker_mapper.dropna(inplace=True)\n",
    "    ticker_mapper[[\"first_date\", \"last_date\"]] = ticker_mapper[[\"first_date\", \"last_date\"]].apply(pd.to_datetime, axis=0)\n",
    "\n",
    "    ticker_mapper_consolidated = ticker_mapper.copy(deep=True)\n",
    "    ticker_mapper_consolidated[\"is_primary_ticker\"] = False\n",
    "    ticker_mapper_consolidated = ticker_mapper_consolidated.groupby(\"company_name\", as_index=False).apply(consolidate_tickers)\n",
    "    print(f\"{ticker_mapper_consolidated.shape[0]} entries before consolidation. {ticker_mapper_consolidated[ticker_mapper_consolidated.is_primary_ticker].shape} entries after.\")\n",
    "    ticker_mapper_consolidated.to_parquet(\"data_shared/ticker_name_mapper_consolidated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_mapper_consolidated = pd.read_parquet(\"data_shared/ticker_name_mapper_consolidated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite tickers with consolidated ticker, i.e. the ticker of the time series we use to construct input-output pairs\n",
    "news[\"stocks\"] = news.stocks.map(lambda ticker: get_primary_ticker(ticker, mapper=ticker_mapper_consolidated))\n",
    "# Some tickers don't exist, they will be converted to NaNs\n",
    "news = news.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index              3418496\n",
       "time               3418496\n",
       "stocks             3418496\n",
       "parsed_body     1939337930\n",
       "entry_time         3418496\n",
       "nn_exit_time       3418496\n",
       "dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge News with Price Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Also merge with  non-adjusted prices. We don't trade penny stocks.\n",
    "# If the price is smaller than 1 when the news come out we don't trade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy: pd.DataFrame = pd.read_parquet(path=f\"{config.data.iqfeed.minute.cleaned}/SPY_1min.parquet\")\n",
    "spy.columns = [x.strip(\"adj_\") for x in spy.columns]\n",
    "spy.columns = [f\"SPY_{x}\" for x in spy.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ticker_mapper_consolidated' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m unique_tickers \u001b[38;5;241m=\u001b[39m \u001b[43mticker_mapper_consolidated\u001b[49m\u001b[38;5;241m.\u001b[39mstocks[ticker_mapper_consolidated\u001b[38;5;241m.\u001b[39mis_primary_ticker]\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ticker_mapper_consolidated' is not defined"
     ]
    }
   ],
   "source": [
    "unique_tickers = ticker_mapper_consolidated.stocks[ticker_mapper_consolidated.is_primary_ticker].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique_tickers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, ticker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43munique_tickers\u001b[49m):\n\u001b[0;32m      2\u001b[0m     clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'unique_tickers' is not defined"
     ]
    }
   ],
   "source": [
    "for i, ticker in enumerate(unique_tickers):\n",
    "    clear_output(wait=True)\n",
    "    print(f\"{i} - {ticker}\", flush=True)\n",
    "\n",
    "    ticker_news = news.loc[news.stocks == ticker, :].reset_index()\n",
    "\n",
    "    prices: pd.DataFrame = pd.read_parquet(path=f\"{config.data.iqfeed.minute.cleaned}/{ticker}_1min.parquet\")\n",
    "    prices.columns = [x.strip(\"adj_\") for x in prices.columns]\n",
    "    prices = prices.sort_values(\"time\")\n",
    "\n",
    "    # We generally neeed to use `merge_asof` here instad of simple `merge`, because\n",
    "    # Sometimes no auction occurred or was recorded at 16:00 or things of this sort.\n",
    "    \n",
    "    # Left key must be sorted\n",
    "    ticker_news.sort_values(\"entry_time\", inplace=True)\n",
    "    merged = pd.merge_asof(ticker_news, prices, left_on=\"entry_time\", right_on=\"time\", direction=\"forward\")\n",
    "    merged.sort_values(\"nn_exit_time\", inplace=True)\n",
    "    merged = pd.merge_asof(merged, prices, left_on=\"nn_exit_time\", right_on=\"time\", suffixes=(\"_entry\", \"_exit\"), direction=\"backward\")\n",
    "    # We use the O part of the OHLC for intra day candles here for convenienece as well\n",
    "    merged[\"r\"] = merged[\"open_exit\"] / merged[\"open_entry\"] - 1\n",
    "\n",
    "    # Ideally we do this for every stock first and then we come back with the complete dataframe... (depends on if it fits in memory)\n",
    "    # Merge news and stock prices with spy prices\n",
    "    merged.sort_values(\"entry_time\", inplace=True)\n",
    "    merged = pd.merge_asof(merged, spy, left_on=\"entry_time\", right_on=\"time\", direction=\"forward\")\n",
    "\n",
    "    # TODO: Don't use intraday as exit here (closing candle) but the actual closing auction...\n",
    "    # But for that we need the daily time series, not with minute frequency\n",
    "    merged.sort_values(\"nn_exit_time\", inplace=True)\n",
    "    merged = pd.merge_asof(merged, spy, left_on=\"nn_exit_time\", right_on=\"time\", suffixes=(\"_entry\", \"_exit\"), direction=\"backward\")\n",
    "\n",
    "    # Calculate to potentially filter out penny stocks later on\n",
    "    merged[\"unadj_entry_open\"] = merged[\"open_entry\"] / merged[\"cum_split_ratio_entry\"]\n",
    "    \n",
    "    merged[\"r_spy\"] = merged[\"SPY_close_exit\"] / merged[\"SPY_close_entry\"] - 1\n",
    "\n",
    "    merged.set_index(\"index\", inplace=True)\n",
    "    \n",
    "    keep_columns = [\"entry_time\", \"nn_exit_time\", \"r\", \"unadj_entry_open\", \"r_spy\"]\n",
    "    news.loc[merged.index, keep_columns] = merged.loc[:, keep_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>stocks</th>\n",
       "      <th>company_name</th>\n",
       "      <th>short_name</th>\n",
       "      <th>first_date</th>\n",
       "      <th>last_date</th>\n",
       "      <th>is_primary_ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <th>74475</th>\n",
       "      <td>ALV</td>\n",
       "      <td>Autoliv, Inc.</td>\n",
       "      <td>Autoliv</td>\n",
       "      <td>2010-01-04 09:31:00-05:00</td>\n",
       "      <td>2023-12-15 16:01:00-05:00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          stocks   company_name short_name                first_date  \\\n",
       "778 74475    ALV  Autoliv, Inc.    Autoliv 2010-01-04 09:31:00-05:00   \n",
       "\n",
       "                          last_date  is_primary_ticker  \n",
       "778 74475 2023-12-15 16:01:00-05:00               True  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_mapper_consolidated[ticker_mapper_consolidated.stocks == \"ALV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "427312 news before. 419094 news after dropping NaNs.NaNs should occurr, when we don't have a price time series when news occurred.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{news.shape[0]} news before. {news.dropna().shape[0]} news after dropping NaNs.\"\n",
    "      f\"NaNs should occurr, when we don't have a price time series when news occurred.\")\n",
    "news = news.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set training, validation and test set indices\n",
    "news[\"split\"] = \"training\"\n",
    "news.loc[news.time >= config.model.data.val_cutoff_date, \"split\"] = \"validation\"\n",
    "news.loc[news.time >= config.model.data.test_cutoff_date, \"split\"] = \"testing\"\n",
    "news[\"split\"] = news[\"split\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALTERNATIVELY Set training, validation and test set indices\n",
    "N = news.shape[0]\n",
    "news[\"split\"] = \"training\"\n",
    "news.iloc[int(N * 0.7):, : ].loc[:, \"split\"] = \"validation\"\n",
    "news.iloc[int(N * 0.9):, : ].loc[:, \"split\"] = \"testing\"\n",
    "news[\"split\"] = news[\"split\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293365 samples in training set.\n",
      " 83819 samples in validation set.\n",
      " 0 samples in testing set.\n"
     ]
    }
   ],
   "source": [
    "train_N = news[news[\"split\"] == \"training\"].shape[0]\n",
    "valid_N = news[news[\"split\"] == \"validation\"].shape[0]\n",
    "test_N = news[news[\"split\"] == \"testing\"].shape[0]\n",
    "\n",
    "print(f\"{train_N} samples in training set.\"\n",
    "      f\"\\n {valid_N} samples in validation set.\"\n",
    "      f\"\\n {test_N} samples in testing set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Disk\n",
    "news.to_parquet(config.data.merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## Merge with Daily Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.tickers import get_tickers\n",
    "tickers = get_tickers(config.data.iqfeed.daily.cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_parquet(path=config.data.merged)\n",
    "dataset[[\"std_252\", \"dollar_volume\", 'r_intra_(t-1)', 'unadj_open']] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = [\"std_252\", \"dollar_volume\", 'r_intra_(t-1)', 'unadj_open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = tickers[0]\n",
    "prices = pd.read_parquet(path=f\"{config.data.iqfeed.daily.cleaned}/{ticker}_daily.parquet\")\n",
    "prices.index = prices.index.tz_localize(\"US/Eastern\")\n",
    "ticker_dat = dataset.loc[dataset.stocks == ticker, :].reset_index()\n",
    "merged = pd.merge_asof(ticker_dat, prices[indicators], left_on=\"entry_time\", right_on=\"date\", direction=\"backward\")\n",
    "merged.set_index(\"index\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ticker in enumerate(tickers):\n",
    "    clear_output(wait=True)\n",
    "    print(f\"{i} - {ticker}\", flush=True)\n",
    "    \n",
    "    prices = pd.read_parquet(path=f\"{config.data.iqfeed.daily.cleaned}/{ticker}_daily.parquet\")\n",
    "    prices.index = prices.index.tz_localize(\"US/Eastern\")\n",
    "    ticker_dat = dataset.loc[dataset.stocks == ticker, :].reset_index().drop(columns=indicators)\n",
    "    ticker_dat.sort_values(\"entry_time\", inplace=True)\n",
    "    merged = pd.merge_asof(ticker_dat, prices[indicators], left_on=\"entry_time\", right_on=\"date\", direction=\"backward\")\n",
    "    merged.set_index(\"index\", inplace=True)\n",
    "    dataset.loc[merged.index, indicators] = merged[indicators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_parquet(path=config.data.merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_parquet(path=config.data.merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'SPY_close_exit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mg:\\Meine Ablage\\NewsTrading\\trading_bot\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3651\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mg:\\Meine Ablage\\NewsTrading\\trading_bot\\.venv\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mg:\\Meine Ablage\\NewsTrading\\trading_bot\\.venv\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'SPY_close_exit'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dat\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr_spy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdat\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSPY_close_exit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m/\u001b[39m dat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPY_close_entry\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      2\u001b[0m dat\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr_mkt_adj\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m  dat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m dat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr_spy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m dat\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz_score\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr_mkt_adj\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m dat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd_252\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mg:\\Meine Ablage\\NewsTrading\\trading_bot\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mg:\\Meine Ablage\\NewsTrading\\trading_bot\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3655\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3656\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'SPY_close_exit'"
     ]
    }
   ],
   "source": [
    "dat.loc[:, \"r_mkt_adj\"] =  dat[\"r\"] - dat[\"r_spy\"]\n",
    "dat.loc[:, \"z_score\"] = dat[\"r_mkt_adj\"] / dat[\"std_252\"]\n",
    "# dat.loc[:, \"z_score_label\"] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.to_parquet(path=config.data.merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
