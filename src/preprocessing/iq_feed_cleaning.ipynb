{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adrianbeer/trading_bot/blob/main/src/preprocessing/iq_feed_cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tU-UtmILfEI6"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "cwd=\"/content/drive/MyDrive/NewsTrading/trading_bot\"\n",
        "%cd /content/drive/MyDrive/NewsTrading/trading_bot\n",
        "%pip install -r requirements_clean.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IovrlVPZi2i5"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5w_3GBj2fBkB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import plotly.express as px\n",
        "import pytz\n",
        "eastern = pytz.timezone('US/Eastern')\n",
        "import yfinance\n",
        "\n",
        "from dotmap import DotMap\n",
        "import yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0FoLjE8cy_04"
      },
      "outputs": [],
      "source": [
        "config_name = \"src/config_gcs.yaml\"\n",
        "config = DotMap(yaml.safe_load(open(config_name)), _dynamic=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QVtb04smfBkE"
      },
      "outputs": [],
      "source": [
        "def filter_trading_hours(df, time_column):\n",
        "    T = df[time_column].dt\n",
        "    min_mask = (T.hour >= 10) | ((T.hour == 9) & (T.minute >= 31))\n",
        "    max_mask = (T.hour < 16) | ((T.hour == 16) & (T.minute <= 1))\n",
        "    return df.loc[min_mask & max_mask, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gAx2fHuMfBkF"
      },
      "outputs": [],
      "source": [
        "def preprocess_iq_feed_prices(prices: pd.DataFrame) -> pd.DataFrame:\n",
        "    if \"time\" in prices.columns:\n",
        "        # Intra-day data\n",
        "        prices.loc[:, \"time\"] = prices.loc[:, \"time\"].dt.tz_localize(None)\n",
        "        prices.loc[:, \"time\"] = prices.loc[:, \"time\"].dt.tz_localize(eastern)\n",
        "        prices.drop_duplicates(keep=\"first\", inplace=True)\n",
        "        prices.dropna(inplace=True)\n",
        "\n",
        "        prices = filter_trading_hours(df=prices, time_column=\"time\")\n",
        "\n",
        "        # Deals with duplicate rows which occurr when not all the digits for volume are\n",
        "        # correctly entered, but only the first 1-3. So keep the largest.\n",
        "        prices = prices.sort_values([\"time\", \"volume\"], ascending=[True, False])\n",
        "        prices = prices.drop_duplicates(subset=[\"time\"], keep=\"first\")\n",
        "\n",
        "        prices.set_index(\"time\", inplace=True)\n",
        "        prices.sort_index(ascending=True, inplace=True)\n",
        "        assert prices.index.is_unique\n",
        "        prices.index = prices.index.astype('datetime64[ns, US/Eastern]')\n",
        "    else:\n",
        "        # Daily data\n",
        "        prices.dropna(inplace=True)\n",
        "        prices[\"date\"] = pd.to_datetime(prices.date)\n",
        "    return prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Qk56dd8BfBkH"
      },
      "outputs": [],
      "source": [
        "def calc_backward_adjustment_factors(ticker: str, dataframe: bool =False):\n",
        "    \"\"\"Calculates the backward adjust factors based on data from yfinance.\n",
        "\n",
        "    `cum_split_ratio` and `backward_adjustment_factor` are synonymous.\n",
        "    IMPORTANT !!!!!!!!!!\n",
        "    In yahoo finance the `Close` is adj. for splits and the `Adj. Close` for splits and dividends\n",
        "    Dividends on Yahoo Finance are adjusted for splits!\n",
        "    \"\"\"\n",
        "    df = yfinance.download(ticker, period=\"14y\",actions=True)\n",
        "    df.sort_index(ascending=True, inplace=True)\n",
        "    df[\"split_ratio\"] = 1\n",
        "\n",
        "    # Add splits\n",
        "    df[\"Stock Splits\"] = df[\"Stock Splits\"].shift(-1, fill_value=1)\n",
        "    split_mask = df[\"Stock Splits\"] > 0\n",
        "    df.loc[split_mask, \"split_ratio\"] = 1 / df[\"Stock Splits\"]\n",
        "\n",
        "    # Add dividends\n",
        "    dividend_mask = (df[\"Dividends\"] > 0)\n",
        "    # The Close is split, but not dividend adjusted\n",
        "    df.loc[dividend_mask, \"split_ratio\"] = (1 - df.loc[dividend_mask, \"Dividends\"].values / df.shift(1).loc[dividend_mask, \"Close\"].values)\n",
        "\n",
        "    df[\"cum_split_ratio\"] = np.cumprod(df[\"split_ratio\"][::-1])[::-1]\n",
        "    if dataframe:\n",
        "      return df\n",
        "    else:\n",
        "      return df[\"cum_split_ratio\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "QlydIlm_wApZ"
      },
      "outputs": [],
      "source": [
        "def get_gcs_tickers():\n",
        "  from google.cloud import storage\n",
        "  storage_client = storage.Client()\n",
        "  bucket = config.data.iqfeed.minute.raw.split(\"/\")[2]\n",
        "  prefix = \"/\".join(config.data.iqfeed.minute.raw.split(\"/\")[3:]) + \"/\"\n",
        "  bucket = storage_client.get_bucket(bucket)\n",
        "  blobs = bucket.list_blobs(prefix=prefix)\n",
        "  tickers = [subpath.name.split(\"/\")[-1].split(\"_\")[0] for subpath in blobs]\n",
        "  tickers = [t for t in tickers if t != '']\n",
        "  return tickers\n",
        "\n",
        "def get_local_tickers():\n",
        "  onlyfiles = [f for f in listdir(config.data.iqfeed.minute.raw) if isfile(join(config.data.iqfeed.minute.raw, f))]\n",
        "  tickers = [x.split(\"_\")[0] for x in onlyfiles]\n",
        "  return tickers\n",
        "\n",
        "def get_tickers():\n",
        "  if config.environment == \"colab\":\n",
        "    tickers = get_gcs_tickers()\n",
        "  if config.environment == \"local\":\n",
        "    tickers = get_local_tickers()\n",
        "  return tickers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSYCrTSyfBkJ"
      },
      "outputs": [],
      "source": [
        "def calc_adj_prices(prices, bafs):\n",
        "    X = bafs.index.tz_localize(\"US/Eastern\")\n",
        "    # danger: this was passed as reference, not valuee.\n",
        "    bafs.index = X  + pd.DateOffset(hours=16, minutes=1)\n",
        "    prices_adj = pd.merge_asof(prices, bafs, left_index=True, right_on=\"Date\", direction=\"forward\")\n",
        "    prices_adj.loc[:, [\"open\", \"high\", \"low\", \"close\"]] = prices_adj[[\"open\", \"high\", \"low\", \"close\"]].mul(prices_adj.cum_split_ratio, axis=\"index\")\n",
        "    prices_adj.loc[:, \"volume\"] = prices_adj[\"volume\"].div(prices_adj.cum_split_ratio, axis=\"index\")\n",
        "    prices_adj.drop(columns=\"Date\", inplace=True)\n",
        "    prices_adj.rename(columns=dict(\n",
        "        zip([\"open\", \"high\", \"low\", \"close\", \"volume\"],\n",
        "            [f\"adj_{x}\" for x in [\"open\", \"high\", \"low\", \"close\", \"volume\"]])\n",
        "        ),\n",
        "                      inplace=True)\n",
        "    return prices_adj"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tickers = get_tickers()"
      ],
      "metadata": {
        "id": "xy6pi2w2-z34"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tickers"
      ],
      "metadata": {
        "id": "oa7ylAtBAD94",
        "outputId": "4c88f576-31f0-4896-bec4-8fdc146a4ec8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['AABB', 'AA', 'A']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "D0FYXGEYfBkG",
        "outputId": "0f4c2f3f-a0db-4f8f-a5b8-85b6d121275b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AABB\n",
            "gs://extreme-lore-398917-bzg/iqfeed/raw/AABB_1min.parquet\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ArrowInvalid",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-35f9a0620d5d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{config.data.iqfeed.minute.raw}/{ticker}_1min.parquet\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   prices = pd.read_parquet(path=path,\n\u001b[0m\u001b[1;32m      6\u001b[0m                            columns=[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n\u001b[1;32m      7\u001b[0m   \u001b[0mprices\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_iq_feed_prices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     return impl.read(\n\u001b[0m\u001b[1;32m    504\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m         )\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             result = self.api.parquet.read_table(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             ).to_pandas(**to_pandas_kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, memory_map, read_dictionary, filesystem, filters, buffer_size, partitioning, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[1;32m   2869\u001b[0m             )\n\u001b[1;32m   2870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2871\u001b[0;31m         return dataset.read(columns=columns, use_threads=use_threads,\n\u001b[0m\u001b[1;32m   2872\u001b[0m                             use_pandas_metadata=use_pandas_metadata)\n\u001b[1;32m   2873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[1;32m   2515\u001b[0m                 )\n\u001b[1;32m   2516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2517\u001b[0;31m         table = self._dataset.to_table(\n\u001b[0m\u001b[1;32m   2518\u001b[0m             \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filter_expression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2519\u001b[0m             \u001b[0muse_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_threads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/_dataset.pyx\u001b[0m in \u001b[0;36mpyarrow._dataset.Dataset.to_table\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/_dataset.pyx\u001b[0m in \u001b[0;36mpyarrow._dataset.Dataset.scanner\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/_dataset.pyx\u001b[0m in \u001b[0;36mpyarrow._dataset.Scanner.from_dataset\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/_dataset.pyx\u001b[0m in \u001b[0;36mpyarrow._dataset._populate_builder\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mArrowInvalid\u001b[0m: No match for FieldRef.Name(open) in adj_open: double\nadj_high: double\nadj_low: double\nadj_close: double\nadj_volume: double\ncum_split_ratio: double\ntime: timestamp[us, tz=US/Eastern]\n__fragment_index: int32\n__batch_index: int32\n__last_in_fragment: bool\n__filename: string"
          ]
        }
      ],
      "source": [
        "for ticker in tickers:\n",
        "  print(ticker)\n",
        "  path = f\"{config.data.iqfeed.minute.raw}/{ticker}_1min.parquet\"\n",
        "  prices = pd.read_parquet(path=path,\n",
        "                           columns=[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
        "  prices: pd.DataFrame = preprocess_iq_feed_prices(prices)\n",
        "  # Adjust for Splits and Dividends\n",
        "  bafs = calc_backward_adjustment_factors(ticker, dataframe=False)\n",
        "  adj_prices = calc_adj_prices(prices, bafs)\n",
        "\n",
        "  # Save adjusted files to disk\n",
        "  adj_prices.to_parquet(path=f\"gs://extreme-lore-398917-bzg/iqfeed/cleaned/{ticker}_1min.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hcixcnMfBkI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHSaVNfHfBkK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}