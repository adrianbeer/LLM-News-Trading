{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import plotly.express as px\n",
    "import pytz\n",
    "eastern = pytz.timezone('US/Eastern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List off functions for modifying the return\n",
    "raw_iq_feed_data_dir = \"D:/IQFeedData\"\n",
    "\n",
    "# Input-Output-Rows for the neural network training and validation\n",
    "train_output_dir = \"D:/Data/NN_Training\"\n",
    "\n",
    "# Input-Rows for testing\n",
    "test_output_dir = \"D:/Data/NN_Testing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_trading_hours(df, time_column):\n",
    "    T = df[time_column].dt\n",
    "    min_mask = (T.hour >= 10) | ((T.hour == 9) & (T.minute >= 31))\n",
    "    max_mask = (T.hour < 16) | ((T.hour == 16) & (T.minute <= 1))\n",
    "    return df.loc[min_mask & max_mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_iq_feed_prices(prices: pd.DataFrame) -> pd.DataFrame: \n",
    "    if \"time\" in prices.columns:\n",
    "        # Intra-day data\n",
    "        prices.loc[:, \"time\"] = prices.loc[:, \"time\"].dt.tz_localize(None)\n",
    "        prices.loc[:, \"time\"] = prices.loc[:, \"time\"].dt.tz_localize(eastern)\n",
    "        prices.drop_duplicates(keep=\"first\", inplace=True)\n",
    "        prices.dropna(inplace=True)\n",
    "        \n",
    "        prices = filter_trading_hours(df=prices, time_column=\"time\")\n",
    "\n",
    "        # Deals with duplicate rows which occurr when not all the digits for volume are \n",
    "        # correctly entered, but only the first 1-3. So keep the largest.\n",
    "        prices = prices.sort_values([\"time\", \"volume\"], ascending=[True, False])\n",
    "        prices = prices.drop_duplicates(subset=[\"time\"], keep=\"first\")\n",
    "\n",
    "        prices.set_index(\"time\", inplace=True)\n",
    "        prices.sort_index(ascending=True, inplace=True)\n",
    "        assert prices.index.is_unique\n",
    "    else:\n",
    "        # Daily data\n",
    "        prices.dropna(inplace=True)\n",
    "        prices[\"date\"] = pd.to_datetime(prices.date)\n",
    "    return prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles = [f for f in listdir(raw_iq_feed_data_dir) if isfile(join(raw_iq_feed_data_dir, f))]\n",
    "tickers = [x.split(\"_\")[0] for x in onlyfiles]\n",
    "ticker = \"GOOGL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices: pd.DataFrame = preprocess_iq_feed_prices(pd.read_parquet(path=f\"{raw_iq_feed_data_dir}/{ticker}_1min.parquet\", \n",
    "                                                                 columns=[\"time\", \"close\", \"open\", \"volume\"]))\n",
    "prices_daily: pd.DataFrame = preprocess_iq_feed_prices(pd.read_parquet(path=f\"{raw_iq_feed_data_dir}/daily/{ticker}_daily.parquet\", \n",
    "                                                                       columns=[\"date\", \"close\", \"open\", \"volume\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "intra_opens = prices[(prices.index.hour == 9) & (prices.index.minute == 0), \"open\"]\n",
    "intra_opens.index = intra_opens.index.date\n",
    "intra_opens.index.name = \"date\"\n",
    "\n",
    "intra_closes = prices[(prices.index.hour == 16) & (prices.index.minute == 0), \"open\"]\n",
    "intra_closes.index = intra_closes.index.date\n",
    "intra_closes.index.name = \"date\"\n",
    "intra_closes.name = \"intra_close\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(intra_opens, prices_daily[\"date\", \"close\", \"open\"], left_on=\"date\", right_on=\"date\", suffixes=(\"_intra\", \"_eod\"))\n",
    "merged = pd.merge(merged, intra_closes, left_on=\"date\", right_on=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.sort_index(ascending=False)\n",
    "merged[\"split_ratio\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Dividend and Split Events (They are handled exactly the same, but dividend events are smaller)\n",
    "# To adjust, we go backwrds in time. \n",
    "# When a split has occurred the close will be the first price that is influenced in the historical adjusted time series.\n",
    "# Assuming the split/dividend has occurred overnight.\n",
    "for row in merged:\n",
    "     split_ratio = merged[row, \"close_eod\"] / merged[row, \"close_intra\"]\n",
    "     if np.abs(split_ratio - 1) >= 0.01:\n",
    "         merged[row:, [\"close_intra\", \"split_ratio\"]]  = [merged[row:, \"close_intra\"] * split_ratio, \"split_ratio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices[\"split_ratio\"] = 1\n",
    "prices[merged.index, \"split_ratio\"] = merged[\"split_ratio\"]\n",
    "prices[\"cum_split_ratio\"] = np.cumsum(prices[\"split_ratio\"][::-1])[::-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_ratio < 1 => price goes down\n",
    "prices[[\"open\", \"high\", \"low\", \"close\"]] = prices[[\"open\", \"high\", \"low\", \"close\"]] * prices[\"cum_split_ratio\"]\n",
    "\n",
    "# split_ratio < 1 => volume goes up, since price goes down and pricevolume has to stay the same \n",
    "# (volume denotes number of stocks traded)\n",
    "prices[\"volume\"] = prices[\"volume\"] / prices[\"cum_split_ratio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(merged[\"close_intra\", \"close_eod\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check quality looking at the amount of splits/dividends\n",
    "# Should be a maximum of 5 (one split event and 4 dividends)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check quality looking at the difference between intra_day adj open and eod adj open.\n",
    "# This difference should be smaller than the split_ratio for all days where the split_ratio is not 1\n",
    "# This difference should also be smaller in general than our threshold for splits (1%?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once this has been checked and intraday data consistency/continuity is assured we could\n",
    "# Replace all the closes and opens from the intra day data set with those of the eod data set.\n",
    "# However this leads to problems, if we calculate wrong a single split event.\n",
    "# If we don't do this replacement only one day is affected (split day). \n",
    "# If we do the replacement multiple days will be affected by the error."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
