{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "## Clean and Adjust Intraday Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "tU-UtmILfEI6"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m cwd\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/NewsTrading/trading_bot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "cwd=\"/content/drive/MyDrive/NewsTrading/trading_bot\"\n",
        "%cd /content/drive/MyDrive/NewsTrading/trading_bot\n",
        "%pip install -r requirements_clean.txt\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5w_3GBj2fBkB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import plotly.express as px\n",
        "import pytz\n",
        "eastern = pytz.timezone('US/Eastern')\n",
        "import yfinance\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from dotmap import DotMap\n",
        "import yaml\n",
        "from src.config import config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QVtb04smfBkE"
      },
      "outputs": [],
      "source": [
        "def filter_trading_hours(df, time_column):\n",
        "    T = df[time_column].dt\n",
        "    min_mask = (T.hour >= 10) | ((T.hour == 9) & (T.minute >= 31))\n",
        "    max_mask = (T.hour < 16) | ((T.hour == 16) & (T.minute <= 1))\n",
        "    return df.loc[min_mask & max_mask, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gAx2fHuMfBkF"
      },
      "outputs": [],
      "source": [
        "def preprocess_iq_feed_prices(prices: pd.DataFrame) -> pd.DataFrame:\n",
        "    if \"time\" in prices.columns:\n",
        "        # Intra-day data\n",
        "        prices.loc[:, \"time\"] = prices.loc[:, \"time\"].dt.tz_localize(None)\n",
        "        prices.loc[:, \"time\"] = prices.loc[:, \"time\"].dt.tz_localize(eastern)\n",
        "        prices.drop_duplicates(keep=\"first\", inplace=True)\n",
        "        prices.dropna(inplace=True)\n",
        "\n",
        "        prices = filter_trading_hours(df=prices, time_column=\"time\")\n",
        "\n",
        "        # Deals with duplicate rows which occurr when not all the digits for volume are\n",
        "        # correctly entered, but only the first 1-3. So keep the largest.\n",
        "        prices = prices.sort_values([\"time\", \"volume\"], ascending=[True, False])\n",
        "        prices = prices.drop_duplicates(subset=[\"time\"], keep=\"first\")\n",
        "\n",
        "        prices.set_index(\"time\", inplace=True)\n",
        "        prices.sort_index(ascending=True, inplace=True)\n",
        "        assert prices.index.is_unique\n",
        "        prices.index = prices.index.astype('datetime64[ns, US/Eastern]')\n",
        "    else:\n",
        "        # Daily data\n",
        "        prices.dropna(inplace=True)\n",
        "        prices[\"date\"] = pd.to_datetime(prices.date)\n",
        "    return prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Qk56dd8BfBkH"
      },
      "outputs": [],
      "source": [
        "def calc_backward_adjustment_factors(ticker: str, dataframe: bool =False):\n",
        "    \"\"\"Calculates the backward adjust factors based on data from yfinance.\n",
        "\n",
        "    `cum_split_ratio` and `backward_adjustment_factor` are synonymous.\n",
        "    IMPORTANT !!!!!!!!!!\n",
        "    In yahoo finance the `Close` is adj. for splits and the `Adj. Close` for splits and dividends\n",
        "    Dividends on Yahoo Finance are adjusted for splits!\n",
        "    \"\"\"\n",
        "    df = yfinance.download(ticker, period=\"max\",actions=True)\n",
        "    if ticker in yfinance.shared._ERRORS:\n",
        "      return None\n",
        "    df.sort_index(ascending=True, inplace=True)\n",
        "    df[\"split_ratio\"] = 1\n",
        "\n",
        "    # Add splits\n",
        "    df[\"Stock Splits\"] = df[\"Stock Splits\"].shift(-1, fill_value=1)\n",
        "    split_mask = df[\"Stock Splits\"] > 0\n",
        "    df.loc[split_mask, \"split_ratio\"] = 1 / df[\"Stock Splits\"]\n",
        "\n",
        "    # Add dividends\n",
        "    dividend_mask = (df[\"Dividends\"] > 0)\n",
        "    # The Close is split, but not dividend adjusted\n",
        "    df.loc[dividend_mask, \"split_ratio\"] = (1 - df.loc[dividend_mask, \"Dividends\"].values / df.shift(1).loc[dividend_mask, \"Close\"].values)\n",
        "\n",
        "    df[\"cum_split_ratio\"] = np.cumprod(df[\"split_ratio\"][::-1])[::-1]\n",
        "    if dataframe:\n",
        "      return df\n",
        "    else:\n",
        "      return df[\"cum_split_ratio\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QlydIlm_wApZ"
      },
      "outputs": [],
      "source": [
        "def get_gcs_tickers():\n",
        "  from google.cloud import storage\n",
        "  storage_client = storage.Client()\n",
        "  bucket = config.data.iqfeed.minute.raw.split(\"/\")[2]\n",
        "  prefix = \"/\".join(config.data.iqfeed.minute.raw.split(\"/\")[3:]) + \"/\"\n",
        "  bucket = storage_client.get_bucket(bucket)\n",
        "  blobs = bucket.list_blobs(prefix=prefix)\n",
        "  tickers = [subpath.name.split(\"/\")[-1].split(\"_\")[0] for subpath in blobs]\n",
        "  tickers = [t for t in tickers if t != '']\n",
        "  return tickers\n",
        "\n",
        "def get_local_tickers():\n",
        "  onlyfiles = [f for f in listdir(config.data.iqfeed.minute.raw) if isfile(join(config.data.iqfeed.minute.raw, f))]\n",
        "  tickers = [x.split(\"_\")[0] for x in onlyfiles]\n",
        "  return tickers\n",
        "\n",
        "def get_tickers():\n",
        "  if config.environment == \"colab\":\n",
        "    tickers = get_gcs_tickers()\n",
        "  if config.environment == \"local\":\n",
        "    tickers = get_local_tickers()\n",
        "  return tickers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BSYCrTSyfBkJ"
      },
      "outputs": [],
      "source": [
        "def calc_adj_prices(prices, bafs):\n",
        "    X = bafs.index.tz_localize(\"US/Eastern\")\n",
        "    # danger: this was passed as reference, not valuee.\n",
        "    bafs.index = X  + pd.DateOffset(hours=16, minutes=1)\n",
        "    prices_adj = pd.merge_asof(prices, bafs, left_index=True, right_on=\"Date\", direction=\"forward\")\n",
        "    prices_adj.loc[:, [\"open\", \"high\", \"low\", \"close\"]] = prices_adj[[\"open\", \"high\", \"low\", \"close\"]].mul(prices_adj.cum_split_ratio, axis=\"index\")\n",
        "    prices_adj.loc[:, \"volume\"] = prices_adj[\"volume\"].div(prices_adj.cum_split_ratio, axis=\"index\")\n",
        "    prices_adj.drop(columns=\"Date\", inplace=True)\n",
        "    prices_adj.rename(columns=dict(\n",
        "        zip([\"open\", \"high\", \"low\", \"close\", \"volume\"],\n",
        "            [f\"adj_{x}\" for x in [\"open\", \"high\", \"low\", \"close\", \"volume\"]])\n",
        "        ),\n",
        "                      inplace=True)\n",
        "    return prices_adj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xy6pi2w2-z34"
      },
      "outputs": [],
      "source": [
        "tickers = get_tickers()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0FYXGEYfBkG",
        "outputId": "f5156aa5-6651-4714-c90c-4ccbcd940aa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15 - ABEV\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(tickers)):\n",
        "  ticker = tickers[i]\n",
        "  clear_output(wait=True)\n",
        "  print(f\"{i} - {ticker}\", flush=True)\n",
        "\n",
        "  path = f\"{config.data.iqfeed.minute.raw}/{ticker}_1min.parquet\"\n",
        "  prices = pd.read_parquet(path=path,\n",
        "                           columns=[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
        "  prices: pd.DataFrame = preprocess_iq_feed_prices(prices)\n",
        "  # Adjust for Splits and Dividends\n",
        "  bafs = calc_backward_adjustment_factors(ticker, dataframe=False)\n",
        "  if bafs is None:\n",
        "    # No yfinance data for this stock\n",
        "    continue\n",
        "  adj_prices = calc_adj_prices(prices, bafs)\n",
        "\n",
        "  # Save adjusted files to disk\n",
        "  adj_prices.to_parquet(path=f\"{config.data.iqfeed.minute.cleaned}/{ticker}_1min.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hcixcnMfBkI"
      },
      "source": [
        "## Make Daily Time Series from Intra-Day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def downsample_intraday_prices_to_daily(df: pd.DataFrame):\n",
        "    dic = dict()\n",
        "    dic[\"adj_open\"] = df.iloc[0, :].at[\"adj_open\"]\n",
        "    dic[\"adj_high\"] = df[\"adj_high\"].max()\n",
        "    dic[\"adj_high\"] = df[\"adj_low\"].min()\n",
        "    dic[\"adj_close\"] = df.iloc[-1, :].at[\"adj_close\"]\n",
        "    dic[\"adj_volume\"] = df.loc[:, \"adj_volume\"].mean()\n",
        "    dic[\"cum_split_ratio\"] = df.at[df.index[0], \"cum_split_ratio\"]\n",
        "    \n",
        "    daily = pd.Series(dic, name=df.index.date[0])\n",
        "    return daily"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "onlyfiles = [f for f in listdir(config.data.iqfeed.minute.cleaned) if isfile(join(config.data.iqfeed.minute.cleaned, f))]\n",
        "tickers = [x.split(\"_\")[0] for x in onlyfiles]\n",
        "tickers = np.sort(tickers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def downsample_intraday_prices_to_daily_for_ticker_and_save(ticker: str):\n",
        "    prices = pd.read_parquet(path=f\"{config.data.iqfeed.minute.cleaned}/{ticker}_1min.parquet\")\n",
        "    assert prices.index.is_monotonic_increasing\n",
        "    daily_prices = prices.groupby(prices.index.date).apply(downsample_intraday_prices_to_daily)\n",
        "    daily_prices.to_parquet(path=f\"{config.data.iqfeed.daily.cleaned}/{ticker}_daily.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "pool_obj = ThreadPoolExecutor(max_workers=os.cpu_count()-1)\n",
        "ans = pool_obj.map(downsample_intraday_prices_to_daily_for_ticker_and_save, tickers)\n",
        "result = list(ans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
