{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkovyr-oc4-K"
      },
      "source": [
        "# Benzinga-Nachrichten-Verarbeitung"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A8GOctQmMfj"
      },
      "source": [
        "## Setup up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxfEWak8MNGE"
      },
      "source": [
        "### Cluster spin up"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "cwd=\"/content/drive/MyDrive/NewsTrading/trading_bot\"\n",
        "%cd /content/drive/MyDrive/NewsTrading/trading_bot\n",
        "%pip install -r requirements_clean.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pmVviAjMTaQ",
        "outputId": "09531355-3d92-4489-fcdc-636d80a51074"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/NewsTrading/trading_bot\n",
            "Requirement already satisfied: html2text in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 4)) (2020.1.16)\n",
            "Requirement already satisfied: datefinder in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 5)) (0.7.3)\n",
            "Requirement already satisfied: dask[complete] in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 6)) (2023.8.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 7)) (3.8.1)\n",
            "Requirement already satisfied: dateparser in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 8)) (1.2.0)\n",
            "Requirement already satisfied: sutime in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 9)) (1.0.1)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 10)) (0.2.33)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 11)) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 12)) (1.2.2)\n",
            "Requirement already satisfied: jupyter-server-proxy in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 15)) (4.1.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 16)) (7.0.3)\n",
            "Requirement already satisfied: fsspec==2023.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 19)) (2023.4.0)\n",
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 20)) (2023.4.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 24)) (4.35.2)\n",
            "Requirement already satisfied: regex>=2017.02.08 in /usr/local/lib/python3.10/dist-packages (from datefinder->-r requirements_clean.txt (line 5)) (2023.6.3)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from datefinder->-r requirements_clean.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from datefinder->-r requirements_clean.txt (line 5)) (2023.3.post1)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]->-r requirements_clean.txt (line 6)) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]->-r requirements_clean.txt (line 6)) (2.2.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]->-r requirements_clean.txt (line 6)) (23.2)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]->-r requirements_clean.txt (line 6)) (1.4.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask[complete]->-r requirements_clean.txt (line 6)) (6.0.1)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]->-r requirements_clean.txt (line 6)) (0.12.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]->-r requirements_clean.txt (line 6)) (7.0.0)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]->-r requirements_clean.txt (line 6)) (10.0.1)\n",
            "Requirement already satisfied: lz4>=4.3.2 in /usr/local/lib/python3.10/dist-packages (from dask[complete]->-r requirements_clean.txt (line 6)) (4.3.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements_clean.txt (line 7)) (1.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements_clean.txt (line 7)) (4.66.1)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser->-r requirements_clean.txt (line 8)) (5.2)\n",
            "Requirement already satisfied: JPype1<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from sutime->-r requirements_clean.txt (line 9)) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from yfinance->-r requirements_clean.txt (line 10)) (1.23.5)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from yfinance->-r requirements_clean.txt (line 10)) (2.31.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance->-r requirements_clean.txt (line 10)) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance->-r requirements_clean.txt (line 10)) (4.9.3)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from yfinance->-r requirements_clean.txt (line 10)) (1.4.4)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance->-r requirements_clean.txt (line 10)) (2.3.10)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.10/dist-packages (from yfinance->-r requirements_clean.txt (line 10)) (3.17.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance->-r requirements_clean.txt (line 10)) (4.11.2)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance->-r requirements_clean.txt (line 10)) (1.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements_clean.txt (line 12)) (1.11.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements_clean.txt (line 12)) (3.2.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from jupyter-server-proxy->-r requirements_clean.txt (line 15)) (3.9.1)\n",
            "Requirement already satisfied: jupyter-server>=1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server-proxy->-r requirements_clean.txt (line 15)) (1.24.0)\n",
            "Requirement already satisfied: simpervisor>=1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server-proxy->-r requirements_clean.txt (line 15)) (1.0.0)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs->-r requirements_clean.txt (line 20)) (4.4.2)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs->-r requirements_clean.txt (line 20)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (from gcsfs->-r requirements_clean.txt (line 20)) (1.2.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (from gcsfs->-r requirements_clean.txt (line 20)) (2.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements_clean.txt (line 24)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements_clean.txt (line 24)) (0.17.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements_clean.txt (line 24)) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements_clean.txt (line 24)) (0.4.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance->-r requirements_clean.txt (line 10)) (2.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs->-r requirements_clean.txt (line 20)) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs->-r requirements_clean.txt (line 20)) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs->-r requirements_clean.txt (line 20)) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs->-r requirements_clean.txt (line 20)) (4.9)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance->-r requirements_clean.txt (line 10)) (0.5.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers->-r requirements_clean.txt (line 24)) (4.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask[complete]->-r requirements_clean.txt (line 6)) (3.17.0)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (3.7.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (23.1.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (3.1.2)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (6.1.12)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (5.5.1)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (6.5.4)\n",
            "Requirement already satisfied: nbformat>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (5.9.2)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (0.19.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (23.2.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (0.18.0)\n",
            "Requirement already satisfied: tornado>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (6.3.2)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (5.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (1.7.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.2.0->dask[complete]->-r requirements_clean.txt (line 6)) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance->-r requirements_clean.txt (line 10)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance->-r requirements_clean.txt (line 10)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance->-r requirements_clean.txt (line 10)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance->-r requirements_clean.txt (line 10)) (2023.11.17)\n",
            "Requirement already satisfied: bokeh>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from dask[complete]->-r requirements_clean.txt (line 6)) (3.3.2)\n",
            "Requirement already satisfied: distributed==2023.8.1 in /usr/local/lib/python3.10/dist-packages (from dask[complete]->-r requirements_clean.txt (line 6)) (2023.8.1)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1->dask[complete]->-r requirements_clean.txt (line 6)) (1.0.7)\n",
            "Requirement already satisfied: psutil>=5.7.2 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1->dask[complete]->-r requirements_clean.txt (line 6)) (5.9.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1->dask[complete]->-r requirements_clean.txt (line 6)) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1->dask[complete]->-r requirements_clean.txt (line 6)) (3.0.0)\n",
            "Requirement already satisfied: zict>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1->dask[complete]->-r requirements_clean.txt (line 6)) (3.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib->gcsfs->-r requirements_clean.txt (line 20)) (1.3.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs->-r requirements_clean.txt (line 20)) (2.11.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs->-r requirements_clean.txt (line 20)) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs->-r requirements_clean.txt (line 20)) (2.7.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1 in /usr/local/lib/python3.10/dist-packages (from bokeh>=2.4.2->dask[complete]->-r requirements_clean.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh>=2.4.2->dask[complete]->-r requirements_clean.txt (line 6)) (9.4.0)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh>=2.4.2->dask[complete]->-r requirements_clean.txt (line 6)) (2023.10.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs->-r requirements_clean.txt (line 20)) (1.62.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs->-r requirements_clean.txt (line 20)) (3.20.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage->gcsfs->-r requirements_clean.txt (line 20)) (1.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (2.1.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (4.1.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (0.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (0.9.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (1.5.0)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (2.16.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.2.0->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (2.19.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.2.0->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (4.19.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs->-r requirements_clean.txt (line 20)) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs->-r requirements_clean.txt (line 20)) (3.2.2)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (21.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (0.15.2)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zj0QodzRMNGE"
      },
      "outputs": [],
      "source": [
        "import sys, io, tarfile, os\n",
        "import re\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "import dask\n",
        "import dask.dataframe as dd\n",
        "from dask.distributed import Client\n",
        "from distributed.diagnostics.plugin import WorkerPlugin\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "from src.preprocessing.news_parser import get_company_abbreviation, yahoo_get_wrapper, \\\n",
        "                                          infer_author, filter_body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w1-9R-RMNGH",
        "outputId": "2cb2d564-b15a-4923-b371-30debefd8e21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.scheduler:State start\n",
            "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:37945\n",
            "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:8787/status\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:38717'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:33459'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:36937'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:35875'\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:46253', name: 1, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:46253\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:44728\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:35055', name: 2, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:35055\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:44692\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:34347', name: 3, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:34347\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:44704\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:46155', name: 0, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:46155\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:44716\n",
            "INFO:distributed.scheduler:Receive client connection: Client-81da973a-a3dd-11ee-810f-0242ac1c000c\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:44740\n"
          ]
        }
      ],
      "source": [
        "from dask.distributed import Client, LocalCluster\n",
        "cluster = LocalCluster()\n",
        "client = Client(cluster)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "hqoPHxpNMNGH",
        "outputId": "1c646639-eb4e-4481-d0b2-0cb9d1084e76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-12-26T10:30:04+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n",
            "ERROR:pyngrok.process.ngrok:t=2023-12-26T10:30:04+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session obj=csess id=abecdc728234 err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent session.\\nYou can run multiple tunnels on a single agent session using a configuration file.\\nTo learn more, see https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config/\\n\\nActive ngrok agent sessions in region 'us':\\n  - ts_2a4Y2aAFqESy92mqZjwL0bYHb8w (104.196.32.137)\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2023-12-26T10:30:04+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent session.\\nYou can run multiple tunnels on a single agent session using a configuration file.\\nTo learn more, see https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config/\\n\\nActive ngrok agent sessions in region 'us':\\n  - ts_2a4Y2aAFqESy92mqZjwL0bYHb8w (104.196.32.137)\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-98ce03b91a9b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2WntwErWDt9LxQ2Jfp6C8OxDAMK_7iZVdC1utyZET1PE8cuUg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8787\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublic_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" * ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpublic_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8787\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"auth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating tunnel with options: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartup_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             raise PyngrokNgrokError(\"The ngrok process errored on start: {}.\".format(ngrok_process.startup_error),\n\u001b[0m\u001b[1;32m    396\u001b[0m                                     \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                                     ngrok_process.startup_error)\n",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Your account is limited to 1 simultaneous ngrok agent session.\\nYou can run multiple tunnels on a single agent session using a configuration file.\\nTo learn more, see https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config/\\n\\nActive ngrok agent sessions in region 'us':\\n  - ts_2a4Y2aAFqESy92mqZjwL0bYHb8w (104.196.32.137)\\r\\n\\r\\nERR_NGROK_108\\r\\n."
          ]
        }
      ],
      "source": [
        "## Set Up NGROK-Tunnel\n",
        "conf.get_default().auth_token = \"2WntwErWDt9LxQ2Jfp6C8OxDAMK_7iZVdC1utyZET1PE8cuUg\"\n",
        "\n",
        "public_url = ngrok.connect(8787).public_url\n",
        "print(\" * ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, 8787))\n",
        "\n",
        "if 'client' not in globals():\n",
        "  !python -m pip install jupyter-server-proxy\n",
        "  client = Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CymECzDhh6Mn"
      },
      "source": [
        "### System settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4lfXyeQvfCn"
      },
      "outputs": [],
      "source": [
        "cwd = os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCMMsTZyC0BE"
      },
      "source": [
        "### CUDF für hardware acceleration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqFqwisUh6Mn"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "# !python rapidsai-csp-utils/colab/pip-install.py\n",
        "# import cudf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRqa0EPy_lpx"
      },
      "source": [
        "## Grobes HTML-Parsing\n",
        "Als erstes müssen wir die HTML-Dokumente zu normalem Text umwandeln, ansonsten sind die Text-Zellen zu groß und führen zu Problemen mit PyArrow/Dask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4rQifxX6ygR"
      },
      "outputs": [],
      "source": [
        "input_dir = \"data/raw_bzg/\"\n",
        "output_dir = 'data/unraw1_bzg/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wy5-WZ4B_A2"
      },
      "outputs": [],
      "source": [
        "# for year in range(2019, 2020):\n",
        "#     print(year)\n",
        "#     df = pd.read_parquet(f\"{input_dir}story_df_raw_{year}.parquet\")\n",
        "#     df = dd.from_pandas(df, npartitions=12)\n",
        "#     df[\"html_body\"] = df[\"html_body\"].apply(body_formatter, meta=pd.Series(dtype=\"str\"))\n",
        "#     df = df.rename(columns={\"html_body\":\"body\"})\n",
        "#     name_function = lambda x: f\"data-{year}-{x}.parquet\"\n",
        "#     df.to_parquet(output_dir, name_function=name_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa6QpqfR_t97"
      },
      "source": [
        "## Neu-Partitionierug\n",
        "Sodass alle Partitionen etwa die gleiche Größe haben."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHFpynyiq3O4"
      },
      "outputs": [],
      "source": [
        "input_dir = 'data/unraw1_bzg/'\n",
        "output_dir = 'data/unraw2_bzg/'\n",
        "\n",
        "# ddf = dd.read_parquet(input_dir+\"*.parquet\")\n",
        "# ddf2 = ddf.repartition(npartitions=50)\n",
        "# name_function = lambda x: f\"data-{x}.parquet\"\n",
        "# ddf2.to_parquet(output_dir, name_function=name_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFnIFQW2Nag7"
      },
      "source": [
        "## Author-Inferenz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__EYo7CaFlVj"
      },
      "source": [
        "Ein bisschen die Daten säubern..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-jzA2AbFptW"
      },
      "outputs": [],
      "source": [
        "input_dir = cwd+'/data/unraw2_bzg/'\n",
        "output_dir = cwd+'/data/unraw3_bzg/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef28p8AOnrFD"
      },
      "outputs": [],
      "source": [
        "ddf = dd.read_parquet(input_dir+\"*.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylwarwpXLegE"
      },
      "outputs": [],
      "source": [
        "# Remove rows for which noo stock ticker is recorded\n",
        "ddf = ddf[ddf.stocks != '']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7OPUatcMS7C"
      },
      "outputs": [],
      "source": [
        "# Convert `channels`  datatype from string to list\n",
        "ddf[\"channels\"] = ddf[\"channels\"].apply(eval, meta=pd.Series(dtype='object'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu1EhTqrVpBG"
      },
      "source": [
        "Untersuche als nächstes die Behauptung, dass **PRNewswire** und **Businesswire** den gesamten Markt für Pressemeldungen in den USA kontrollieren. Wenn dem so ist, und sie nicht noch weitere, unwichtige Meldungen veröffentlichen, dann können wir einfach die Newsartikel nach diesen Autoren filtern und uns viel Arbeit ersparen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5mO091MfO6e"
      },
      "outputs": [],
      "source": [
        "dask.config.set(scheduler=\"processes\")\n",
        "ddf[\"inferred_author\"] = None\n",
        "ddf[\"inferred_author\"] = ddf.body.apply(infer_author, meta=pd.Series(dtype=\"string\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTcPjnU0VZZu"
      },
      "outputs": [],
      "source": [
        "# value_counts for authors\n",
        "auhtor_value_counts = pd.concat([ddf.author.value_counts().head(10), ddf.inferred_author.value_counts().head(10)], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNesN9jX1IP0"
      },
      "outputs": [],
      "source": [
        "auhtor_value_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKT6tusBp13y"
      },
      "outputs": [],
      "source": [
        "auhtor_value_counts.sum().diff()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk6p6woeqJqP"
      },
      "source": [
        "Ungefähr 650k Nachrichten werden ausgelassen, wenn nur die vier Hauptvertreiber von Pressemeldungen berücksichtigt werden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X8rHo-CsDCQ"
      },
      "outputs": [],
      "source": [
        "ddf = ddf[~ddf.inferred_author.isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHvzym8sFcX_"
      },
      "outputs": [],
      "source": [
        "ddf[\"inferred_author\"] = ddf[\"inferred_author\"].astype(\"string\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufk8_XUjFwdk"
      },
      "outputs": [],
      "source": [
        "ddf[\"channels\"] = ddf.channels.apply(lambda x: str(x), meta=pd.Series(dtype=\"string\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsEmViTzNvgs"
      },
      "outputs": [],
      "source": [
        "ddf.inferred_author.value_counts().compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfNNyD_31sly"
      },
      "outputs": [],
      "source": [
        "ddf.inferred_author.value_counts().sum().compute()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ddf = ddf.drop(columns=[\"author\"]).rename(columns={\"inferred_author\":\"author\"})"
      ],
      "metadata": {
        "id": "pmMI9DsEhrn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v04Wy5Y4iudn"
      },
      "outputs": [],
      "source": [
        "name_function = lambda x: f\"data-{x}.parquet\"\n",
        "ddf.to_parquet(output_dir, name_function=name_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLdnwdyxNFiH"
      },
      "outputs": [],
      "source": [
        "# Contains 100k rows\n",
        "earnings_ddf = ddf[ddf.channels.apply(lambda x: \"Earnings\" in x, meta=pd.Series(dtype=bool))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-zs5SSiVbyp"
      },
      "outputs": [],
      "source": [
        "# value counts for authors of earnings reports (contrast to value counts of all news articles)\n",
        "earnings_ddf.inferred_author.value_counts().head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dVD6j-G9nie"
      },
      "source": [
        "Hier sehen wir, dass es keine einzige Pressemeldung von **Business Wire** gibt, die mit *Earnings* gekennzeichnet sind. Trotzdem gibt es relevante *Earnings* reports von Business Wire. Dies habe ich kurz verifiziert..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jafqZ87tXDKn"
      },
      "source": [
        "## Vorgehen\n",
        "\n",
        "Benötigt:\n",
        "---------\n",
        "- data_shared/corporation_endings.txt \\\\\n",
        "- input_dir = data/unraw3_bzg/ \\\\\n",
        "\n",
        "Produziert:\n",
        "-----------\n",
        "- data_shared/all_ticker_name_mapper.parquet\n",
        "\n",
        "--------------\n",
        "\n",
        "Wie viele Nachrichten bleiben, wenn wir auf relevante Ticker filtern? Wir wollen nicht(!) - so ist es momentan - auf die momentane Russell 3k-Zusammensetzung filtern, denn wir wollen auch ungelistete bzw. ehemalige Russell-Aktien beachten.\n",
        "\n",
        "\n",
        "**1. Full-Name-Discovery:**\n",
        "\n",
        "Herausfinden des vollen Namens des Unternehmens für jeden Ticker, damit 1. der Text richtig geparst werden kann und 2. damit wir einen Anhaltspunkt für das Ticker-Grouping haben.\n",
        "\n",
        "\n",
        "**2. Ticker-Filtering:**\n",
        "\n",
        "Alle Ticker herausfiltern, die wir nicht brauchen. Wenn wir aber ein großes Aktienuniversum (mit inzwischen ungelisteten Aktien) benutzen, werden wir fast alle Nachrichten behalten können. Allerdings lassen sich so Fehlerhafte Nachrichten/Ticker etc. herausfiltern.\n",
        "\n",
        "\n",
        "**3. Ticker-Grouping:**\n",
        "\n",
        "Was machen wir, wenn wir mehrerer Aktiengattungen für ein Unternehmen haben? Z.B. Vorzugs- und Stammaktien. Wir können i.A. die Stammaktie nehmen, da diese normalerweise ein höheres Handelsvolumen aufweist. D.h. wir bilden alle Ticker der Benzinga-Nachrichten auf den Ticker der Stammaktie ab.\n",
        "\n",
        "\n",
        "**4. Firmennamen-Nachrichtenkörper-Verifikation:**\n",
        "\n",
        "Da Ticker wiederverwendet werden können bzw. sich verändern können wollen wir sicherstellen, dass der Unternehmensname im Nachrichtenkörper vorkommt! Bzw. generell ist das eine gute Datensäuberungs-Maßnahme."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VaSble_1ku1t"
      },
      "outputs": [],
      "source": [
        "input_dir = cwd+'/data/unraw3_bzg/'\n",
        "ddf = dd.read_parquet(input_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JosrB0RfsUsy"
      },
      "outputs": [],
      "source": [
        "all_tickers = ddf.stocks.unique().compute()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tickers sometimes have a dollar sign in front of them.\n",
        "# This is common practice to indicate that the acronym refers to a stock ticker.\n",
        "# This results in ~ 4 duplicate entries.\n",
        "all_tickers = all_tickers.apply(lambda x: x.strip(\"$\"))"
      ],
      "metadata": {
        "id": "JQ3o-6Zimwhh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some stock tickers are in lowercase.\n",
        "# Altough yfinance can handle lowercase tickers we uppercase them\n",
        "# in order to avoid inconsistencies and avoid duplicates.\n",
        "all_tickers = all_tickers.str.upper()\n",
        "all_tickers = all_tickers.drop_duplicates()"
      ],
      "metadata": {
        "id": "BglCX_xQnjNw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colon_tickers = [x for x in all_tickers.values if \":\" in x]\n",
        "print(f\"Around {len(colon_tickers)} stock tickers are from foreign exchanges. \\n\"\n",
        "      f\"The list of foreign exchanges is:\")\n",
        "set([x.split(\":\")[0] for x in colon_tickers])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHKsMwElpmBr",
        "outputId": "b572e960-f464-44dd-85dd-a13d372f4204"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Around 2741 stock tickers are from foreign exchanges. \n",
            "The list of foreign exchanges is:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'BMV', 'CSE', 'PARIS', 'TSX', 'TSXV', 'TYO'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We remove these foreign exchanges:\n",
        "all_tickers = all_tickers[all_tickers.apply(lambda x: \":\" not in x)]"
      ],
      "metadata": {
        "id": "KkJtizLXrOgR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do Whitespaces exit and prevent yfinance from finding company names?\n",
        "# Only one company, and its not even a ticker but the company name\n",
        "all_tickers[all_tickers.apply(lambda x: \" \" in x)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H--sFC-J8cF6",
        "outputId": "deadf50e-af9d-4c5a-bbce-f0ebb50ead8e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10892    SERA PROGNOSTICS, INC.\n",
              "Name: stocks, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdwKGnsPnkYS"
      },
      "source": [
        "### Full-Name-Discovery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8urd8wvsXza"
      },
      "outputs": [],
      "source": [
        "company_names = all_tickers.map(lambda x: yahoo_get_wrapper(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NUslIwYdsmU7"
      },
      "outputs": [],
      "source": [
        "all_mapper = pd.concat([all_tickers, company_names], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tW8AGmgfK_oV"
      },
      "outputs": [],
      "source": [
        "all_mapper.columns = [\"ticker\", \"company_name\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"For {all_mapper[all_mapper.isna().any(axis=1)].shape[0]} tickers, yfinance had no entry, or at least not entry for the `longName`\")\n",
        "# E.g. AIS, PTNR, BSI, LUFK, BFRM. Can't find these stocks on guidants as well and\n",
        "# Some are not headquartered in the US."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-yCIl7vEuUk",
        "outputId": "706c65bd-d30e-4ef3-f5cf-ee3acf77ef3b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For 8492 tickers, yfinance had no entry, or at least not entry for the `longName`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qa3sz1yBMkjw"
      },
      "outputs": [],
      "source": [
        "all_mapper = all_mapper.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FEP_5x3P9F2",
        "outputId": "eec5f5a7-cedf-4c66-b582-ec4cb5062de1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:distributed.utils_perf:full garbage collections took 11% CPU time recently (threshold: 10%)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "556"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Multiple tickers for the same company exist for ~556/2 companies\n",
        "vcs = all_mapper.company_name.value_counts()\n",
        "vcs = vcs[vcs >= 2]\n",
        "all_mapper[all_mapper.company_name.isin(vcs.index)].sort_values(\"company_name\").shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0vWtCfFSoox"
      },
      "source": [
        "Es ist nicht leicht zu sagen, welchen von den Tickern wir bevorzugen sollten. Abgleichen mit den Aktientickern des Kursdatensatzes notwendig, um zu sehen, ob überhaupt nur ein Ticker übereinstimmt. Wenn es für beide Ticker eine Kurszeitreihe gibt, dann sollten wir die nehmen, die ein höheres historisches Volumen hat. Dies ist allerdings etwas, was wir später machen und nicht jetzt. Hier wollen wir zunächst nur die Nachrichten verarbeiten, weswegen wir nur die NaN-Unternehmen rausnehmen und den Rest - *ohne Ticker-Filtering* - weiterverarbeiten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Y6J-M2i4WeW2"
      },
      "outputs": [],
      "source": [
        "mapper = all_mapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dvJpTNzVFmLZ"
      },
      "outputs": [],
      "source": [
        "mapper.columns = [\"ticker\", \"company_names\"]\n",
        "mapper.set_index(\"ticker\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-t2omABfXZRK"
      },
      "outputs": [],
      "source": [
        "company_endings = pd.read_table(\"data_shared/corporation_endings.txt\").iloc[:, 0]\n",
        "# Apply `get_company_abbreviation` twice in order to get rid of Enterprise, Ltd.\n",
        "# Otherwise , Ltd. remains. If no acronym, name stays as is.\n",
        "mapper[\"short_name\"] = mapper.company_names.apply(lambda x: get_company_abbreviation(x, company_endings=company_endings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "z3d5wM_qWhvx"
      },
      "outputs": [],
      "source": [
        "mapper = mapper.applymap(lambda x: x.strip(\" \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "voNHNOrXSIKL"
      },
      "outputs": [],
      "source": [
        "mapper.to_parquet(cwd + \"/data_shared/ticker_name_mapper.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "yrLjdrfwlmRm"
      },
      "outputs": [],
      "source": [
        "mapper = pd.read_parquet(cwd + \"/data_shared/ticker_name_mapper.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "70POMUC-PN-n"
      },
      "outputs": [],
      "source": [
        "filt_ddf = ddf[ddf.stocks.isin(mapper.index.to_list())]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZszkSxWPBK9",
        "outputId": "7c1f8c19-0a4d-4a84-b1d7-2f9113402a59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1436519"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "n = ddf.map_partitions(lambda x: x.shape[0]).compute()\n",
        "print(f\"Es verbleiben {n.sum()} Nachrichten, für die wir den Ticker zu einem Firmennamen auflösen können.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLbhWydOPaaK",
        "outputId": "67783c42-0623-42c3-e7d4-a5b235e2b0fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1002293"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "n = filt_ddf.map_partitions(lambda x: x.shape[0]).compute()\n",
        "n.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "xSH8cQgL5tZu"
      },
      "outputs": [],
      "source": [
        "ddf = filt_ddf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "safdLC-H6GR5"
      },
      "outputs": [],
      "source": [
        "ddf[\"company_name\"] = ddf.stocks.map(lambda x: mapper.company_names.loc[x], meta=pd.Series(dtype=\"string\"))\n",
        "ddf[\"short_name\"] = ddf.stocks.map(lambda x: mapper.short_name.loc[x], meta=pd.Series(dtype=\"string\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xw1JSiI_0JH"
      },
      "source": [
        "### Duplikate Entfernen"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = ddf.map_partitions(lambda x: x.shape[0]).compute().sum()\n",
        "n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92hxVN_efR9f",
        "outputId": "4b1a6b84-a7cc-4049-b36a-c6904fe7256b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1002293"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "cEzrJqEN_yuJ"
      },
      "outputs": [],
      "source": [
        "ddf = ddf.map_partitions(lambda df: df.drop_duplicates())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = ddf.map_partitions(lambda x: x.shape[0]).compute().sum()\n",
        "n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0qVBh55fwez",
        "outputId": "c130b590-6e5c-4d68-db4e-f8ebe0546e1a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "999448"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name_function = lambda x: f\"data-{x}.parquet\"\n",
        "ddf.to_parquet(cwd+'/data/latest/', name_function=name_function)"
      ],
      "metadata": {
        "id": "wQlXGkufSq-w"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F3dOc35YN8k"
      },
      "source": [
        "### Firmennamen-Nachrichtenkörper-Verifikation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "w91ht_CvPZDx"
      },
      "outputs": [],
      "source": [
        "ddf = dd.read_parquet(cwd+'/data/latest/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ddf.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CN9L3axUh6f7",
        "outputId": "317960f6-63dc-4b16-8976-44cb50bbdaf2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "time            datetime64[ns, pytz.FixedOffset(-240)]\n",
              "stocks                                          string\n",
              "title                                           string\n",
              "channels                                        object\n",
              "body                                            string\n",
              "author                                          string\n",
              "company_name                                    object\n",
              "short_name                                      object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If short_name doesn't occurr in the title or in the body, then article seems to be faulty\n",
        "mask = ddf.apply(lambda x:\n",
        "                 bool(re.search(x[\"short_name\"],\n",
        "                                x.title + x[\"body\"].replace(\"( )*\\n( )*\", \" \"),\n",
        "                                re.IGNORECASE)),\n",
        "                 axis=1,\n",
        "                 meta=pd.Series(dtype=bool))"
      ],
      "metadata": {
        "id": "d46wUvcQTzB8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FBv0rbEKYULN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ec535ad-1ce7-4931-e2ff-f5ed619f5709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Around 11158 stocks before filtering\n",
            "Around 9670 stocks after filtering\n"
          ]
        }
      ],
      "source": [
        "print(f\"Around {len(ddf.stocks.unique())} stocks before filtering\")\n",
        "print(f\"Around {len(ddf[mask].stocks.unique())} stocks after filtering\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oNAFvvVmmWm2"
      },
      "outputs": [],
      "source": [
        "# Filter out faulty news\n",
        "ddf = ddf[mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GvhpDVtFh6M6"
      },
      "outputs": [],
      "source": [
        "name_function = lambda x: f\"data-{x}.parquet\"\n",
        "ddf.to_parquet(cwd+'/data/latest2/', name_function=name_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SurJhZXHx0U"
      },
      "source": [
        "### Timedeltas zwischen Nachrichtenmeldungen\n",
        "\n",
        "\n",
        "\n",
        "Wir sehen, dass einige Nachrichten dupliziert vorkommen, d.h. mit einem Timedelta von 0 und mit derselben Überschrift etc. diese gilt es zu eliminieren."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HxMOTfgjpvs"
      },
      "source": [
        "#### Convert ddf to pd.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ddf = dd.read_parquet(cwd+'/data/latest2/')"
      ],
      "metadata": {
        "id": "QpskPuAUkXXu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sPF987k3H8tC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3ef1768-e8c5-41e1-bc83-3ccddc5baf27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Scheduler for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n"
          ]
        }
      ],
      "source": [
        "ddf = ddf.compute()\n",
        "ddf = ddf.sort_values(\"time\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NDJqz8MW5_xW"
      },
      "outputs": [],
      "source": [
        "tmp = ddf[[\"time\", \"stocks\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rZouQiwXHy0r"
      },
      "outputs": [],
      "source": [
        "#### Adding timedeltas to the data frame\n",
        "news_timedeltas = tmp.groupby(\"stocks\").transform(lambda x: x.diff())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrcTtcDj-UjE"
      },
      "outputs": [],
      "source": [
        "# ~3 minutes evaluates to true\n",
        "# (news_timedeltas.index == ddf.index).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "P77WnvamNhf-"
      },
      "outputs": [],
      "source": [
        "ddf.loc[:, \"timedelta\"] = news_timedeltas.time.fillna(pd.Timedelta(days=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3Ij4ENbFQw7b"
      },
      "outputs": [],
      "source": [
        "news_timedeltas = ddf.timedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbnz32U8RFCj",
        "outputId": "d66ef334-9084-4d9e-c628-84e0234a3846"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Components(days=100, hours=0, minutes=0, seconds=0, milliseconds=0, microseconds=0, nanoseconds=0)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "news_timedeltas.iloc[0].components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zcSEhkw6JLHO"
      },
      "outputs": [],
      "source": [
        "same_day_timedeltas = news_timedeltas.apply(lambda x: x.components.days == 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2wKSEQ4JT1E",
        "outputId": "39774545-8237-49a0-dae1-1cafbbbc3d5f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "699538"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "(same_day_timedeltas == 0).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xEOr6c8qsI7G"
      },
      "outputs": [],
      "source": [
        "same_hour_timedeltas = news_timedeltas.apply(lambda x: (x.components.days == 0) & \\\n",
        "                                               (x.components.hours == 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0wfw1X8sMxy",
        "outputId": "be2f4947-7119-442c-d4db-2e2c18c4a9ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43633\n"
          ]
        }
      ],
      "source": [
        "print(same_hour_timedeltas.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "L-KxzooAoIrp"
      },
      "outputs": [],
      "source": [
        "same_minute_timedeltas = news_timedeltas.apply(lambda x: (x.components.days == 0) & \\\n",
        "                                               (x.components.hours == 0) & \\\n",
        "                                               (x.components.minutes == 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-eIKdg7ogxA",
        "outputId": "a07fd68c-cde7-4465-d379-49fa0349b443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16328\n"
          ]
        }
      ],
      "source": [
        "print(same_minute_timedeltas.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "6cECO9zSxX6G"
      },
      "outputs": [],
      "source": [
        "same_day_ddf = ddf.loc[same_day_timedeltas]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcTg5C9lUyQk",
        "outputId": "5b92366d-1884-4df7-e7a4-f598e0e48a86"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    9670.000000\n",
              "mean       90.503413\n",
              "std       164.442156\n",
              "min         1.000000\n",
              "25%         4.000000\n",
              "50%        31.000000\n",
              "75%       117.000000\n",
              "max      5025.000000\n",
              "Name: stocks, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "ddf.stocks.value_counts().describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bshj1secWQ3P"
      },
      "source": [
        "Bis zu 5k Nachrichten pro Firma, z.B. AT&T, was in 13 Jahren ca. einer Nachricht pro Tag entspricht. Wir wollen nicht das eine Firma mit vielen Junk-Nachrichten das Modell dominiert. Wobei das Modell hoffentlich dann auch die Junk-Nachrichten als solche erkennt und ignoriert. Eher wichtig noch einen `staleness`-Faktor, also wie ähnlich die Nachricht zu Vorhergegangenen ist (i.e. Nachrichten desselben Tages oder derselben Woche).\n",
        "\n",
        "Kategorisieren von Nachrichten (mit Text2Topic, wie Salbrechter?) und eliminieren von Business/Strategic etc.\n",
        "Im Falle von Text2Topic, versuche Estimates des Unternehmens von Dritten zu unterscheiden.\n",
        "\n",
        "Wichtig!!! Unterscheide zwischen LERN-Phase und PRODUKTIONS-Phase.\n",
        "Wir können z.B. CLS-Token in der Produktions-Phase vergleichen, in der Lern-Phase aber noch nicht.\n",
        "\n",
        "Text2Vec -> Business category evtl. entfernen-> Intrastock variance average"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRABrPVOcaa9"
      },
      "source": [
        "## Nachrichten-Parsing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "8sBkgXJoMNGq"
      },
      "outputs": [],
      "source": [
        "ddf = dd.read_parquet(cwd+'/data/latest2/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ticker_name_mapper = pd.read_parquet(\"data_shared/ticker_name_mapper.parquet\")\n",
        "ticker_name_mapper.shape[0]"
      ],
      "metadata": {
        "id": "dK5naDT8t1Ap",
        "outputId": "fe411f83-57d1-4e37-f0a8-7cd190261a57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11158"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ticker_name_mapper_reduced.shape[0]"
      ],
      "metadata": {
        "id": "35PGz7YRtu9Z",
        "outputId": "7fabd56d-ca37-4375-93ef-a291c3a5aa09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9670"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "MUy_FfbYMNGq"
      },
      "outputs": [],
      "source": [
        "# TODO: Compare to ticker_name_mapper\n",
        "ticker_name_mapper_reduced = ddf[[\"stocks\", \"company_name\", \"short_name\"]].drop_duplicates(keep=\"first\").compute()\n",
        "ticker_name_mapper_reduced.to_parquet(\"data_shared/ticker_name_mapper_reduced.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqQBHH3QMNGr"
      },
      "source": [
        "### Beispiel/ Untersuchung"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt update && \\\n",
        "    python -m nltk.downloader averaged_perceptron_tagger punkt wordnet && \\\n",
        "    DEBIAN_FRONTEND='noninteractive' apt install -y maven"
      ],
      "metadata": {
        "id": "r9hRFHglwS58",
        "outputId": "c62e297c-9246-483a-830e-f85e062432b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Connecting to ppa.launchpadcontent.net \u001b[0m\r                                                                                                    \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "36 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "maven is already the newest version (3.6.3-5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mvn dependency:copy-dependencies -DoutputDirectory=./jars -f $(python3 -c 'import importlib; import pathlib; print(pathlib.Path(importlib.util.find_spec(\"sutime\").origin).parent / \"pom.xml\")')\n"
      ],
      "metadata": {
        "id": "IOfh3EOfwXDS",
        "outputId": "4a147786-b4e5-4da8-ec54-3dc5c27a675e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\u001b[1;34mINFO\u001b[m] Scanning for projects...\n",
            "[\u001b[1;34mINFO\u001b[m] \n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1m----------< \u001b[0;36medu.stanford.nlp:stanford-corenlp-sutime-python\u001b[0;1m >-----------\u001b[m\n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1mBuilding CoreNLP SUTime Python Wrapper 1.4.0\u001b[m\n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1m--------------------------------[ jar ]---------------------------------\u001b[m\n",
            "[\u001b[1;34mINFO\u001b[m] \n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1m--- \u001b[0;32mmaven-dependency-plugin:3.1.2:copy-dependencies\u001b[m \u001b[1m(default-cli)\u001b[m @ \u001b[36mstanford-corenlp-sutime-python\u001b[0;1m ---\u001b[m\n",
            "[\u001b[1;34mINFO\u001b[m] javax.xml.bind:jaxb-api:jar:2.4.0-b180830.0359 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] javax.activation:javax.activation-api:jar:1.2.0 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] com.sun.xml.bind:jaxb-core:jar:2.3.0.1 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] com.sun.xml.bind:jaxb-impl:jar:2.4.0-b180830.0438 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] javax.activation:activation:jar:1.1.1 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] edu.stanford.nlp:stanford-corenlp:jar:4.0.0 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] com.apple:AppleJavaExtensions:jar:1.4 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] de.jollyday:jollyday:jar:0.4.9 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] org.apache.commons:commons-lang3:jar:3.3.1 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] org.apache.lucene:lucene-queryparser:jar:7.5.0 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] org.apache.lucene:lucene-queries:jar:7.5.0 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] org.apache.lucene:lucene-sandbox:jar:7.5.0 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] org.apache.lucene:lucene-analyzers-common:jar:7.5.0 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] org.apache.lucene:lucene-core:jar:7.5.0 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] javax.servlet:javax.servlet-api:jar:3.0.1 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] xom:xom:jar:1.3.2 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] xml-apis:xml-apis:jar:1.3.03 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] xerces:xercesImpl:jar:2.8.0 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] xalan:xalan:jar:2.7.0 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] joda-time:joda-time:jar:2.10.5 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] org.ejml:ejml-core:jar:0.38 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] com.google.code.findbugs:jsr305:jar:3.0.2 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] org.ejml:ejml-ddense:jar:0.38 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] org.ejml:ejml-simple:jar:0.38 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] org.ejml:ejml-fdense:jar:0.38 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] org.ejml:ejml-cdense:jar:0.38 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] org.ejml:ejml-zdense:jar:0.38 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] org.ejml:ejml-dsparse:jar:0.38 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] org.glassfish:javax.json:jar:1.0.4 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] org.slf4j:slf4j-api:jar:1.7.12 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] com.google.protobuf:protobuf-java:jar:3.9.2 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] edu.stanford.nlp:stanford-corenlp:jar:models:4.0.0 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] org.slf4j:slf4j-simple:jar:1.7.30 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] com.google.code.gson:gson:jar:2.8.6 already exists in destination.\n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1m------------------------------------------------------------------------\u001b[m\n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1;32mBUILD SUCCESS\u001b[m\n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1m------------------------------------------------------------------------\u001b[m\n",
            "[\u001b[1;34mINFO\u001b[m] Total time:  1.378 s\n",
            "[\u001b[1;34mINFO\u001b[m] Finished at: 2023-12-26T10:57:29Z\n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1m------------------------------------------------------------------------\u001b[m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "keF_8QNAkM36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "7d229bb6-cb92-43cf-a752-9cfaadc8d2ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-50-238ff0644498>:3: FutureWarning: Setitem-like behavior with mismatched timezones is deprecated and will change in a future version. Instead of raising (or for Index, Series, and DataFrame methods, coercing to object dtype), the value being set (or passed as a fill_value, or inserted) will be cast to the existing DatetimeArray/DatetimeIndex/Series/DataFrame column's timezone. To retain the old behavior, explicitly cast to object dtype before the operation.\n",
            "  y.loc[:, \"time\"] = pd.to_datetime(y.time).dt.tz_convert(\"UTC\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PRINCETON, N.J.--(BUSINESS WIRE)--\\n\\nBristol-Myers Squibb Company **** (NYSE: BMY) today announced that it received\\nthe 2015 Collaborator Award from the Multiple Myeloma Research Foundation\\n(MMRF) in recognition of the company\\'s commitment to collaboration and to\\nadvancing the research and development of novel therapies for patients with\\nmultiple myeloma. Bristol-Myers Squibb accepted the award as part of the MMRF\\nSatellite Symposium at the 57th American Society of Hematology Annual Meeting\\nand Exposition on December 4, 2015.\\n\\n\"This award is especially meaningful to Bristol-Myers Squibb. We are proud of\\nthe long-standing partnership with the MMRF, which has allowed us to achieve\\nour shared goals of providing hope to those fighting this terrible cancer,\"\\nsaid Michael Giordano, M.D., senior vice president, Head of Development,\\nOncology, Bristol-Myers Squibb. \"We applaud the MMRF for driving innovation in\\nthis space, and we look forward to working with the organization and the\\nbroader multiple myeloma community – including the patients, families, and\\nhealthcare professionals who participate in our clinical trial program – to\\ncontinue to advance research that can deliver improved outcomes for patients.\"\\n\\nThe Collaborator Award is part of an annual initiative by the MMRF to\\nrecognize the exceptional work of its partners from industry and academia who\\nhave played a critical role toward fulfilling the organization\\'s mission: to\\npursue innovative means that accelerate the development of next-generation\\nmultiple myeloma treatments to extend the lives of patients and ultimately\\nlead to a cure.\\n\\n\"We are excited to present this award to Bristol-Myers Squibb which\\nacknowledges their leadership in scientific innovation and unwavering\\ncommitment to multiple myeloma. We appreciate their dedication to\\nunderstanding and embracing the complex needs of multiple myeloma patients, to\\nrespond to healthcare professionals as they work to better fight this disease,\\nand to bring it all together in profound service to our community,\" said\\nWalter M. Capone, president and chief executive officer, MMRF. \"All of us at\\nthe MMRF and Multiple Myeloma Research Consortium, our research partners, and\\nall of the patients for whom we tirelessly work are sincerely grateful for\\nBristol-Myers Squibb\\'s shared dedication to multiple myeloma.\"\\n\\nThe award was announced shortly following the U.S. Food and Drug\\nAdministration\\'s (FDA) approval on November 30, 2015, of Bristol-Myers\\nSquibb\\'s _Empliciti_ ™ (elotuzumab) as a combination therapy with Revlimid®\\n(lenalidomide) and dexamethasone for the treatment of patients with multiple\\nmyeloma who have received one to three prior therapies. With _Empliciti,_\\nBristol-Myers Squibb is providing a new approach to treating multiple myeloma,\\nas it is the first and only immunostimulatory antibody approved for treatment\\nof the disease.\\n\\n**About _Empliciti_**\\n\\n_Empliciti_ is an immunostimulatory antibody that specifically targets\\nSignaling Lymphocyte Activation Molecule Family member 7 (SLAMF7), a cell-\\nsurface glycoprotein. SLAMF7 is expressed on myeloma cells independent of\\ncytogenetic abnormalities. SLAMF7 is also expressed on Natural Killer cells,\\nplasma cells, and at lower levels on specific immune cell subsets of\\ndifferentiated cells within the hematopoietic lineage.\\n\\n_Empliciti_ has a dual mechanism-of-action. It directly activates the immune\\nsystem through Natural Killer cells via the SLAMF7 pathway. _Empliciti_ also\\ntargets SLAMF7 on myeloma cells, tagging these malignant cells for Natural\\nKiller cell-mediated destruction via antibody-dependent cellular toxicity.\\n\\nBristol-Myers Squibb and AbbVie are co-developing _Empliciti_ , with Bristol-\\nMyers Squibb solely responsible for commercial activities. Prior to approval,\\n_Empliciti_ was granted Breakthrough Therapy Designation by the FDA for use in\\ncombination with lenalidomide and dexamethasone for the treatment of multiple\\nmyeloma in patients who have received one to three prior therapies. According\\nto the FDA, Breakthrough Therapy Designation is intended to expedite the\\ndevelopment and review of drugs for serious or life-threatening conditions.\\nThe criteria for Breakthrough Therapy Designation requires preliminary\\nclinical evidence that demonstrates the drug may have substantial improvement\\non at least one clinically significant endpoint over available therapy.\\n\\n**About Multiple Myeloma**\\n\\nMultiple myeloma is a hematologic, or blood, cancer that develops in the bone\\nmarrow. It occurs when a plasma cell, a type of cell in the soft center of\\nbone marrow, becomes cancerous and multiplies uncontrollably. Common symptoms\\nof multiple myeloma include bone pain, fatigue, kidney impairment, and\\ninfections.\\n\\nDespite advances in multiple myeloma treatment over the last decade, less than\\nhalf of patients survive for five or more years after diagnosis. A common\\ncharacteristic for many patients is that they experience a cycle of remission\\nand relapse, in which they stop treatment for a short time, but eventually\\nreturn to a treatment shortly after. It is estimated that annually, more than\\n114,200 new cases of multiple myeloma are diagnosed and more than 80,000\\npeople die from the disease globally.\\n\\n**EMPLICITI (elotuzumab) INDICATIONS & IMPORTANT SAFETY INFORMATION**\\n\\n**INDICATION**\\n\\nEMPLICITI™ (elotuzumab) is indicated in combination with lenalidomide and\\ndexamethasone for the treatment of patients with multiple myeloma who have\\nreceived one to three prior therapies.\\n\\n**IMPORTANT SAFETY INFORMATION**\\n\\n**Infusion Reactions**\\n\\n  * EMPLICITI can cause infusion reactions. Common symptoms include fever, chills, and hypertension. Bradycardia and hypotension also developed during infusions. In the trial, 5% of patients required interruption of the administration of EMPLICITI for a median of 25 minutes due to infusion reactions, and 1% of patients discontinued due to infusion reactions. Of the patients who experienced an infusion reaction, 70% (23/33) had them during the first dose. If a Grade 2 or higher infusion reaction occurs, interrupt the EMPLICITI infusion and institute appropriate medical and supportive measures. If the infusion reaction recurs, stop the EMPLICITI infusion and do not restart it on that day. Severe infusion reactions may require permanent discontinuation of EMPLICITI therapy and emergency treatment. \\n  * Premedicate with dexamethasone, H1 Blocker, H2 Blocker, and acetaminophen prior to infusing with EMPLICITI. \\n\\n**Infections**\\n\\n  * In a clinical trial of patients with multiple myeloma (N=635), infections were reported in 81.4% of patients in the EMPLICITI with lenalidomide/dexamethasone arm (ERd) and 74.4% in the lenalidomide/dexamethasone arm (Rd). Grade 3-4 infections were 28% (ERd) and 24.3% (Rd). Opportunistic infections were reported in 22% (ERd) and 12.9% (Rd). Fungal infections were 9.7% (ERd) and 5.4% (Rd). Herpes zoster was 13.5% (ERd) and 6.9% (Rd). Discontinuations due to infections were 3.5% (ERd) and 4.1% (Rd). Fatal infections were 2.5% (ERd) and 2.2% (Rd). Monitor patients for development of infections and treat promptly. \\n\\n**Second Primary Malignancies**\\n\\n  * In a clinical trial of patients with multiple myeloma (N=635), invasive second primary malignancies (SPM) were 9.1% (ERd) and 5.7% (Rd). The rate of hematologic malignancies were the same between ERd and Rd treatment arms (1.6%). Solid tumors were reported in 3.5% (ERd) and 2.2% (Rd). Skin cancer was reported in 4.4% (ERd) and 2.8% (Rd). Monitor patients for the development of SPMs. \\n\\n**Hepatotoxicity**\\n\\n  * Elevations in liver enzymes (AST/ALT greater than 3 times the upper limit, total bilirubin greater than 2 times the upper limit, and alkaline phosphatase less than 2 times the upper limit) consistent with hepatotoxicity were 2.5% (ERd) and 0.6% (Rd). Two patients experiencing hepatotoxicity discontinued treatment; however, 6 out of 8 patients had resolution and continued treatment. Monitor liver enzymes periodically. Stop EMPLICITI upon Grade 3 or higher elevation of liver enzymes. After return to baseline values, continuation of treatment may be considered. \\n\\n**Interference with Determination of Complete Response**\\n\\n  * EMPLICITI is a humanized IgG kappa monoclonal antibody that can be detected on both the serum protein electrophoresis and immunofixation assays used for the clinical monitoring of endogenous M-protein. This interference can impact the determination of complete response and possibly relapse from complete response in patients with IgG kappa myeloma protein. \\n\\n**Pregnancy/Females and Males of Reproductive Potential**\\n\\n  * There are no studies with EMPLICITI with pregnant women to inform any drug associated risks. \\n  * There is a risk of fetal harm, including severe life-threatening human birth defects associated with lenalidomide and it is contraindicated for use in pregnancy. Refer to the lenalidomide full prescribing information for requirements regarding contraception and the prohibitions against blood and/or sperm donation due to presence and transmission in blood and/or semen and for additional information. \\n\\n**Adverse Reactions**\\n\\n  * Infusion reactions were reported in approximately 10% of patients treated with EMPLICITI with lenalidomide and dexamethasone. All reports of infusion reaction were Grade 3 or lower. Grade 3 infusion reactions occurred in 1% of patients. \\n  * Serious adverse reactions were 65.4% (ERd) and 56.5% (Rd). The most frequent serious adverse reactions in the ERd arm compared to the Rd arm were: pneumonia (15.4%, 11%), pyrexia (6.9%, 4.7%), respiratory tract infection (3.1%, 1.3%), anemia (2.8%, 1.9%), pulmonary embolism (3.1%, 2.5%), and acute renal failure (2.5%, 1.9%). \\n  * The most common adverse reactions in ERd and Rd, respectively (>20%) were fatigue (61.6%, 51.7%), diarrhea (46.9%, 36.0%), pyrexia (37.4%, 24.6%), constipation (35.5%, 27.1%), cough (34.3%, 18.9%), peripheral neuropathy (26.7%, 20.8%), nasopharyngitis (24.5%, 19.2%), upper respiratory tract infection (22.6%, 17.4%), decreased appetite (20.8%, 12.6%), and pneumonia (20.1%, 14.2%). \\n\\n**Please see the full Prescribing Information** **here** **.**\\n\\n**About Bristol-Myers Squibb**\\n\\nBristol-Myers Squibb is a global biopharmaceutical company whose mission is to\\ndiscover, develop and deliver innovative medicines that help patients prevail\\nover serious diseases. For more information about Bristol-Myers Squibb, visit\\nwww.bms.com or follow us on Twitter at http://twitter.com/bmsnews.\\n\\n**Bristol-Myers Squibb Forward-Looking Statement**\\n\\n_This press release contains \"forward-looking statements\" as that term is\\ndefined in the Private Securities Litigation Reform Act of 1995 regarding the\\nresearch, development and commercialization of pharmaceutical products. Such\\nforward-looking statements are based on current expectations and involve\\ninherent risks and uncertainties, including factors that could delay, divert\\nor change any of them, and could cause actual outcomes and results to differ\\nmaterially from current expectations. No forward-looking statement can be\\nguaranteed. Forward-looking statements in this press release should be\\nevaluated together with the many uncertainties that affect Bristol-Myers\\nSquibb\\'s business, particularly those identified in the cautionary factors\\ndiscussion in Bristol-Myers Squibb\\'s Annual Report on Form 10-K for the year\\nended December 31, 2014 in our Quarterly Reports on Form 10-Q and our Current\\nReports on Form 8-K. Bristol-Myers Squibb undertakes no obligation to publicly\\nupdate any forward-looking statement, whether as a result of new information,\\nfuture events or otherwise._\\n\\n**Endnotes:**\\n\\n_Empliciti_ is a trademark of Bristol-Myers Squibb Company. Revlimid is a\\nregistered trademark of Celgene Corporation.\\n\\n© 2015 Bristol-Myers Squibb Company. All rights reserved.\\n\\nView source version on businesswire.com:\\nhttp://www.businesswire.com/news/home/20151204005330/en/\\n\\n**Bristol-Myers Squibb Company**  \\n **Media:**  \\nAudrey Abernathy, 919-605-4521, audrey.abernathy@bms.com  \\nor  \\nJaisy Wagner Styles, 610-291-5168, jaisy.styles@bms.com  \\nor  \\n **Investors:**  \\nRanya Dajani, 609-252-5330, ranya.dajani@bms.com  \\nor  \\nBill Szablewski, 609-252-5894, william.szablewski@bms.com\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "sample_partition = ddf.get_partition(20)\n",
        "y = sample_partition.head(5)\n",
        "y.loc[:, \"time\"] = pd.to_datetime(y.time).dt.tz_convert(\"UTC\")\n",
        "x = y.iloc[2]\n",
        "x.body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "mSAGuM_Acrwr"
      },
      "outputs": [],
      "source": [
        "res = client.submit(filter_body, row=x, logging=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "LieS1minMNGs",
        "outputId": "8ea54276-2a3e-4976-8e6b-ed68a3d9479f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the company.today announced that it received the 2015 Collaborator Award from the Multiple Myeloma Research Foundation (MMRF) in recognition of the company\\'s commitment to collaboration and to advancing the research and development of novel therapies for patients with multiple myeloma.the company  accepted the award as part of the MMRF Satellite Symposium at the 57th American Society of Hematology Annual Meeting and Expositi.\"This award is especially meaningful to  the company.We are proud of the long standing partnership with the MMRF, which has allowed us to achieve our shared goals of providing hope to those fighting this terrible cancer,\" said Michael Giordano, M.D., senior vice president, Head of Development, Oncology,  the company.\"We applaud the MMRF for driving innovation in this space, and we look forward to working with the organization and the broader multiple myeloma community – including the patients, families, and healthcare professionals who participate in our clinical trial program – to continue to advance research that can deliver improved outcomes for patients.\" The Collaborator Award is part of an annual initiative by the MMRF to recognize the exceptional work of its partners from industry and academia who have played a critical role toward fulfilling the organization\\'s mission: to pursue innovative means that accelerate the development of next generation multiple myeloma treatments to extend the lives of patients and ultimately lead to a cure.\"We are excited to present this award to  the company  which acknowledges their leadership in scientific innovation and unwavering commitment to multiple myeloma.We appreciate their dedication to understanding and embracing the complex needs of multiple myeloma patients, to respond to healthcare professionals as they work to better fight this disease, and to bring it all together in profound service to our community,\" said Walter M.Capone, president and chief executive officer, MMRF.\"All of us at the MMRF and Multiple Myeloma Research Consortium, our research partners, and all of the patients for whom we tirelessly work are sincerely grateful for  the company \\'s shared dedication to multiple myeloma.\" The award was announced shortly following the U.S.Food and Drug Administration\\'s (FDA) approval   the company \\'s _Empliciti_ ™ (elotuzumab) as a combination therapy with Revlimid® (lenalidomide) and dexamethasone for the treatment of patients with multiple myeloma who have received one to three prior therapies.With _Empliciti,_  the company  is providing a new approach to treating multiple myeloma, as it is the first and only immunostimulatory antibody approved for treatment of the disease.About _Empliciti_._Empliciti_ is an immunostimulatory antibody that specifically targets Signaling Lymphocyte Activation Molecule Family member 7 (SLAMF7), a cell  surface glycoprotein.SLAMF7 is expressed on myeloma cells independent of cytogenetic abnormalities.SLAMF7 is also expressed on Natural Killer cells, plasma cells, and at lower levels on specific immune cell subsets of differentiated cells within the hematopoietic lineage._Empliciti_ has a dual mechanism of action.It directly activates the immune system through Natural Killer cells via the SLAMF7 pathway._Empliciti_ also targets SLAMF7 on myeloma cells, tagging these malignant cells for Natural Killer cell mediated destruction via antibody dependent cellular toxicity.the company  and AbbVie are co developing _Empliciti_ , with Bristol  Myers Squibb solely responsible for commercial activities.Prior to approval, _Empliciti_ was granted Breakthrough Therapy Designation by the FDA for use in combination with lenalidomide and dexamethasone for the treatment of multiple myeloma in patients who have received one to three prior therapies.According to the FDA, Breakthrough Therapy Designation is intended to expedite the development and review of drugs for serious or life threatening conditions.The criteria for Breakthrough Therapy Designation requires preliminary clinical evidence that demonstrates the drug may have substantial improvement on at least one clinically significant endpoint over available therapy.About Multiple Myeloma.Multiple myeloma is a hematologic, or blood, cancer that develops in the bone marrow.It occurs when a plasma cell, a type of cell in the soft center of bone marrow, becomes cancerous and multiplies uncontrollably.Common symptoms of multiple myeloma include bone pain, fatigue, kidney impairment, and infections.Despite advances in multiple myeloma treatment over the last decade, less than half of patients survive for five or more years after diagnosis.A common characteristic for many patients is that they experience a cycle of remission and relapse, in which they stop treatment for a short time, but eventually return to a treatment shortly after.It is estimated that annually, more than 114,200 new cases of multiple myeloma are diagnosed and more than 80,000 people die from the disease globally.EMPLICITI (elotuzumab) INDICATIONS & IMPORTANT SAFETY INFORMATION.INDICATION.EMPLICITI™ (elotuzumab) is indicated in combination with lenalidomide and dexamethasone for the treatment of patients with multiple myeloma who have received one to three prior therapies.IMPORTANT SAFETY INFORMATION.Infusion Reactions.EMPLICITI can cause infusion reactions.Common symptoms include fever, chills, and hypertension.Bradycardia and hypotension also developed during infusions.In the trial, 5% of patients required interruption of the administration of EMPLICITI for a median of 25 minutes due to infusion reactions, and 1% of patients discontinued due to infusion reactions.Of the patients who experienced an infusion reaction, 70% (23 33) had them during the first dose.If a Grade 2 or higher infusion reaction occurs, interrupt the EMPLICITI infusion and institute appropriate medical and supportive measures.If the infusion reaction recurs, stop the EMPLICITI infusion and do not restart it on that day.Severe infusion reactions may require permanent discontinuation of EMPLICITI therapy and emergency treatment.Premedicate with dexamethasone, H1 Blocker, H2 Blocker, and acetaminophen prior to infusing with EMPLICITI.Infections.In a clinical trial of patients with multiple myeloma (N=635), infections were reported in 81.4% of patients in the EMPLICITI with lenalidomide dexamethasone arm (ERd) and 74.4% in the lenalidomide dexamethasone arm (Rd).Grade 3 4 infections were 28% (ERd) and 24.3% (Rd).Opportunistic infections were reported in 22% (ERd) and 12.9% (Rd).Fungal infections were 9.7% (ERd) and 5.4% (Rd).Herpes zoster was 13.5% (ERd) and 6.9% (Rd).Discontinuations due to infections were 3.5% (ERd) and 4.1% (Rd).Fatal infections were 2.5% (ERd) and 2.2% (Rd).Monitor patients for development of infections and treat promptly.Second Primary Malignancies.In a clinical trial of patients with multiple myeloma (N=635), invasive second primary malignancies (SPM) were 9.1% (ERd) and 5.7% (Rd).The rate of hematologic malignancies were the same between ERd and Rd treatment arms (1.6%).Solid tumors were reported in 3.5% (ERd) and 2.2% (Rd).Skin cancer was reported in 4.4% (ERd) and 2.8% (Rd).Monitor patients for the development of SPMs.Hepatotoxicity.Elevations in liver enzymes (AST ALT greater than 3 times the upper limit, total bilirubin greater than 2 times the upper limit, and alkaline phosphatase less than 2 times the upper limit) consistent with hepatotoxicity were 2.5% (ERd) and 0.6% (Rd).Two patients experiencing hepatotoxicity discontinued treatment; however, 6 out of 8 patients had resolution and continued treatment.Monitor liver enzymes periodically.Stop EMPLICITI upon Grade 3 or higher elevation of liver enzymes.After return to baseline values, continuation of treatment may be considered.Interference with Determination of Complete Response.EMPLICITI is a humanized IgG kappa monoclonal antibody that can be detected on both the serum protein electrophoresis and immunofixation assays used for the clinical monitoring of endogenous M protein.This interference can impact the determination of complete response and possibly relapse from complete response in patients with IgG kappa myeloma protein.Pregnancy Females and Males of Reproductive Potential.There are no studies with EMPLICITI with pregnant women to inform any drug associated risks.There is a risk of fetal harm, including severe life threatening human birth defects associated with lenalidomide and it is contraindicated for use in pregnancy.Refer to the lenalidomide full prescribing information for requirements regarding contraception and the prohibitions against blood and or sperm donation due to presence and transmission in blood and or semen and for additional information.Adverse Reactions.Infusion reactions were reported in approximately 10% of patients treated with EMPLICITI with lenalidomide and dexamethasone.All reports of infusion reaction were Grade 3 or lower.Grade 3 infusion reactions occurred in 1% of patients.Serious adverse reactions were 65.4% (ERd) and 56.5% (Rd).The most frequent serious adverse reactions in the ERd arm compared to the Rd arm were: pneumonia (15.4%, 11%), pyrexia (6.9%, 4.7%), respiratory tract infection (3.1%, 1.3%), anemia (2.8%, 1.9%), pulmonary embolism (3.1%, 2.5%), and acute renal failure (2.5%, 1.9%).The most common adverse reactions in ERd and Rd, respectively (>20%) were fatigue (61.6%, 51.7%), diarrhea (46.9%, 36.0%), pyrexia (37.4%, 24.6%), constipation (35.5%, 27.1%), cough (34.3%, 18.9%), peripheral neuropathy (26.7%, 20.8%), nasopharyngitis (24.5%, 19.2%), upper respiratory tract infection (22.6%, 17.4%), decreased appetite (20.8%, 12.6%), and pneumonia (20.1%, 14.2%).Please see the full Prescribing Information.here.About  the company   the company  is a global biopharmaceutical company whose mission is to discover, develop and deliver innovative medicines that help patients prevail over serious diseases.com bmsnews.the company  Forward Looking Statement._This press release contains \"forward looking statements\" as that term is defined in the Private Securities Litigation Reform Act of 1995 regarding the research, development and commercialization of pharmaceutical products.Such forward looking statements are based on current expectations and involve inherent risks and uncertainties, including factors that could delay, divert or change any of them, and could cause actual outcomes and results to differ materially from current expectations.No forward looking statement can be guaranteed.Forward looking statements in this press release should be evaluated together with the many uncertainties that affect  the company \\'s business, particularly those identified in the cautionary factors discussion in  the company \\'s Annual Report on Form 10 K for the year ended  in our Quarterly Reports on Form 10 Q and our Current Reports on Form 8 K.the company  undertakes no obligation to publicly update any forward looking statement, whether as a result of new information, future events or otherwise._.Endnotes:._Empliciti_ is a trademark of  the company.Revlimid is a registered trademark of Celgene Corporation.© 2015  the company.All rights reserved.View source version on businesswire'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "res.result()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj6mofzeqcqB"
      },
      "source": [
        "### Anwenden der filter_body-Funktion auf alle Reihen:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ddf[\"time\"]"
      ],
      "metadata": {
        "id": "ihyfbNF4x1qw",
        "outputId": "8ab694e1-4949-4791-c06b-b6f92309bda3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime64[ns, pytz.FixedOffset(-240)]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tqzGy3bMNGs"
      },
      "outputs": [],
      "source": [
        "def handle_timezone(x):\n",
        "    try:\n",
        "        return x.tz_convert(\"US/Eastern\")\n",
        "    except Exception as e:\n",
        "        return x.tz_localize(\"US/Eastern\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qubEijBxMNGt"
      },
      "outputs": [],
      "source": [
        "# Still need to parse time correctly...\n",
        "ddf[\"time\"] = ddf[\"time\"].map(lambda x: handle_timezone(pd.to_datetime(x)), meta=pd.Series(dtype=\"datetime64[ns, US/Eastern]\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_and_reconnect_drive():\n",
        "    try:\n",
        "        # Check if Google Drive is still connected\n",
        "        os.listdir('/content/drive')\n",
        "    except:\n",
        "        # If not, reconnect it\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "jWL4ndv3kPq2"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "KGlOJPEvFxuP"
      },
      "outputs": [],
      "source": [
        "for pth in range(ddf.npartitions):\n",
        "  print(f\"Start {pth} partition\")\n",
        "  par = ddf.get_partition(pth)\n",
        "  par[\"parsed_body\"] = par.map_partitions(lambda y: y.apply(lambda x: filter_body(x),\n",
        "                                                  axis=1),\n",
        "                                                  meta=pd.Series(dtype=\"string\"))\n",
        "  par = client.persist(par)\n",
        "  par.to_parquet(f\"/data/processed_news/data-{pth}.parquet\")\n",
        "  check_and_reconnect_drive()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihzi516YMNGv"
      },
      "source": [
        "## Filtern von Newstiteln"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdbc2yRqMNGv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1DIhhqZMNGv"
      },
      "source": [
        "## Voranstellen von gefilterten Newstiteln an Nachrichtenkörper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFEujkJ-MNGw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaK5pLz4mRHF"
      },
      "source": [
        "## Analyse der durschnittlichen Tokenlänge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y192tnIfpIXj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0bW6kYmpI1W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VG5rnSiNpWVN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mc6lpRCpfVT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}