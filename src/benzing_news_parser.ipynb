{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkovyr-oc4-K"
      },
      "source": [
        "# Benzinga-Nachrichten-Verarbeitung"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A8GOctQmMfj"
      },
      "source": [
        "## Setup up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cluster spin up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, io, tarfile, os\n",
        "import subprocess\n",
        "from src.cloud_manager import spin_up_cluster, fn_to_targz_string\n",
        "from distributed.diagnostics.plugin import WorkerPlugin\n",
        "from dask.distributed import Client\n",
        "import dask\n",
        "###\n",
        "import dask.dataframe as dd\n",
        "import pandas as pd\n",
        "from src.preprocessing.news_parser import get_company_abbreviation, yahoo_get_wrapper, \\\n",
        "                                          infer_author, filter_body\n",
        "# import nltk\n",
        "\n",
        "# Mainly for topic modeling\n",
        "# from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# import gcsfs\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/opt/conda/lib/python3.10/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
              "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
              "</pre>\n"
            ],
            "text/plain": [
              "/opt/conda/lib/python3.10/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
              "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "ConnectTimeout",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/anyio/_core/_tasks.py:115\u001b[0m, in \u001b[0;36mfail_after\u001b[0;34m(delay, shield)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m get_async_backend()\u001b[39m.\u001b[39mcreate_cancel_scope(\n\u001b[1;32m    113\u001b[0m     deadline\u001b[39m=\u001b[39mdeadline, shield\u001b[39m=\u001b[39mshield\n\u001b[1;32m    114\u001b[0m ) \u001b[39mas\u001b[39;00m cancel_scope:\n\u001b[0;32m--> 115\u001b[0m     \u001b[39myield\u001b[39;00m cancel_scope\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m cancel_scope\u001b[39m.\u001b[39mcancelled_caught \u001b[39mand\u001b[39;00m current_time() \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m cancel_scope\u001b[39m.\u001b[39mdeadline:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpcore/_backends/anyio.py:114\u001b[0m, in \u001b[0;36mAnyIOBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mwith\u001b[39;00m anyio\u001b[39m.\u001b[39mfail_after(timeout):\n\u001b[0;32m--> 114\u001b[0m     stream: anyio\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mByteStream \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m anyio\u001b[39m.\u001b[39mconnect_tcp(\n\u001b[1;32m    115\u001b[0m         remote_host\u001b[39m=\u001b[39mhost,\n\u001b[1;32m    116\u001b[0m         remote_port\u001b[39m=\u001b[39mport,\n\u001b[1;32m    117\u001b[0m         local_host\u001b[39m=\u001b[39mlocal_address,\n\u001b[1;32m    118\u001b[0m     )\n\u001b[1;32m    119\u001b[0m     \u001b[39m# By default TCP sockets opened in `asyncio` include TCP_NODELAY.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/anyio/_core/_sockets.py:217\u001b[0m, in \u001b[0;36mconnect_tcp\u001b[0;34m(remote_host, remote_port, local_host, tls, ssl_context, tls_standard_compatible, tls_hostname, happy_eyeballs_delay)\u001b[0m\n\u001b[1;32m    216\u001b[0m oserrors: \u001b[39mlist\u001b[39m[\u001b[39mOSError\u001b[39;00m] \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 217\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mwith\u001b[39;00m create_task_group() \u001b[39mas\u001b[39;00m tg:\n\u001b[1;32m    218\u001b[0m     \u001b[39mfor\u001b[39;00m i, (af, addr) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(target_addrs):\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/anyio/_backends/_asyncio.py:668\u001b[0m, in \u001b[0;36mTaskGroup.__aexit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    667\u001b[0m     \u001b[39mif\u001b[39;00m exc_val \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m ignore_exception:\n\u001b[0;32m--> 668\u001b[0m         \u001b[39mraise\u001b[39;00m cancelled_exc_while_waiting_tasks\n\u001b[1;32m    670\u001b[0m \u001b[39mreturn\u001b[39;00m ignore_exception\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/anyio/_backends/_asyncio.py:648\u001b[0m, in \u001b[0;36mTaskGroup.__aexit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 648\u001b[0m     \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mwait(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_scope\u001b[39m.\u001b[39m_tasks)\n\u001b[1;32m    649\u001b[0m \u001b[39mexcept\u001b[39;00m CancelledError \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    650\u001b[0m     \u001b[39m# This task was cancelled natively; reraise the CancelledError later\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[39m# unless this task was already interrupted by another exception\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/tasks.py:384\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(fs, timeout, return_when)\u001b[0m\n\u001b[1;32m    382\u001b[0m fs \u001b[39m=\u001b[39m {ensure_future(f, loop\u001b[39m=\u001b[39mloop) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fs}\n\u001b[0;32m--> 384\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m _wait(fs, timeout, return_when, loop)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/tasks.py:491\u001b[0m, in \u001b[0;36m_wait\u001b[0;34m(fs, timeout, return_when, loop)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 491\u001b[0m     \u001b[39mawait\u001b[39;00m waiter\n\u001b[1;32m    492\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
            "\u001b[0;31mCancelledError\u001b[0m: Cancelled by cancel scope 7f0b06ad9cc0",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpcore/_exceptions.py:10\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:  \u001b[39m# noqa: PIE786\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpcore/_backends/anyio.py:113\u001b[0m, in \u001b[0;36mAnyIOBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mwith\u001b[39;00m anyio\u001b[39m.\u001b[39mfail_after(timeout):\n\u001b[1;32m    114\u001b[0m         stream: anyio\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mByteStream \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m anyio\u001b[39m.\u001b[39mconnect_tcp(\n\u001b[1;32m    115\u001b[0m             remote_host\u001b[39m=\u001b[39mhost,\n\u001b[1;32m    116\u001b[0m             remote_port\u001b[39m=\u001b[39mport,\n\u001b[1;32m    117\u001b[0m             local_host\u001b[39m=\u001b[39mlocal_address,\n\u001b[1;32m    118\u001b[0m         )\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen\u001b[39m.\u001b[39;49mthrow(typ, value, traceback)\n\u001b[1;32m    154\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/anyio/_core/_tasks.py:118\u001b[0m, in \u001b[0;36mfail_after\u001b[0;34m(delay, shield)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m cancel_scope\u001b[39m.\u001b[39mcancelled_caught \u001b[39mand\u001b[39;00m current_time() \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m cancel_scope\u001b[39m.\u001b[39mdeadline:\n\u001b[0;32m--> 118\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
            "\u001b[0;31mTimeoutError\u001b[0m: ",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpx/_transports/default.py:66\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpx/_transports/default.py:366\u001b[0m, in \u001b[0;36mAsyncHTTPTransport.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 366\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool\u001b[39m.\u001b[39mhandle_async_request(req)\n\u001b[1;32m    368\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(resp\u001b[39m.\u001b[39mstream, typing\u001b[39m.\u001b[39mAsyncIterable)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpcore/_async/connection_pool.py:268\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponse_closed(status)\n\u001b[0;32m--> 268\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    269\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpcore/_async/connection_pool.py:251\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m connection\u001b[39m.\u001b[39mhandle_async_request(request)\n\u001b[1;32m    252\u001b[0m \u001b[39mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    253\u001b[0m     \u001b[39m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[39m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[39m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[39m# up as HTTP/1.1.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpcore/_async/connection.py:99\u001b[0m, in \u001b[0;36mAsyncHTTPConnection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connect_failed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m         \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    100\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection\u001b[39m.\u001b[39mis_available():\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpcore/_async/connection.py:76\u001b[0m, in \u001b[0;36mAsyncHTTPConnection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     stream \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connect(request)\n\u001b[1;32m     78\u001b[0m     ssl_object \u001b[39m=\u001b[39m stream\u001b[39m.\u001b[39mget_extra_info(\u001b[39m\"\u001b[39m\u001b[39mssl_object\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpcore/_async/connection.py:124\u001b[0m, in \u001b[0;36mAsyncHTTPConnection._connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mwith\u001b[39;00m Trace(\u001b[39m\"\u001b[39m\u001b[39mconnect_tcp\u001b[39m\u001b[39m\"\u001b[39m, logger, request, kwargs) \u001b[39mas\u001b[39;00m trace:\n\u001b[0;32m--> 124\u001b[0m     stream \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_network_backend\u001b[39m.\u001b[39mconnect_tcp(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    125\u001b[0m     trace\u001b[39m.\u001b[39mreturn_value \u001b[39m=\u001b[39m stream\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpcore/_backends/auto.py:30\u001b[0m, in \u001b[0;36mAutoBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_backend()\n\u001b[0;32m---> 30\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mconnect_tcp(\n\u001b[1;32m     31\u001b[0m     host,\n\u001b[1;32m     32\u001b[0m     port,\n\u001b[1;32m     33\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m     34\u001b[0m     local_address\u001b[39m=\u001b[39mlocal_address,\n\u001b[1;32m     35\u001b[0m     socket_options\u001b[39m=\u001b[39msocket_options,\n\u001b[1;32m     36\u001b[0m )\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpcore/_backends/anyio.py:112\u001b[0m, in \u001b[0;36mAnyIOBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    107\u001b[0m exc_map \u001b[39m=\u001b[39m {\n\u001b[1;32m    108\u001b[0m     \u001b[39mTimeoutError\u001b[39;00m: ConnectTimeout,\n\u001b[1;32m    109\u001b[0m     \u001b[39mOSError\u001b[39;00m: ConnectError,\n\u001b[1;32m    110\u001b[0m     anyio\u001b[39m.\u001b[39mBrokenResourceError: ConnectError,\n\u001b[1;32m    111\u001b[0m }\n\u001b[0;32m--> 112\u001b[0m \u001b[39mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    113\u001b[0m     \u001b[39mwith\u001b[39;00m anyio\u001b[39m.\u001b[39mfail_after(timeout):\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen\u001b[39m.\u001b[39;49mthrow(typ, value, traceback)\n\u001b[1;32m    154\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpcore/_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(exc, from_exc):\n\u001b[0;32m---> 14\u001b[0m         \u001b[39mraise\u001b[39;00m to_exc(exc) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mraise\u001b[39;00m\n",
            "\u001b[0;31mConnectTimeout\u001b[0m: ",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/workspaces/trading_bot/src/benzing_news_parser.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f626565722f446f63756d656e74732f47697468756250726f6a656b74652f74726164696e675f626f74222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f686f6d652f626565722f446f63756d656e74732f47697468756250726f6a656b74652f74726164696e675f626f742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f686f6d652f626565722f446f63756d656e74732f47697468756250726f6a656b74652f74726164696e675f626f742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f686f6d652f626565722f446f63756d656e74732f47697468756250726f6a656b74652f74726164696e675f626f742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/trading_bot/src/benzing_news_parser.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdask_kubernetes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moperator\u001b[39;00m \u001b[39mimport\u001b[39;00m KubeCluster\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f626565722f446f63756d656e74732f47697468756250726f6a656b74652f74726164696e675f626f74222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f686f6d652f626565722f446f63756d656e74732f47697468756250726f6a656b74652f74726164696e675f626f742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f686f6d652f626565722f446f63756d656e74732f47697468756250726f6a656b74652f74726164696e675f626f742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f686f6d652f626565722f446f63756d656e74732f47697468756250726f6a656b74652f74726164696e675f626f742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/trading_bot/src/benzing_news_parser.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m cluster \u001b[39m=\u001b[39m KubeCluster()\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/dask_kubernetes/operator/kubecluster/kubecluster.py:274\u001b[0m, in \u001b[0;36mKubeCluster.__init__\u001b[0;34m(self, name, namespace, image, n_workers, resources, env, worker_command, port_forward_cluster_ip, create_mode, shutdown_on_close, idle_timeout, resource_timeout, scheduler_service_type, custom_cluster_spec, scheduler_forward_port, jupyter, loop, asynchronous, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m called_from_running_loop:\n\u001b[1;32m    273\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loop_runner\u001b[39m.\u001b[39mstart()\n\u001b[0;32m--> 274\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msync(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_start)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/distributed/utils.py:357\u001b[0m, in \u001b[0;36mSyncMethodMixin.sync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m future\n\u001b[1;32m    356\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 357\u001b[0m     \u001b[39mreturn\u001b[39;00m sync(\n\u001b[1;32m    358\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloop, func, \u001b[39m*\u001b[39;49margs, callback_timeout\u001b[39m=\u001b[39;49mcallback_timeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    359\u001b[0m     )\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/distributed/utils.py:424\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39mif\u001b[39;00m error:\n\u001b[1;32m    423\u001b[0m     typ, exc, tb \u001b[39m=\u001b[39m error\n\u001b[0;32m--> 424\u001b[0m     \u001b[39mraise\u001b[39;00m exc\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    425\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    426\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/distributed/utils.py:397\u001b[0m, in \u001b[0;36msync.<locals>.f\u001b[0;34m()\u001b[0m\n\u001b[1;32m    395\u001b[0m         future \u001b[39m=\u001b[39m wait_for(future, callback_timeout)\n\u001b[1;32m    396\u001b[0m     future \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(future)\n\u001b[0;32m--> 397\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39myield\u001b[39;00m future\n\u001b[1;32m    398\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    399\u001b[0m     error \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mexc_info()\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tornado/gen.py:767\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    766\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 767\u001b[0m         value \u001b[39m=\u001b[39m future\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    768\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    769\u001b[0m         \u001b[39m# Save the exception for later. It's important that\u001b[39;00m\n\u001b[1;32m    770\u001b[0m         \u001b[39m# gen.throw() not be called inside this try/except block\u001b[39;00m\n\u001b[1;32m    771\u001b[0m         \u001b[39m# because that makes sys.exc_info behave unexpectedly.\u001b[39;00m\n\u001b[1;32m    772\u001b[0m         exc: Optional[\u001b[39mException\u001b[39;00m] \u001b[39m=\u001b[39m e\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/dask_kubernetes/operator/kubecluster/kubecluster.py:298\u001b[0m, in \u001b[0;36mKubeCluster._start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m     show_rich_output_task \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mcreate_task(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_show_rich_output())\n\u001b[1;32m    297\u001b[0m cluster \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m DaskCluster(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, namespace\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamespace)\n\u001b[0;32m--> 298\u001b[0m cluster_exists \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m cluster\u001b[39m.\u001b[39mexists()\n\u001b[1;32m    300\u001b[0m \u001b[39mif\u001b[39;00m cluster_exists \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_mode \u001b[39m==\u001b[39m CreateMode\u001b[39m.\u001b[39mCREATE_ONLY:\n\u001b[1;32m    301\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCluster \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m already exists and create mode is \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mCreateMode\u001b[39m.\u001b[39mCREATE_ONLY\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m     )\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kr8s/_objects.py:220\u001b[0m, in \u001b[0;36mAPIObject.exists\u001b[0;34m(self, ensure)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mexists\u001b[39m(\u001b[39mself\u001b[39m, ensure\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m    219\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check if this object exists in Kubernetes.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exists(ensure\u001b[39m=\u001b[39mensure)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kr8s/_objects.py:225\u001b[0m, in \u001b[0;36mAPIObject._exists\u001b[0;34m(self, ensure)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check if this object exists in Kubernetes.\"\"\"\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[39masync\u001b[39;00m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39mcall_api(\n\u001b[1;32m    226\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mGET\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    227\u001b[0m         version\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mversion,\n\u001b[1;32m    228\u001b[0m         url\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendpoint\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    229\u001b[0m         namespace\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamespace,\n\u001b[1;32m    230\u001b[0m         raise_for_status\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    231\u001b[0m     ) \u001b[39mas\u001b[39;00m resp:\n\u001b[1;32m    232\u001b[0m         status \u001b[39m=\u001b[39m resp\u001b[39m.\u001b[39mstatus_code\n\u001b[1;32m    233\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/contextlib.py:199\u001b[0m, in \u001b[0;36m_AsyncGeneratorContextManager.__aenter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[1;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m anext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen)\n\u001b[1;32m    200\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopAsyncIteration\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kr8s/_api.py:133\u001b[0m, in \u001b[0;36mApi.call_api\u001b[0;34m(self, method, version, base, namespace, url, raise_for_status, stream, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[39myield\u001b[39;00m response\n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_session\u001b[39m.\u001b[39mrequest(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    134\u001b[0m     \u001b[39mif\u001b[39;00m raise_for_status:\n\u001b[1;32m    135\u001b[0m         response\u001b[39m.\u001b[39mraise_for_status()\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpx/_client.py:1530\u001b[0m, in \u001b[0;36mAsyncClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[39mBuild and send a request.\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m[0]: /advanced/#merging-of-configuration\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m request \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild_request(\n\u001b[1;32m   1518\u001b[0m     method\u001b[39m=\u001b[39mmethod,\n\u001b[1;32m   1519\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1528\u001b[0m     extensions\u001b[39m=\u001b[39mextensions,\n\u001b[1;32m   1529\u001b[0m )\n\u001b[0;32m-> 1530\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(request, auth\u001b[39m=\u001b[39mauth, follow_redirects\u001b[39m=\u001b[39mfollow_redirects)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpx/_client.py:1617\u001b[0m, in \u001b[0;36mAsyncClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m   1609\u001b[0m follow_redirects \u001b[39m=\u001b[39m (\n\u001b[1;32m   1610\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfollow_redirects\n\u001b[1;32m   1611\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m   1612\u001b[0m     \u001b[39melse\u001b[39;00m follow_redirects\n\u001b[1;32m   1613\u001b[0m )\n\u001b[1;32m   1615\u001b[0m auth \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m-> 1617\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send_handling_auth(\n\u001b[1;32m   1618\u001b[0m     request,\n\u001b[1;32m   1619\u001b[0m     auth\u001b[39m=\u001b[39mauth,\n\u001b[1;32m   1620\u001b[0m     follow_redirects\u001b[39m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1621\u001b[0m     history\u001b[39m=\u001b[39m[],\n\u001b[1;32m   1622\u001b[0m )\n\u001b[1;32m   1623\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1624\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpx/_client.py:1645\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1642\u001b[0m request \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m auth_flow\u001b[39m.\u001b[39m\u001b[39m__anext__\u001b[39m()\n\u001b[1;32m   1644\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1645\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m   1646\u001b[0m         request,\n\u001b[1;32m   1647\u001b[0m         follow_redirects\u001b[39m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1648\u001b[0m         history\u001b[39m=\u001b[39mhistory,\n\u001b[1;32m   1649\u001b[0m     )\n\u001b[1;32m   1650\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1651\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpx/_client.py:1682\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1679\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mrequest\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m   1680\u001b[0m     \u001b[39mawait\u001b[39;00m hook(request)\n\u001b[0;32m-> 1682\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send_single_request(request)\n\u001b[1;32m   1683\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1684\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m]:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpx/_client.py:1719\u001b[0m, in \u001b[0;36mAsyncClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1715\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1716\u001b[0m     )\n\u001b[1;32m   1718\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39mrequest):\n\u001b[0;32m-> 1719\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m transport\u001b[39m.\u001b[39mhandle_async_request(request)\n\u001b[1;32m   1721\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(response\u001b[39m.\u001b[39mstream, AsyncByteStream)\n\u001b[1;32m   1722\u001b[0m response\u001b[39m.\u001b[39mrequest \u001b[39m=\u001b[39m request\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpx/_transports/default.py:365\u001b[0m, in \u001b[0;36mAsyncHTTPTransport.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(request\u001b[39m.\u001b[39mstream, AsyncByteStream)\n\u001b[1;32m    353\u001b[0m req \u001b[39m=\u001b[39m httpcore\u001b[39m.\u001b[39mRequest(\n\u001b[1;32m    354\u001b[0m     method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    355\u001b[0m     url\u001b[39m=\u001b[39mhttpcore\u001b[39m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m     extensions\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    364\u001b[0m )\n\u001b[0;32m--> 365\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m    366\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool\u001b[39m.\u001b[39mhandle_async_request(req)\n\u001b[1;32m    368\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(resp\u001b[39m.\u001b[39mstream, typing\u001b[39m.\u001b[39mAsyncIterable)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    151\u001b[0m     value \u001b[39m=\u001b[39m typ()\n\u001b[1;32m    152\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen\u001b[39m.\u001b[39;49mthrow(typ, value, traceback)\n\u001b[1;32m    154\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m exc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m value\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/httpx/_transports/default.py:83\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[39mraise\u001b[39;00m\n\u001b[1;32m     82\u001b[0m message \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(exc)\n\u001b[0;32m---> 83\u001b[0m \u001b[39mraise\u001b[39;00m mapped_exc(message) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
            "\u001b[0;31mConnectTimeout\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from dask_kubernetes.operator import KubeCluster\n",
        "cluster = KubeCluster()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster = spin_up_cluster()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cant extract this out of the notebook as then import would be missing...\n",
        "def extract_targz_string(s,*args,**kwargs):\n",
        "    with io.BytesIO() as bt:\n",
        "        bt.write(s) \n",
        "        bt.seek(0)\n",
        "        with tarfile.open(fileobj=bt,mode='r:gz') as tf:\n",
        "            tf.extractall(*args,**kwargs)\n",
        "\n",
        "class SourceDirectoryCopier(WorkerPlugin):\n",
        "    def __init__(self, targz):\n",
        "      self.targz = targz\n",
        "      super().__init__()\n",
        "\n",
        "    def setup(self, worker=None, **kwargs):\n",
        "        self.worker = worker\n",
        "        for elem in self.targz:\n",
        "            extract_targz_string(elem)\n",
        "        sys.path.append(\"/\")\n",
        "        print(\"Added a new worker at:\", worker)\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"loky\"], check=True)\n",
        "\n",
        "\n",
        "targz = [fn_to_targz_string(\"src\"), fn_to_targz_string(\"extreme-lore-398917-ac46de419eb2.json\")]\n",
        "plugin = SourceDirectoryCopier(targz)\n",
        "client: Client = Client(cluster)\n",
        "client.run_on_scheduler(plugin.setup)\n",
        "client.register_plugin(plugin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(cluster.dashboard_link)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/workspaces/trading_bot/extreme-lore-398917-ac46de419eb2.json\"\n",
        "os.environ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from os import listdir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# client.run_on_scheduler(listdir, \"/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CymECzDhh6Mn"
      },
      "source": [
        "### System settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4lfXyeQvfCn"
      },
      "outputs": [],
      "source": [
        "using_colab = False\n",
        "using_laptop = True\n",
        "import os\n",
        "\n",
        "%%capture\n",
        "if using_colab:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCMMsTZyC0BE"
      },
      "source": [
        "### CUDF für hardware acceleration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqFqwisUh6Mn"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "# !python rapidsai-csp-utils/colab/pip-install.py\n",
        "# import cudf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7IaohEqCoiM"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlMR8ADYv3Iy"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "if using_colab:\n",
        "  !sudo apt update\n",
        "  !sudo apt install maven;\n",
        "\n",
        "  !pip install pyarrow==11.0.0\n",
        "  !pip install html2text\n",
        "  !pip install datefinder\n",
        "  !pip install -U dask[complete]\n",
        "  !pip install nltk\n",
        "  !pip install dateparser\n",
        "  !pip install pyngrok\n",
        "  !pip install sutime\n",
        "  !pip install pyngrok\n",
        "\n",
        "  # This is required for sutime\n",
        "  !mvn dependency:copy-dependencies -DoutputDirectory=./jars -f $(python3 -c 'import importlib; import pathlib; print(pathlib.Path(importlib.util.find_spec(\"sutime\").origin).parent / \"pom.xml\")');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVUeXGml6-Si"
      },
      "source": [
        "### Setup NGROK tunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiY7l2l9685M",
        "outputId": "c8ad5229-cafa-48b1-c33e-20de5178dd3f"
      },
      "outputs": [],
      "source": [
        "# ------- Only required when working on google colab, not required when working with google cloud engine -----------------\n",
        "if using_colab:\n",
        "  from pyngrok import ngrok, conf\n",
        "  conf.get_default().auth_token = \"2WntwErWDt9LxQ2Jfp6C8OxDAMK_7iZVdC1utyZET1PE8cuUg\"\n",
        "\n",
        "  public_url = ngrok.connect(8787).public_url\n",
        "  print(\" * ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, 8787))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U69xWIixZSM",
        "outputId": "79053918-c60b-40f6-c77b-06069a9f0f7e"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "if 'client' not in globals():\n",
        "  !python -m pip install jupyter-server-proxy\n",
        "  client = Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRqa0EPy_lpx"
      },
      "source": [
        "## Grobes HTML-Parsing\n",
        "Als erstes müssen wir die HTML-Dokumente zu normalem Text umwandeln, ansonsten sind die Text-Zellen zu groß und führen zu Problemen mit PyArrow/Dask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neM6jNH6NAnM",
        "outputId": "ef8b18b3-9cdb-4cb1-8d8e-ed4d38fb97ef"
      },
      "outputs": [],
      "source": [
        "dask.config.set(scheduler=\"threads\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4rQifxX6ygR"
      },
      "outputs": [],
      "source": [
        "input_dir = \"data/raw_bzg/\"\n",
        "output_dir = 'data/unraw1_bzg/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wy5-WZ4B_A2"
      },
      "outputs": [],
      "source": [
        "# for year in range(2019, 2020):\n",
        "#     print(year)\n",
        "#     df = pd.read_parquet(f\"{input_dir}story_df_raw_{year}.parquet\")\n",
        "#     df = dd.from_pandas(df, npartitions=12)\n",
        "#     df[\"html_body\"] = df[\"html_body\"].apply(body_formatter, meta=pd.Series(dtype=\"str\"))\n",
        "#     df = df.rename(columns={\"html_body\":\"body\"})\n",
        "#     name_function = lambda x: f\"data-{year}-{x}.parquet\"\n",
        "#     df.to_parquet(output_dir, name_function=name_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa6QpqfR_t97"
      },
      "source": [
        "## Neu-Partitionierug\n",
        "Sodass alle Partitionen etwa die gleiche Größe haben."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHFpynyiq3O4"
      },
      "outputs": [],
      "source": [
        "input_dir = 'data/unraw1_bzg/'\n",
        "output_dir = 'data/unraw2_bzg/'\n",
        "\n",
        "# ddf = dd.read_parquet(input_dir+\"*.parquet\")\n",
        "# ddf2 = ddf.repartition(npartitions=50)\n",
        "# name_function = lambda x: f\"data-{x}.parquet\"\n",
        "# ddf2.to_parquet(output_dir, name_function=name_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFnIFQW2Nag7"
      },
      "source": [
        "## Author-Inferenz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__EYo7CaFlVj"
      },
      "source": [
        "Ein bisschen die Daten säubern..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-jzA2AbFptW"
      },
      "outputs": [],
      "source": [
        "input_dir = cwd+'/data/unraw2_bzg/'\n",
        "output_dir = cwd+'/data/unraw3_bzg/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef28p8AOnrFD"
      },
      "outputs": [],
      "source": [
        "ddf = dd.read_parquet(input_dir+\"*.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylwarwpXLegE"
      },
      "outputs": [],
      "source": [
        "# Remove rows for which noo stock ticker is recorded\n",
        "ddf = ddf[ddf.stocks != '']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7OPUatcMS7C"
      },
      "outputs": [],
      "source": [
        "# Convert `channels`  datatype from string to list\n",
        "ddf[\"channels\"] = ddf[\"channels\"].apply(eval, meta=pd.Series(dtype='object'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu1EhTqrVpBG"
      },
      "source": [
        "Untersuche als nächstes die Behauptung, dass **PRNewswire** und **Businesswire** den gesamten Markt für Pressemeldungen in den USA kontrollieren. Wenn dem so ist, und sie nicht noch weitere, unwichtige Meldungen veröffentlichen, dann können wir einfach die Newsartikel nach diesen Autoren filtern und uns viel Arbeit ersparen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5mO091MfO6e"
      },
      "outputs": [],
      "source": [
        "dask.config.set(scheduler=\"processes\")\n",
        "ddf[\"inferred_author\"] = None\n",
        "ddf[\"inferred_author\"] = ddf.body.apply(infer_author, meta=pd.Series(dtype=\"string\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTcPjnU0VZZu"
      },
      "outputs": [],
      "source": [
        "# value_counts for authors\n",
        "auhtor_value_counts = pd.concat([ddf.author.value_counts().head(10), ddf.inferred_author.value_counts().head(10)], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "UNesN9jX1IP0",
        "outputId": "1b8d2697-13fd-4d33-cc0f-1f7ce3cb4ee8"
      },
      "outputs": [],
      "source": [
        "auhtor_value_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKT6tusBp13y",
        "outputId": "07dcdac3-737a-48e0-a3dd-bf7a1f83d028"
      },
      "outputs": [],
      "source": [
        "auhtor_value_counts.sum().diff()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk6p6woeqJqP"
      },
      "source": [
        "Ungefähr 650k Nachrichten werden ausgelassen, wenn nur die vier Hauptvertreiber von Pressemeldungen berücksichtigt werden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X8rHo-CsDCQ"
      },
      "outputs": [],
      "source": [
        "ddf = ddf[~ddf.inferred_author.isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHvzym8sFcX_"
      },
      "outputs": [],
      "source": [
        "ddf[\"inferred_author\"] = ddf[\"inferred_author\"].astype(\"string\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufk8_XUjFwdk"
      },
      "outputs": [],
      "source": [
        "ddf[\"channels\"] = ddf.channels.apply(lambda x: str(x), meta=pd.Series(dtype=\"string\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsEmViTzNvgs",
        "outputId": "2ed92bc7-5f41-4e52-9614-b8e6dddacd84"
      },
      "outputs": [],
      "source": [
        "ddf.inferred_author.value_counts().compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfNNyD_31sly",
        "outputId": "1647ed45-d50f-48c2-ddc5-198135f50d34"
      },
      "outputs": [],
      "source": [
        "ddf.inferred_author.value_counts().sum().compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v04Wy5Y4iudn"
      },
      "outputs": [],
      "source": [
        "name_function = lambda x: f\"data-{x}.parquet\"\n",
        "ddf.to_parquet(output_dir, name_function=name_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY5ZI0hQitRh"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLdnwdyxNFiH"
      },
      "outputs": [],
      "source": [
        "# Contains 100k rows\n",
        "earnings_ddf = ddf[ddf.channels.apply(lambda x: \"Earnings\" in x, meta=pd.Series(dtype=bool))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-zs5SSiVbyp",
        "outputId": "2569a9dd-dceb-433e-fefe-44ee9abaec20"
      },
      "outputs": [],
      "source": [
        "# value counts for authors of earnings reports (contrast to value counts of all news articles)\n",
        "earnings_ddf.inferred_author.value_counts().head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dVD6j-G9nie"
      },
      "source": [
        "Hier sehen wir, dass es keine einzige Pressemeldung von **Business Wire** gibt, die mit *Earnings* gekennzeichnet sind. Trotzdem gibt es relevante *Earnings* reports von Business Wire. Dies habe ich kurz verifiziert..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jafqZ87tXDKn"
      },
      "source": [
        "## Vorgehen\n",
        "\n",
        "Benötigt:\n",
        "---------\n",
        "- data/tickers.pkl (alt) \\\\\n",
        "- data_shared/corporation_endings.txt \\\\\n",
        "- input_dir = data/unraw3_bzg/ \\\\\n",
        "- output_dir = data/unraw4_bzg/\n",
        "\n",
        "Produziert:\n",
        "-----------\n",
        "- data_shared/all_ticker_name_mapper.parquet\n",
        "\n",
        "--------------\n",
        "\n",
        "Wie viele Nachrichten bleiben, wenn wir auf relevante Ticker filtern? Wir wollen nicht(!) - so ist es momentan - auf die momentane Russell 3k-Zusammensetzung filtern, denn wir wollen auch ungelistete bzw. ehemalige Russell-Aktien beachten.\n",
        "\n",
        "\n",
        "**1. Full-Name-Discovery:**\n",
        "\n",
        "Herausfinden des vollen Namens des Unternehmens für jeden Ticker, damit 1. der Text richtig geparst werden kann und 2. damit wir einen Anhaltspunkt für das Ticker-Grouping haben.\n",
        "\n",
        "\n",
        "**2. Ticker-Filtering:**\n",
        "\n",
        "Alle Ticker herausfiltern, die wir nicht brauchen. Wenn wir aber ein großes Aktienuniversum (mit inzwischen ungelisteten Aktien) benutzen, werden wir fast alle Nachrichten behalten können. Allerdings lassen sich so Fehlerhafte Nachrichten/Ticker etc. herausfiltern.\n",
        "\n",
        "\n",
        "**3. Ticker-Grouping:**\n",
        "\n",
        "Was machen wir, wenn wir mehrerer Aktiengattungen für ein Unternehmen haben? Z.B. Vorzugs- und Stammaktien. Wir können i.A. die Stammaktie nehmen, da diese normalerweise ein höheres Handelsvolumen aufweist. D.h. wir bilden alle Ticker der Benzinga-Nachrichten auf den Ticker der Stammaktie ab.\n",
        "\n",
        "\n",
        "**4. Firmennamen-Nachrichtenkörper-Verifikation:**\n",
        "\n",
        "Da Ticker wiederverwendet werden können bzw. sich verändern können wollen wir sicherstellen, dass der Unternehmensname im Nachrichtenkörper vorkommt! Bzw. generell ist das eine gute Datensäuberungs-Maßnahme."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaSble_1ku1t"
      },
      "outputs": [],
      "source": [
        "input_dir = cwd+'/data/unraw3_bzg/'\n",
        "output_dir = cwd+'/data/unraw4_bzg/'\n",
        "ddf = dd.read_parquet(input_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JosrB0RfsUsy"
      },
      "outputs": [],
      "source": [
        "all_tickers = ddf.stocks.unique().compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdwKGnsPnkYS"
      },
      "source": [
        "### Full-Name-Discovery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8urd8wvsXza"
      },
      "outputs": [],
      "source": [
        "company_names = all_tickers.apply(lambda x: yahoo_get_wrapper(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUslIwYdsmU7"
      },
      "outputs": [],
      "source": [
        "all_mapper = pd.concat([all_tickers, company_names], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tW8AGmgfK_oV"
      },
      "outputs": [],
      "source": [
        "all_mapper.columns = [\"ticker\", \"company_name\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qa3sz1yBMkjw"
      },
      "outputs": [],
      "source": [
        "all_mapper = all_mapper.dropna() # This drops like half of the tickers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "_O1hNOdoNUIs",
        "outputId": "baf61a9c-1701-4fe0-8130-2542364b5df1"
      },
      "outputs": [],
      "source": [
        "all_mapper[all_mapper.company_name.apply(lambda x: \"Alphabet\" in x)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMO7H_z0Nosk",
        "outputId": "7b43b771-6a28-4f4c-b996-2198695a218b"
      },
      "outputs": [],
      "source": [
        "print(len(all_mapper))\n",
        "print(len(all_mapper.company_name.unique()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ee1RuxlPpD1"
      },
      "outputs": [],
      "source": [
        "vcs = all_mapper.company_name.value_counts()\n",
        "vcs = vcs[vcs >= 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "5FEP_5x3P9F2",
        "outputId": "35345a74-a533-4b27-d617-826b14866af6"
      },
      "outputs": [],
      "source": [
        "# Ticker-Grouping\n",
        "all_mapper[all_mapper.company_name.isin(vcs.index)].sort_values(\"company_name\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0vWtCfFSoox"
      },
      "source": [
        "Es ist nicht leicht zu sagen, welchen von den Tickern wir bevorzugen sollten. Abgleichen mit den Aktientickern des Kursdatensatzes notwendig, um zu sehen, ob überhaupt nur ein Ticker übereinstimmt. Wenn es für beide Ticker eine Kurszeitreihe gibt, dann sollten wir die nehmen, die ein höheres historisches Volumen hat. Dies ist allerdings etwas, was wir später machen und nicht jetzt. Hier wollen wir zunächst nur die Nachrichten verarbeiten, weswegen wir nur die NaN-Unternehmen rausnehmen und den Rest - *ohne Ticker-Filtering* - weiterverarbeiten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6J-M2i4WeW2"
      },
      "outputs": [],
      "source": [
        "mapper = all_mapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvJpTNzVFmLZ"
      },
      "outputs": [],
      "source": [
        "mapper.columns = [\"ticker\", \"company_names\"]\n",
        "mapper = mapper[mapper.isna().sum(axis=1) == 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdRcHXQdGQ5M"
      },
      "outputs": [],
      "source": [
        "mapper = mapper.set_index(\"ticker\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t2omABfXZRK"
      },
      "outputs": [],
      "source": [
        "company_endings = pd.read_table(\"data_shared/corporation_endings.txt\").iloc[:, 0]\n",
        "# Apply get_company_abbreviation twice in order to get rid of Enterprise, Ltd.\n",
        "# Otherwise , Ltd. remains.\n",
        "mapper[\"short_name\"] = mapper.company_names.apply(lambda x: get_company_abbreviation(x, company_endings=company_endings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3d5wM_qWhvx",
        "outputId": "9cfa82b4-47f2-4395-8005-8ab9184bd904"
      },
      "outputs": [],
      "source": [
        "print(mapper.short_name.isna().sum()) # 2037 stocks for which we don't have an ending to abbreviate\n",
        "# mapper.loc[:, \"short_name\"] = mapper.short_name.fillna(mapper.company_names)\n",
        "mapper = mapper.applymap(lambda x: x.strip(\" \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voNHNOrXSIKL"
      },
      "outputs": [],
      "source": [
        "mapper.to_parquet(cwd + \"/data_shared/ticker_name_mapper.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrLjdrfwlmRm"
      },
      "outputs": [],
      "source": [
        "mapper = pd.read_parquet(cwd + \"/data_shared/ticker_name_mapper.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70POMUC-PN-n"
      },
      "outputs": [],
      "source": [
        "filt_ddf = ddf[ddf.stocks.isin(mapper.index.to_list())]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZszkSxWPBK9",
        "outputId": "46f900da-de14-469a-c618-fe235be49e4b"
      },
      "outputs": [],
      "source": [
        "ddf.shape[0].compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLbhWydOPaaK",
        "outputId": "c5381d2d-62dd-4291-975f-e6e060bf293b"
      },
      "outputs": [],
      "source": [
        "filt_ddf.shape[0].compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRN7QEojStnr"
      },
      "source": [
        "Es verbleiben circa 1 Mio. Nachrichten, für die wir den Ticker zu einem FIrmennamen auflösen können."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuLyEUaicPl_"
      },
      "source": [
        "Diese Nachrichten können wir nun wirklich parsen, und danach ordentlich kategorisieren."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSH8cQgL5tZu"
      },
      "outputs": [],
      "source": [
        "ddf = filt_ddf\n",
        "ddf = ddf.drop(columns=[\"author\"]).rename(columns={\"inferred_author\":\"author\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "safdLC-H6GR5"
      },
      "outputs": [],
      "source": [
        "# TODO: Kann effizienter werden mit GroupBy-Operation\n",
        "ddf[\"company_name\"] = ddf.stocks.apply(lambda x: mapper.company_names.loc[x], meta=pd.Series(dtype=\"string\"))\n",
        "ddf[\"short_name\"] = ddf.stocks.apply(lambda x: mapper.short_name.loc[x], meta=pd.Series(dtype=\"string\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTzB5osyPRnf"
      },
      "outputs": [],
      "source": [
        "name_function = lambda x: f\"data-{x}.parquet\"\n",
        "ddf.to_parquet(cwd+'/data/latest/', name_function=name_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F3dOc35YN8k"
      },
      "source": [
        "### Firmennamen-Nachrichtenkörper-Verifikation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "w91ht_CvPZDx"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: /workspaces/trading_bot/data/latest",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/dask/backends.py:136\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    137\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/dask/dataframe/io/parquet/core.py:538\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, columns, filters, categories, index, storage_options, engine, use_nullable_dtypes, dtype_backend, calculate_divisions, ignore_metadata_file, metadata_task_size, split_row_groups, blocksize, aggregate_files, parquet_file_extension, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m     blocksize \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m read_metadata_result \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39;49mread_metadata(\n\u001b[1;32m    539\u001b[0m     fs,\n\u001b[1;32m    540\u001b[0m     paths,\n\u001b[1;32m    541\u001b[0m     categories\u001b[39m=\u001b[39;49mcategories,\n\u001b[1;32m    542\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m    543\u001b[0m     use_nullable_dtypes\u001b[39m=\u001b[39;49muse_nullable_dtypes,\n\u001b[1;32m    544\u001b[0m     dtype_backend\u001b[39m=\u001b[39;49mdtype_backend,\n\u001b[1;32m    545\u001b[0m     gather_statistics\u001b[39m=\u001b[39;49mcalculate_divisions,\n\u001b[1;32m    546\u001b[0m     filters\u001b[39m=\u001b[39;49mfilters,\n\u001b[1;32m    547\u001b[0m     split_row_groups\u001b[39m=\u001b[39;49msplit_row_groups,\n\u001b[1;32m    548\u001b[0m     blocksize\u001b[39m=\u001b[39;49mblocksize,\n\u001b[1;32m    549\u001b[0m     aggregate_files\u001b[39m=\u001b[39;49maggregate_files,\n\u001b[1;32m    550\u001b[0m     ignore_metadata_file\u001b[39m=\u001b[39;49mignore_metadata_file,\n\u001b[1;32m    551\u001b[0m     metadata_task_size\u001b[39m=\u001b[39;49mmetadata_task_size,\n\u001b[1;32m    552\u001b[0m     parquet_file_extension\u001b[39m=\u001b[39;49mparquet_file_extension,\n\u001b[1;32m    553\u001b[0m     dataset\u001b[39m=\u001b[39;49mdataset_options,\n\u001b[1;32m    554\u001b[0m     read\u001b[39m=\u001b[39;49mread_options,\n\u001b[1;32m    555\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mother_options,\n\u001b[1;32m    556\u001b[0m )\n\u001b[1;32m    558\u001b[0m \u001b[39m# In the future, we may want to give the engine the\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[39m# option to return a dedicated element for `common_kwargs`.\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[39m# However, to avoid breaking the API, we just embed this\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[39m# data in the first element of `parts` for now.\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \u001b[39m# The logic below is inteded to handle backward and forward\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# compatibility with a user-defined engine.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/dask/dataframe/io/parquet/arrow.py:532\u001b[0m, in \u001b[0;36mArrowDatasetEngine.read_metadata\u001b[0;34m(cls, fs, paths, categories, index, use_nullable_dtypes, dtype_backend, gather_statistics, filters, split_row_groups, blocksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[39m# Stage 1: Collect general dataset information\u001b[39;00m\n\u001b[0;32m--> 532\u001b[0m dataset_info \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_collect_dataset_info(\n\u001b[1;32m    533\u001b[0m     paths,\n\u001b[1;32m    534\u001b[0m     fs,\n\u001b[1;32m    535\u001b[0m     categories,\n\u001b[1;32m    536\u001b[0m     index,\n\u001b[1;32m    537\u001b[0m     gather_statistics,\n\u001b[1;32m    538\u001b[0m     filters,\n\u001b[1;32m    539\u001b[0m     split_row_groups,\n\u001b[1;32m    540\u001b[0m     blocksize,\n\u001b[1;32m    541\u001b[0m     aggregate_files,\n\u001b[1;32m    542\u001b[0m     ignore_metadata_file,\n\u001b[1;32m    543\u001b[0m     metadata_task_size,\n\u001b[1;32m    544\u001b[0m     parquet_file_extension,\n\u001b[1;32m    545\u001b[0m     kwargs,\n\u001b[1;32m    546\u001b[0m )\n\u001b[1;32m    548\u001b[0m \u001b[39m# Stage 2: Generate output `meta`\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/dask/dataframe/io/parquet/arrow.py:1047\u001b[0m, in \u001b[0;36mArrowDatasetEngine._collect_dataset_info\u001b[0;34m(cls, paths, fs, categories, index, gather_statistics, filters, split_row_groups, blocksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, kwargs)\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[39mif\u001b[39;00m ds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1047\u001b[0m     ds \u001b[39m=\u001b[39m pa_ds\u001b[39m.\u001b[39;49mdataset(\n\u001b[1;32m   1048\u001b[0m         paths,\n\u001b[1;32m   1049\u001b[0m         filesystem\u001b[39m=\u001b[39;49m_wrapped_fs(fs),\n\u001b[1;32m   1050\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_processed_dataset_kwargs,\n\u001b[1;32m   1051\u001b[0m     )\n\u001b[1;32m   1053\u001b[0m \u001b[39m# Get file_frag sample and extract physical_schema\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyarrow/dataset.py:776\u001b[0m, in \u001b[0;36mdataset\u001b[0;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(_is_path_like(elem) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m source):\n\u001b[0;32m--> 776\u001b[0m     \u001b[39mreturn\u001b[39;00m _filesystem_dataset(source, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    777\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(elem, Dataset) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m source):\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyarrow/dataset.py:454\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[0;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(source, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m--> 454\u001b[0m     fs, paths_or_selector \u001b[39m=\u001b[39m _ensure_multiple_sources(source, filesystem)\n\u001b[1;32m    455\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyarrow/dataset.py:373\u001b[0m, in \u001b[0;36m_ensure_multiple_sources\u001b[0;34m(paths, filesystem)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39melif\u001b[39;00m file_type \u001b[39m==\u001b[39m FileType\u001b[39m.\u001b[39mNotFound:\n\u001b[0;32m--> 373\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(info\u001b[39m.\u001b[39mpath)\n\u001b[1;32m    374\u001b[0m \u001b[39melif\u001b[39;00m file_type \u001b[39m==\u001b[39m FileType\u001b[39m.\u001b[39mDirectory:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: /workspaces/trading_bot/data/latest",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/workspaces/trading_bot/src/benzing_news_parser.ipynb Cell 79\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f626565722f446f63756d656e74732f47697468756250726f6a656b74652f74726164696e675f626f74222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f686f6d652f626565722f446f63756d656e74732f47697468756250726f6a656b74652f74726164696e675f626f742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f686f6d652f626565722f446f63756d656e74732f47697468756250726f6a656b74652f74726164696e675f626f742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f686f6d652f626565722f446f63756d656e74732f47697468756250726f6a656b74652f74726164696e675f626f742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/trading_bot/src/benzing_news_parser.ipynb#Y141sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m ddf \u001b[39m=\u001b[39m dd\u001b[39m.\u001b[39;49mread_parquet(\u001b[39m'\u001b[39;49m\u001b[39mdata/latest/\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/dask/backends.py:138\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    137\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mtype\u001b[39m(e)(\n\u001b[1;32m    139\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling the \u001b[39m\u001b[39m{\u001b[39;00mfuncname(func)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmethod registered to the \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackend\u001b[39m}\u001b[39;00m\u001b[39m backend.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOriginal Message: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: /workspaces/trading_bot/data/latest"
          ]
        }
      ],
      "source": [
        "ddf = dd.read_parquet(cwd+'/data/latest/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeFzTaK77g0j"
      },
      "outputs": [],
      "source": [
        "# ddf[\"time\"] = ddf[\"time\"].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def f(df):\n",
        "    df[\"time\"] = df[\"time\"].astype(str)\n",
        "    return df\n",
        "# futures = client.submit(f, ddf)\n",
        "# ddf = ddf.map_partitions(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# results = client.gather(futures)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Weq1TOz3Yuvp"
      },
      "outputs": [],
      "source": [
        "mask = ddf.apply(lambda x: bool(re.search(x[\"short_name\"], x[\"body\"].replace(\"\\n\", \" \"), re.IGNORECASE)) or \\\n",
        "                 bool(re.search(x[\"short_name\"], x.title, re.IGNORECASE)),\n",
        "                 axis=1,\n",
        "                 meta=pd.Series(dtype=bool))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBv0rbEKYULN"
      },
      "outputs": [],
      "source": [
        "# Around 11k stocks before `filtering`\n",
        "# len(ddf.stocks.unique())\n",
        "\n",
        "# Around 10k stocks after `filtering`\n",
        "# len(ddf[mask].stocks.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-6OLBEJQYg_",
        "outputId": "1ff89d15-0394-48d1-cca5-ef3d72cf8b52"
      },
      "outputs": [],
      "source": [
        "ddf[mask].shape[0].compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtK_9YL-ORgB",
        "outputId": "27e6c0b8-bd03-4979-97af-ea40f9cde5ec"
      },
      "outputs": [],
      "source": [
        "# Case 1: Firmenname wird in abgekürzter Version benutzt, die wir nicht methodisch rekonstruieren können\n",
        "#   Z.B.: IBM -> International Business Machines,\n",
        "#   oder UPS -> United Parcel Service\n",
        "#   oder GE -> General Electric\n",
        "# Case 2: Ticker wurde recycled und zeigt inzwischen auf eine andere Firma.\n",
        "# Will hierzu nicht auch noch Daten kaufen, deswegen werden hier leider einige Nachrichten verworfen werden.\n",
        "\n",
        "ddf[~mask].shape[0].compute() # ~120k Nachrichten mit `FALSCHEM` Firmennamen -> Textinhalt ableichen mit Firmennamen des Kursdaten-Datensatzes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_17NxbxjXbZo"
      },
      "outputs": [],
      "source": [
        "# ddf[~mask].stocks.value_counts().compute().head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNAFvvVmmWm2"
      },
      "outputs": [],
      "source": [
        "# Filter for high conviction entries\n",
        "ddf = ddf[mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIKojMpEmyas"
      },
      "outputs": [],
      "source": [
        "ddf = ddf.repartition(npartitions=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ddf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xw1JSiI_0JH"
      },
      "source": [
        "### Duplikate Entfernen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHfiHqVTjZJ-"
      },
      "outputs": [],
      "source": [
        "# import cudf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8Yn_8iXjYl4"
      },
      "outputs": [],
      "source": [
        "# ddf = ddf.map_partitions(cudf.from_pandas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEzrJqEN_yuJ"
      },
      "outputs": [],
      "source": [
        "res = ddf.map_partitions(lambda x: x.drop_duplicates())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MM9AoaeT_48w"
      },
      "outputs": [],
      "source": [
        "# res.shape[0].compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5o-p2FkSACbs"
      },
      "outputs": [],
      "source": [
        "ddf = res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Cs74YSdZ9qEr",
        "outputId": "e5fe014e-73df-4157-ef7e-5ed837f9ba23"
      },
      "outputs": [],
      "source": [
        "ddf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvhpDVtFh6M6"
      },
      "outputs": [],
      "source": [
        "name_function = lambda x: f\"data-{x}.parquet\"\n",
        "ddf.to_parquet(cwd+'/data/latest2/', name_function=name_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HxMOTfgjpvs"
      },
      "source": [
        "#### Convert ddf to pd.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPF987k3H8tC"
      },
      "outputs": [],
      "source": [
        "#ddf = ddf.compute()\n",
        "ddf = ddf.sort_values(\"time\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SurJhZXHx0U"
      },
      "source": [
        "### Timedeltas zwischen Nachrichtenmeldungen\n",
        "\n",
        "\n",
        "\n",
        "Wir sehen, dass einige Nachrichten dupliziert vorkommen, d.h. mit einem Timedelta von 0 und mit derselben Überschrift etc. diese gilt es zu eliminieren."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDJqz8MW5_xW"
      },
      "outputs": [],
      "source": [
        "tmp = ddf[[\"time\", \"stocks\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "rZouQiwXHy0r",
        "outputId": "9b7f2880-cc0c-4213-d001-4301fa2451fa"
      },
      "outputs": [],
      "source": [
        "#### Adding timedeltas to the data frame\n",
        "news_timedeltas = tmp.groupby(\"stocks\").transform(lambda x: x.diff())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrcTtcDj-UjE",
        "outputId": "40234ebb-e422-4975-9960-e8387ac0b38b"
      },
      "outputs": [],
      "source": [
        "# ~3 minutes evaluates to true\n",
        "# (news_timedeltas.index == ddf.index).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P77WnvamNhf-",
        "outputId": "47792774-bb9d-446b-ae11-8a9faab88f29"
      },
      "outputs": [],
      "source": [
        "ddf.loc[:, \"timedelta\"] = news_timedeltas.time.fillna(pd.Timedelta(days=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ij4ENbFQw7b"
      },
      "outputs": [],
      "source": [
        "news_timedeltas = ddf.timedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbnz32U8RFCj",
        "outputId": "d123e27d-36c4-4777-b5a9-d6fb4356e38e"
      },
      "outputs": [],
      "source": [
        "news_timedeltas.iloc[0].components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcSEhkw6JLHO"
      },
      "outputs": [],
      "source": [
        "same_day_timedeltas = news_timedeltas.apply(lambda x: x.components.days == 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2wKSEQ4JT1E",
        "outputId": "1b80ea3e-072c-4c0f-a068-e52bb6dbcda7"
      },
      "outputs": [],
      "source": [
        "(same_day_timedeltas == 0).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEOr6c8qsI7G"
      },
      "outputs": [],
      "source": [
        "same_hour_timedeltas = news_timedeltas.apply(lambda x: (x.components.days == 0) & \\\n",
        "                                               (x.components.hours == 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0wfw1X8sMxy",
        "outputId": "37ed353e-674e-462f-d1f7-73331e5d037a"
      },
      "outputs": [],
      "source": [
        "print(same_hour_timedeltas.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-KxzooAoIrp"
      },
      "outputs": [],
      "source": [
        "same_minute_timedeltas = news_timedeltas.apply(lambda x: (x.components.days == 0) & \\\n",
        "                                               (x.components.hours == 0) & \\\n",
        "                                               (x.components.minutes == 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-eIKdg7ogxA",
        "outputId": "1cdc2aec-8102-4f7a-b373-8e48be498941"
      },
      "outputs": [],
      "source": [
        "print(same_minute_timedeltas.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cECO9zSxX6G"
      },
      "outputs": [],
      "source": [
        "same_day_ddf = ddf.loc[same_day_timedeltas]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcTg5C9lUyQk",
        "outputId": "5aa9db7e-ac62-47f3-adc3-512e5f16234a"
      },
      "outputs": [],
      "source": [
        "ddf.stocks.value_counts().describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bshj1secWQ3P"
      },
      "source": [
        "Bis zu 5k Nachrichten pro Firma, z.B. AT&T, was in 13 Jahren ca. einer Nachricht pro Tag entspricht. Wir wollen nicht das eine Firma mit vielen Junk-Nachrichten das Modell dominiert.\n",
        "\n",
        "Kategorisieren von Nachrichten (mit Text2Topic, wie Salbrechter?) und eliminieren von Business/Strategic etc.\n",
        "Im Falle von Text2Topic, versuche Estimates des Unternehmens von Dritten zu unterscheiden.\n",
        "\n",
        "Wichtig!!! Unterscheide zwischen LERN-Phase und PRODUKTIONS-Phase.\n",
        "Wir können z.B. CLS-Token in der Produktions-Phase vergleichen, in der Lern-Phase aber noch nicht.\n",
        "\n",
        "Text2Vec -> Business category evtl. entfernen-> Intrastock variance average"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRKgjNUn7Pb3"
      },
      "source": [
        "## Topic Modeling (Nach Aschluss Auslagern in Python file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scaling up cluster and installing required packages\n",
        "cluster.scale(5)\n",
        "client.wait_for_workers(5)\n",
        "client.run(worker_setup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ddf = dd.read_parquet(\"gcs://extreme-lore-398917-bzg/latest2/\",\n",
        "                      storage_options={'token': token})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-Vkpo3vnhlK"
      },
      "outputs": [],
      "source": [
        "# Remove numbers\n",
        "docs = ddf.body.map(lambda x: re.sub(r'\\d+', '', x), meta=pd.Series(dtype=\"string\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs.name=\"body\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZyQX5zE2HZZ"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.topic_modeling.helpers import tokenize, keyword_filter\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ufa0owo6be4L",
        "outputId": "050d7474-07e9-4262-af9f-b0bfe9d0a6f0"
      },
      "outputs": [],
      "source": [
        "keywords_list1 = [\"launch\", \"business\", \"strategy\", \"management\", \"product\", \"service\", \"app\", \"customer\", \"merge\"]\n",
        "keywords_list2 = [\"upgrade\", \"downgrade\", \"raise\", \"cut\", \"buy\", \"sell\", \"hold\", \"outperform\", \"underperform\", \"analyst\", \"estimate\"]\n",
        "keywords_list3 = [\"ebit\", \"eps\", \"earnings\", \"report\", \"financial\", \"quarter\", \"annual\", \"year\", \"ended\", \"net\", \"income\"]\n",
        "total_keywords = keywords_list1 + keywords_list2 + keywords_list3\n",
        "len(total_keywords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdsbibbHEXa2"
      },
      "outputs": [],
      "source": [
        "# Tokenization + rough filtering\n",
        "tfidf_docs = docs.map(lambda x: keyword_filter(tokenize(x), total_keywords), meta=pd.Series(dtype=\"object\"))\n",
        "tfidf_docs = tfidf_docs.reset_index().repartition(npartitions=5)\n",
        "tfidf_docs.columns = [\"index\", \"body\"]\n",
        "tfidf_docs.to_parquet(\"gcs://extreme-lore-398917-bzg/tfidf-tokens\",\n",
        "                                                    storage_options={'token': token})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_docs = dd.read_parquet(\"gcs://extreme-lore-398917-bzg/tfidf-tokens/\",\n",
        "                                                    storage_options={'token': token},\n",
        "                                                    dtype_backend=\"pyarrow\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_docs = tfidf_docs.compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Numpy representation is without commas making us unable to convert strings to list via eval\n",
        "tfidf_docs.loc[:, \"body\"] = tfidf_docs.body.map(lambda x: x.replace(\" \", \", \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def f(tokens):\n",
        "    if len(tokens) == 0:\n",
        "        return np.array([\"wordtopreventemptydocumentswhencalculatingtfidf\"])\n",
        "    else:\n",
        "        return np.array(tokens)\n",
        "        \n",
        "tfidf_docs.loc[:, \"body\"] = tfidf_docs.body.map(lambda x: f(eval(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_docs.iloc[0].body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_tfidfs_list = []\n",
        "for topic_tokenizer in [keywords1, keywords2, keywords3]:\n",
        "    lazy_result = dask.delayed(TfidfVectorizer(tokenizer=topic_tokenizer, lowercase=False).fit_transform)(tfidf_docs.body)\n",
        "    lazy_sums = dask.delayed(np.apply_along_axis)(np.sum, 1, lazy_result.todense())\n",
        "    topic_tfidfs_list.append(lazy_sums)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_indicator_list = dask.compute(*topic_tfidfs_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame(topic_indicator_list)\n",
        "df=df.transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_docs.head().iloc[4].body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__BEnYJj1-1Z"
      },
      "source": [
        "### LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsdTIt0xOZ0k"
      },
      "outputs": [],
      "source": [
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import Phrases\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhGqa-Kbk0T_"
      },
      "outputs": [],
      "source": [
        "# Split the documents into tokens.\n",
        "lda_docs = docs.apply(lambda x: word_tokenize(str.lower(x)), meta=pd.Series(dtype=\"object\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyuoeiVWiKBp"
      },
      "outputs": [],
      "source": [
        "# Remove words that are only one character.\n",
        "# Lemmatize the documents.\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lda_docs = lda_docs.apply(lambda x: [lemmatizer.lemmatize(token) for token in x if len(token) >1], meta=pd.Series(dtype=\"object\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwqu5jn_B12x",
        "outputId": "b0e65f6c-9800-4466-897c-c1f5fd025a1e"
      },
      "outputs": [],
      "source": [
        "lda_docs.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7XOh3dLmi0X",
        "outputId": "9adca3c4-9b4d-463d-f222-4d58446a1418"
      },
      "outputs": [],
      "source": [
        "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
        "bigram = Phrases(docs, min_count=20)\n",
        "def f(doc):\n",
        "  doc = doc\n",
        "  for token in bigram[doc]:\n",
        "      if '_' in token:\n",
        "          # Token is a bigram, add to document.\n",
        "          doc.append(token)\n",
        "  return doc\n",
        "lda_docs = lda_docs.apply(lambda doc: f(doc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5GbbTglNiFI"
      },
      "outputs": [],
      "source": [
        "# lda_docs.name = \"body\"\n",
        "# lda_docs = lda_docs.to_frame().reset_index()\n",
        "# lda_docs = lda_docs.reset_index().repartition()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N60eZ9T-OD49"
      },
      "outputs": [],
      "source": [
        "# lda_docs.to_parquet(cwd + \"/data/lda_docs\", schema={\"body\": pa.string(),  \"index\":pa.int32()})\n",
        "# lda_docs = dd.read_parquet(cwd + \"/data/lda_docs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGD2ZOf0m4Kp"
      },
      "outputs": [],
      "source": [
        "# Remove rare and common tokens.\n",
        "\n",
        "# Create a dictionary representation of the documents.\n",
        "dictionary = Dictionary(docs)\n",
        "\n",
        "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
        "dictionary.filter_extremes(no_below=1000, no_above=0.6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlJ5ZyFknNi_",
        "outputId": "e52cdad5-cd53-4f90-dd6b-1fc5ecc3ffa2"
      },
      "outputs": [],
      "source": [
        "# Bag-of-words representation of the documents.\n",
        "corpus = [dictionary.doc2bow(doc) for doc in docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xfj1is7XnO27"
      },
      "outputs": [],
      "source": [
        "print('Number of unique tokens: %d' % len(dictionary))\n",
        "print('Number of documents: %d' % len(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJEDyIf-nTv6"
      },
      "outputs": [],
      "source": [
        "# Train LDA model.\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "# Set training parameters.\n",
        "num_topics = 10\n",
        "chunksize = 2000\n",
        "passes = 20\n",
        "iterations = 400\n",
        "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
        "\n",
        "# Make an index to word dictionary.\n",
        "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
        "id2word = dictionary.id2token\n",
        "\n",
        "model = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    chunksize=chunksize,\n",
        "    alpha='auto',\n",
        "    eta='auto',\n",
        "    iterations=iterations,\n",
        "    num_topics=num_topics,\n",
        "    passes=passes,\n",
        "    eval_every=eval_every\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74ZPDdqSnVoN"
      },
      "outputs": [],
      "source": [
        "top_topics = model.top_topics(corpus)\n",
        "\n",
        "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
        "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
        "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
        "\n",
        "from pprint import pprint\n",
        "pprint(top_topics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRABrPVOcaa9"
      },
      "source": [
        "## Nachrichten-Parsing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dask.config.set({'distributed.scheduler.worker-ttl': \"60 minutes\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wait an appropriate amount of time to give autoscaler opportunity to create nodes.\n",
        "# Even CommClosedError occurrs... just wait\n",
        "n = 2\n",
        "# cluster.scale(n, worker_group=\"highmem\")\n",
        "client.wait_for_workers(n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = 1\n",
        "# cluster.scale(0, worker_group=\"highmem-single\")\n",
        "client.wait_for_workers(n) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "ddf = dd.read_parquet(\"gcs://extreme-lore-398917-bzg/latest2/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "ticker_name_mapper_reduced = ddf[[\"stocks\", \"company_name\", \"short_name\"]].drop_duplicates(keep=\"first\").compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "ticker_name_mapper_reduced.to_parquet(\"data_shared/ticker_name_mapper_reduced.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Beispiel/ Untersuchung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keF_8QNAkM36"
      },
      "outputs": [],
      "source": [
        "sample_partition = ddf.get_partition(20)\n",
        "y = sample_partition.head(5)\n",
        "y.loc[:, \"time\"] = pd.to_datetime(y.time).dt.tz_convert(\"UTC\")\n",
        "x = y.iloc[2]\n",
        "x.body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSAGuM_Acrwr"
      },
      "outputs": [],
      "source": [
        "res = client.submit(filter_body, row=x, logging=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res.result()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj6mofzeqcqB"
      },
      "source": [
        "### Anwenden der filter_body-Funktion auf alle Reihen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def handle_timezone(x):\n",
        "    try:\n",
        "        return x.tz_convert(\"US/Eastern\")\n",
        "    except Exception as e:\n",
        "        return x.tz_localize(\"US/Eastern\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Still need to parse time correctly...\n",
        "ddf[\"time\"] = ddf[\"time\"].map(lambda x: handle_timezone(pd.to_datetime(x)), meta=pd.Series(dtype=\"datetime64[ns, US/Eastern]\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGlOJPEvFxuP"
      },
      "outputs": [],
      "source": [
        "par = ddf  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gPPJDRKPv72"
      },
      "outputs": [],
      "source": [
        "par[\"parsed_body\"] = par.map_partitions(lambda y: y.apply(lambda x: filter_body(x),\n",
        "                                                axis=1),\n",
        "                                                meta=pd.Series(dtype=\"string\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hier der error letztes Mal\n",
        "# from src.cloud_manager import VMManager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "par = client.persist(par) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client.nthreads()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time \n",
        "while(True):\n",
        "    time.sleep(10)\n",
        "    print(client.get_scheduler_logs())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dask.distributed import wait, progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# progress(par)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wait(par)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vlCN3tKDfnw8"
      },
      "outputs": [],
      "source": [
        "# Do this only after having persisted, because this blocks the console making us unable to interact witth client/cluster\n",
        "# and can interfere with communication between client and scheduler.\n",
        "\n",
        "# VMManager needs to be redone... do os.system(kubectl delete dsk example), its a cleaner shutdown and also deletes nodes\n",
        "# with VMManager(cluster) as _:\n",
        "par.to_parquet(\"gcs://extreme-lore-398917-bzg/processed_news\",\n",
        "        storage_options={'token': token})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uPo1Ruo0N0q"
      },
      "outputs": [],
      "source": [
        "client # no connection, can we connect via ip??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster # connection still here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaK5pLz4mRHF"
      },
      "source": [
        "## Analyse der durschnittlichen Tokenlänge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y192tnIfpIXj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0bW6kYmpI1W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VG5rnSiNpWVN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMBWNMfIpebc"
      },
      "source": [
        "## Generating timestamp-stock pairs in order to know for which days we need stock data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mc6lpRCpfVT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
