{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkovyr-oc4-K"
      },
      "source": [
        "# Benzinga-Nachrichten-Verarbeitung\n",
        "\n",
        "Don't forget to shut down cluster at the end of the day!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A8GOctQmMfj"
      },
      "source": [
        "## Setup up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cluster\n",
        "**Don't** import anything before this section, otherwise cluster setup will/might fail!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing Instance: dask-e479e488-worker-9f687daa\n",
            "Closing Instance: dask-e479e488-scheduler\n"
          ]
        }
      ],
      "source": [
        "cluster.close()\n",
        "client.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google Cloud Authentification\n",
        "import os\n",
        "# this is how the OAuth2 token is available on your cluster\n",
        "token = os.environ.get(\"CLOUDSDK_AUTH_ACCESS_TOKEN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/workspaces/trading_bot/extreme-lore-398917-ac46de419eb2.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"DASK_CLOUDPROVIDER__GCP__PROJECTID\"] = \"extreme-lore-398917\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Launching cluster with the following configuration: \n",
            "  Source Image: projects/ubuntu-os-cloud/global/images/ubuntu-minimal-1804-bionic-v20201014 \n",
            "  Docker Image: hermelinkluntjes/thesis:test \n",
            "  Machine Type: c2d-standard-2 \n",
            "  Filesystem Size: 50 \n",
            "  Disk Type: pd-standard \n",
            "  N-GPU Type:  \n",
            "  Zone: us-east1-c \n",
            "Creating scheduler instance\n",
            "dask-e479e488-scheduler\n",
            "\tInternal IP: 10.142.0.20\n",
            "\tExternal IP: 34.148.87.124\n",
            "Waiting for scheduler to run at 34.148.87.124:8786\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-10-31 11:00:42,091 - distributed.deploy.cluster - WARNING - Failed to sync cluster info multiple times - perhaps there's a connection issue? Error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 490, in connect\n",
            "    stream = await self.client.connect(\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/tornado/tcpclient.py\", line 279, in connect\n",
            "    af, addr, stream = await connector.start(connect_timeout=timeout)\n",
            "asyncio.exceptions.CancelledError\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/conda/lib/python3.10/asyncio/tasks.py\", line 456, in wait_for\n",
            "    return fut.result()\n",
            "asyncio.exceptions.CancelledError\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
            "    comm = await wait_for(\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/distributed/utils.py\", line 1910, in wait_for\n",
            "    return await asyncio.wait_for(fut, timeout)\n",
            "  File \"/opt/conda/lib/python3.10/asyncio/tasks.py\", line 458, in wait_for\n",
            "    raise exceptions.TimeoutError() from exc\n",
            "asyncio.exceptions.TimeoutError\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/distributed/deploy/cluster.py\", line 165, in _sync_cluster_info\n",
            "    await self.scheduler_comm.set_metadata(\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/distributed/core.py\", line 1313, in send_recv_from_rpc\n",
            "    comm = await self.live_comm()\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/distributed/core.py\", line 1272, in live_comm\n",
            "    comm = await connect(\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/distributed/comm/core.py\", line 368, in connect\n",
            "    raise OSError(\n",
            "OSError: Timed out trying to connect to tls://34.23.170.135:8786 after 30 s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scheduler is running\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/contextlib.py:142: UserWarning: Creating your cluster is taking a surprisingly long time. This is likely due to pending resources. Hang tight! \n",
            "  next(self.gen)\n"
          ]
        }
      ],
      "source": [
        "from dask.distributed import Client\n",
        "from dask_cloudprovider.gcp import GCPCluster\n",
        "n_workers = 1\n",
        "cluster = GCPCluster(n_workers=n_workers,   \n",
        "                     machine_type=\"c2d-standard-2\",\n",
        "                     docker_image=\"hermelinkluntjes/thesis:test\")\n",
        "# Setup container\n",
        "client = Client(cluster)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'client' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/workspaces/trading_bot/src/benzing_news_parser.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22673a5c5c4d65696e652041626c6167655c5c4e65777354726164696e675c5c74726164696e675f626f74222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22673a5c5c4d65696e652041626c6167655c5c4e65777354726164696e675c5c74726164696e675f626f745c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f672533412f4d65696e6525323041626c6167652f4e65777354726164696e672f74726164696e675f626f742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f673a2f4d65696e652041626c6167652f4e65777354726164696e672f74726164696e675f626f742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/trading_bot/src/benzing_news_parser.ipynb#Y350sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m client\u001b[39m.\u001b[39mwait_for_workers(n_workers)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
          ]
        }
      ],
      "source": [
        "client.wait_for_workers(n_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io, tarfile\n",
        "import os\n",
        "def fn_to_targz_string(fn):\n",
        "    with io.BytesIO() as bt:\n",
        "        with tarfile.open(fileobj=bt,mode='w:gz') as tf:\n",
        "            tf.add(fn,arcname=os.path.basename(fn))\n",
        "        bt.seek(0)\n",
        "        s=bt.read()\n",
        "    return s\n",
        "\n",
        "def extract_targz_string(s,*args,**kwargs):\n",
        "    with io.BytesIO() as bt:\n",
        "        bt.write(s)\n",
        "        bt.seek(0)\n",
        "        with tarfile.open(fileobj=bt,mode='r:gz') as tf:\n",
        "            tf.extractall(*args,**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "targz = fn_to_targz_string(\"src\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "client.run_on_scheduler(extract_targz_string, targz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'tls://10.142.0.22:32947': {'status': 'OK'}}"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# scheduler-setup.py\n",
        "from distributed.diagnostics.plugin import WorkerPlugin\n",
        "\n",
        "class SourceDirectoryCopier(WorkerPlugin):\n",
        "    def __init__(self, targz):\n",
        "      self.targz = targz\n",
        "      super().__init__()\n",
        "\n",
        "    def setup(self, scheduler=None, worker=None, **kwargs):\n",
        "        extract_targz_string(self.targz)\n",
        "        print(\"Added a new worker at:\", worker)\n",
        "\n",
        "plugin = SourceDirectoryCopier(targz)\n",
        "client.register_plugin(plugin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "def myfunc():\n",
        "    from src.preprocessing.news_parser import filter_body\n",
        "    return filter_body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'tls://10.142.0.23:46333': <function src.preprocessing.news_parser.filter_body(row: pandas.core.series.Series, logging=False) -> str>}"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run(myfunc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function src.preprocessing.news_parser.filter_body(row: pandas.core.series.Series, logging=False) -> str>"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_scheduler(myfunc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function src.preprocessing.news_parser.filter_body(row: pandas.core.series.Series, logging=False) -> str>"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = client.submit(myfunc)\n",
        "x.result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating worker instance\n",
            "dask-e479e488-worker-9f687daa\n",
            "\tInternal IP: 10.142.0.23\n",
            "\tExternal IP: 34.23.170.135\n"
          ]
        }
      ],
      "source": [
        "cluster.scale(1)\n",
        "client.wait_for_workers(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cluster.scale(3)\n",
        "# client.wait_for_workers(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CymECzDhh6Mn"
      },
      "source": [
        "### System settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4lfXyeQvfCn"
      },
      "outputs": [],
      "source": [
        "using_colab = False\n",
        "using_laptop = True\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmP1mPmHvcqA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "if using_colab:\n",
        "  from google.colab import drive\n",
        "    drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCMMsTZyC0BE"
      },
      "source": [
        "### CUDF für hardware acceleration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqFqwisUh6Mn"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "# !python rapidsai-csp-utils/colab/pip-install.py\n",
        "# import cudf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7IaohEqCoiM"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlMR8ADYv3Iy"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "if using_colab:\n",
        "  !sudo apt update\n",
        "  !sudo apt install maven;\n",
        "\n",
        "  !pip install pyarrow==11.0.0\n",
        "  !pip install html2text\n",
        "  !pip install datefinder\n",
        "  !pip install -U dask[complete]\n",
        "  !pip install nltk\n",
        "  !pip install dateparser\n",
        "  !pip install pyngrok\n",
        "  !pip install sutime\n",
        "  !pip install pyngrok\n",
        "\n",
        "  # This is required for sutime\n",
        "  !mvn dependency:copy-dependencies -DoutputDirectory=./jars -f $(python3 -c 'import importlib; import pathlib; print(pathlib.Path(importlib.util.find_spec(\"sutime\").origin).parent / \"pom.xml\")');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChZHYvDl57qU",
        "outputId": "81453ac2-ae67-4bb6-b09a-70f7a1f425d4"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import dask.dataframe as dd\n",
        "import dask\n",
        "import pandas as pd\n",
        "from src.preprocessing.news_parser import get_company_abbreviation, yahoo_get_wrapper, \\\n",
        "                                          infer_author, filter_body\n",
        "import nltk\n",
        "\n",
        "# Mainly for topic modeling\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import gcsfs\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVUeXGml6-Si"
      },
      "source": [
        "### Setup NGROK tunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiY7l2l9685M",
        "outputId": "c8ad5229-cafa-48b1-c33e-20de5178dd3f"
      },
      "outputs": [],
      "source": [
        "# ------- Only required when working on google colab, not required when working with google cloud engine -----------------\n",
        "if using_colab:\n",
        "  from pyngrok import ngrok, conf\n",
        "  conf.get_default().auth_token = \"2WntwErWDt9LxQ2Jfp6C8OxDAMK_7iZVdC1utyZET1PE8cuUg\"\n",
        "\n",
        "  public_url = ngrok.connect(8787).public_url\n",
        "  print(\" * ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, 8787))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U69xWIixZSM",
        "outputId": "79053918-c60b-40f6-c77b-06069a9f0f7e"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "if 'client' not in globals():\n",
        "  !python -m pip install jupyter-server-proxy\n",
        "  client = Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRqa0EPy_lpx"
      },
      "source": [
        "## Grobes HTML-Parsing\n",
        "Als erstes müssen wir die HTML-Dokumente zu normalem Text umwandeln, ansonsten sind die Text-Zellen zu groß und führen zu Problemen mit PyArrow/Dask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neM6jNH6NAnM",
        "outputId": "ef8b18b3-9cdb-4cb1-8d8e-ed4d38fb97ef"
      },
      "outputs": [],
      "source": [
        "dask.config.set(scheduler=\"threads\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4rQifxX6ygR"
      },
      "outputs": [],
      "source": [
        "input_dir = \"data/raw_bzg/\"\n",
        "output_dir = 'data/unraw1_bzg/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wy5-WZ4B_A2"
      },
      "outputs": [],
      "source": [
        "# for year in range(2019, 2020):\n",
        "#     print(year)\n",
        "#     df = pd.read_parquet(f\"{input_dir}story_df_raw_{year}.parquet\")\n",
        "#     df = dd.from_pandas(df, npartitions=12)\n",
        "#     df[\"html_body\"] = df[\"html_body\"].apply(body_formatter, meta=pd.Series(dtype=\"str\"))\n",
        "#     df = df.rename(columns={\"html_body\":\"body\"})\n",
        "#     name_function = lambda x: f\"data-{year}-{x}.parquet\"\n",
        "#     df.to_parquet(output_dir, name_function=name_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa6QpqfR_t97"
      },
      "source": [
        "## Neu-Partitionierug\n",
        "Sodass alle Partitionen etwa die gleiche Größe haben."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHFpynyiq3O4"
      },
      "outputs": [],
      "source": [
        "input_dir = 'data/unraw1_bzg/'\n",
        "output_dir = 'data/unraw2_bzg/'\n",
        "\n",
        "# ddf = dd.read_parquet(input_dir+\"*.parquet\")\n",
        "# ddf2 = ddf.repartition(npartitions=50)\n",
        "# name_function = lambda x: f\"data-{x}.parquet\"\n",
        "# ddf2.to_parquet(output_dir, name_function=name_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFnIFQW2Nag7"
      },
      "source": [
        "## Author-Inferenz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__EYo7CaFlVj"
      },
      "source": [
        "Ein bisschen die Daten säubern..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-jzA2AbFptW"
      },
      "outputs": [],
      "source": [
        "input_dir = cwd+'/data/unraw2_bzg/'\n",
        "output_dir = cwd+'/data/unraw3_bzg/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef28p8AOnrFD"
      },
      "outputs": [],
      "source": [
        "ddf = dd.read_parquet(input_dir+\"*.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylwarwpXLegE"
      },
      "outputs": [],
      "source": [
        "# Remove rows for which noo stock ticker is recorded\n",
        "ddf = ddf[ddf.stocks != '']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7OPUatcMS7C"
      },
      "outputs": [],
      "source": [
        "# Convert `channels`  datatype from string to list\n",
        "ddf[\"channels\"] = ddf[\"channels\"].apply(eval, meta=pd.Series(dtype='object'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu1EhTqrVpBG"
      },
      "source": [
        "Untersuche als nächstes die Behauptung, dass **PRNewswire** und **Businesswire** den gesamten Markt für Pressemeldungen in den USA kontrollieren. Wenn dem so ist, und sie nicht noch weitere, unwichtige Meldungen veröffentlichen, dann können wir einfach die Newsartikel nach diesen Autoren filtern und uns viel Arbeit ersparen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5mO091MfO6e"
      },
      "outputs": [],
      "source": [
        "dask.config.set(scheduler=\"processes\")\n",
        "ddf[\"inferred_author\"] = None\n",
        "ddf[\"inferred_author\"] = ddf.body.apply(infer_author, meta=pd.Series(dtype=\"string\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTcPjnU0VZZu"
      },
      "outputs": [],
      "source": [
        "# value_counts for authors\n",
        "auhtor_value_counts = pd.concat([ddf.author.value_counts().head(10), ddf.inferred_author.value_counts().head(10)], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "UNesN9jX1IP0",
        "outputId": "1b8d2697-13fd-4d33-cc0f-1f7ce3cb4ee8"
      },
      "outputs": [],
      "source": [
        "auhtor_value_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKT6tusBp13y",
        "outputId": "07dcdac3-737a-48e0-a3dd-bf7a1f83d028"
      },
      "outputs": [],
      "source": [
        "auhtor_value_counts.sum().diff()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk6p6woeqJqP"
      },
      "source": [
        "Ungefähr 650k Nachrichten werden ausgelassen, wenn nur die vier Hauptvertreiber von Pressemeldungen berücksichtigt werden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X8rHo-CsDCQ"
      },
      "outputs": [],
      "source": [
        "ddf = ddf[~ddf.inferred_author.isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHvzym8sFcX_"
      },
      "outputs": [],
      "source": [
        "ddf[\"inferred_author\"] = ddf[\"inferred_author\"].astype(\"string\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufk8_XUjFwdk"
      },
      "outputs": [],
      "source": [
        "ddf[\"channels\"] = ddf.channels.apply(lambda x: str(x), meta=pd.Series(dtype=\"string\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsEmViTzNvgs",
        "outputId": "2ed92bc7-5f41-4e52-9614-b8e6dddacd84"
      },
      "outputs": [],
      "source": [
        "ddf.inferred_author.value_counts().compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfNNyD_31sly",
        "outputId": "1647ed45-d50f-48c2-ddc5-198135f50d34"
      },
      "outputs": [],
      "source": [
        "ddf.inferred_author.value_counts().sum().compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v04Wy5Y4iudn"
      },
      "outputs": [],
      "source": [
        "name_function = lambda x: f\"data-{x}.parquet\"\n",
        "ddf.to_parquet(output_dir, name_function=name_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY5ZI0hQitRh"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLdnwdyxNFiH"
      },
      "outputs": [],
      "source": [
        "# Contains 100k rows\n",
        "earnings_ddf = ddf[ddf.channels.apply(lambda x: \"Earnings\" in x, meta=pd.Series(dtype=bool))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-zs5SSiVbyp",
        "outputId": "2569a9dd-dceb-433e-fefe-44ee9abaec20"
      },
      "outputs": [],
      "source": [
        "# value counts for authors of earnings reports (contrast to value counts of all news articles)\n",
        "earnings_ddf.inferred_author.value_counts().head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dVD6j-G9nie"
      },
      "source": [
        "Hier sehen wir, dass es keine einzige Pressemeldung von **Business Wire** gibt, die mit *Earnings* gekennzeichnet sind. Trotzdem gibt es relevante *Earnings* reports von Business Wire. Dies habe ich kurz verifiziert..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jafqZ87tXDKn"
      },
      "source": [
        "## Vorgehen\n",
        "\n",
        "Benötigt:\n",
        "---------\n",
        "- data/tickers.pkl (alt) \\\\\n",
        "- data_shared/corporation_endings.txt \\\\\n",
        "- input_dir = data/unraw3_bzg/ \\\\\n",
        "- output_dir = data/unraw4_bzg/\n",
        "\n",
        "Produziert:\n",
        "-----------\n",
        "- data_shared/all_ticker_name_mapper.parquet\n",
        "\n",
        "--------------\n",
        "\n",
        "Wie viele Nachrichten bleiben, wenn wir auf relevante Ticker filtern? Wir wollen nicht(!) - so ist es momentan - auf die momentane Russell 3k-Zusammensetzung filtern, denn wir wollen auch ungelistete bzw. ehemalige Russell-Aktien beachten.\n",
        "\n",
        "\n",
        "**1. Full-Name-Discovery:**\n",
        "\n",
        "Herausfinden des vollen Namens des Unternehmens für jeden Ticker, damit 1. der Text richtig geparst werden kann und 2. damit wir einen Anhaltspunkt für das Ticker-Grouping haben.\n",
        "\n",
        "\n",
        "**2. Ticker-Filtering:**\n",
        "\n",
        "Alle Ticker herausfiltern, die wir nicht brauchen. Wenn wir aber ein großes Aktienuniversum (mit inzwischen ungelisteten Aktien) benutzen, werden wir fast alle Nachrichten behalten können. Allerdings lassen sich so Fehlerhafte Nachrichten/Ticker etc. herausfiltern.\n",
        "\n",
        "\n",
        "**3. Ticker-Grouping:**\n",
        "\n",
        "Was machen wir, wenn wir mehrerer Aktiengattungen für ein Unternehmen haben? Z.B. Vorzugs- und Stammaktien. Wir können i.A. die Stammaktie nehmen, da diese normalerweise ein höheres Handelsvolumen aufweist. D.h. wir bilden alle Ticker der Benzinga-Nachrichten auf den Ticker der Stammaktie ab.\n",
        "\n",
        "\n",
        "**4. Firmennamen-Nachrichtenkörper-Verifikation:**\n",
        "\n",
        "Da Ticker wiederverwendet werden können bzw. sich verändern können wollen wir sicherstellen, dass der Unternehmensname im Nachrichtenkörper vorkommt! Bzw. generell ist das eine gute Datensäuberungs-Maßnahme."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaSble_1ku1t"
      },
      "outputs": [],
      "source": [
        "input_dir = cwd+'/data/unraw3_bzg/'\n",
        "output_dir = cwd+'/data/unraw4_bzg/'\n",
        "ddf = dd.read_parquet(input_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JosrB0RfsUsy"
      },
      "outputs": [],
      "source": [
        "all_tickers = ddf.stocks.unique().compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdwKGnsPnkYS"
      },
      "source": [
        "### Full-Name-Discovery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8urd8wvsXza"
      },
      "outputs": [],
      "source": [
        "company_names = all_tickers.apply(lambda x: yahoo_get_wrapper(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUslIwYdsmU7"
      },
      "outputs": [],
      "source": [
        "all_mapper = pd.concat([all_tickers, company_names], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tW8AGmgfK_oV"
      },
      "outputs": [],
      "source": [
        "all_mapper.columns = [\"ticker\", \"company_name\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qa3sz1yBMkjw"
      },
      "outputs": [],
      "source": [
        "all_mapper = all_mapper.dropna() # This drops like half of the tickers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "_O1hNOdoNUIs",
        "outputId": "baf61a9c-1701-4fe0-8130-2542364b5df1"
      },
      "outputs": [],
      "source": [
        "all_mapper[all_mapper.company_name.apply(lambda x: \"Alphabet\" in x)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMO7H_z0Nosk",
        "outputId": "7b43b771-6a28-4f4c-b996-2198695a218b"
      },
      "outputs": [],
      "source": [
        "print(len(all_mapper))\n",
        "print(len(all_mapper.company_name.unique()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ee1RuxlPpD1"
      },
      "outputs": [],
      "source": [
        "vcs = all_mapper.company_name.value_counts()\n",
        "vcs = vcs[vcs >= 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "5FEP_5x3P9F2",
        "outputId": "35345a74-a533-4b27-d617-826b14866af6"
      },
      "outputs": [],
      "source": [
        "# Ticker-Grouping\n",
        "all_mapper[all_mapper.company_name.isin(vcs.index)].sort_values(\"company_name\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0vWtCfFSoox"
      },
      "source": [
        "Es ist nicht leicht zu sagen, welchen von den Tickern wir bevorzugen sollten. Abgleichen mit den Aktientickern des Kursdatensatzes notwendig, um zu sehen, ob überhaupt nur ein Ticker übereinstimmt. Wenn es für beide Ticker eine Kurszeitreihe gibt, dann sollten wir die nehmen, die ein höheres historisches Volumen hat. Dies ist allerdings etwas, was wir später machen und nicht jetzt. Hier wollen wir zunächst nur die Nachrichten verarbeiten, weswegen wir nur die NaN-Unternehmen rausnehmen und den Rest - *ohne Ticker-Filtering* - weiterverarbeiten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6J-M2i4WeW2"
      },
      "outputs": [],
      "source": [
        "mapper = all_mapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvJpTNzVFmLZ"
      },
      "outputs": [],
      "source": [
        "mapper.columns = [\"ticker\", \"company_names\"]\n",
        "mapper = mapper[mapper.isna().sum(axis=1) == 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdRcHXQdGQ5M"
      },
      "outputs": [],
      "source": [
        "mapper = mapper.set_index(\"ticker\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t2omABfXZRK"
      },
      "outputs": [],
      "source": [
        "company_endings = pd.read_table(\"data_shared/corporation_endings.txt\").iloc[:, 0]\n",
        "# Apply get_company_abbreviation twice in order to get rid of Enterprise, Ltd.\n",
        "# Otherwise , Ltd. remains.\n",
        "mapper[\"short_name\"] = mapper.company_names.apply(lambda x: get_company_abbreviation(x, company_endings=company_endings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3d5wM_qWhvx",
        "outputId": "9cfa82b4-47f2-4395-8005-8ab9184bd904"
      },
      "outputs": [],
      "source": [
        "print(mapper.short_name.isna().sum()) # 2037 stocks for which we don't have an ending to abbreviate\n",
        "# mapper.loc[:, \"short_name\"] = mapper.short_name.fillna(mapper.company_names)\n",
        "mapper = mapper.applymap(lambda x: x.strip(\" \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voNHNOrXSIKL"
      },
      "outputs": [],
      "source": [
        "mapper.to_parquet(cwd + \"/data_shared/ticker_name_mapper.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrLjdrfwlmRm"
      },
      "outputs": [],
      "source": [
        "mapper = pd.read_parquet(cwd + \"/data_shared/ticker_name_mapper.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70POMUC-PN-n"
      },
      "outputs": [],
      "source": [
        "filt_ddf = ddf[ddf.stocks.isin(mapper.index.to_list())]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZszkSxWPBK9",
        "outputId": "46f900da-de14-469a-c618-fe235be49e4b"
      },
      "outputs": [],
      "source": [
        "ddf.shape[0].compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLbhWydOPaaK",
        "outputId": "c5381d2d-62dd-4291-975f-e6e060bf293b"
      },
      "outputs": [],
      "source": [
        "filt_ddf.shape[0].compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRN7QEojStnr"
      },
      "source": [
        "Es verbleiben circa 1 Mio. Nachrichten, für die wir den Ticker zu einem FIrmennamen auflösen können."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuLyEUaicPl_"
      },
      "source": [
        "Diese Nachrichten können wir nun wirklich parsen, und danach ordentlich kategorisieren."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSH8cQgL5tZu"
      },
      "outputs": [],
      "source": [
        "ddf = filt_ddf\n",
        "ddf = ddf.drop(columns=[\"author\"]).rename(columns={\"inferred_author\":\"author\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "safdLC-H6GR5"
      },
      "outputs": [],
      "source": [
        "# TODO: Kann effizienter werden mit GroupBy-Operation\n",
        "ddf[\"company_name\"] = ddf.stocks.apply(lambda x: mapper.company_names.loc[x], meta=pd.Series(dtype=\"string\"))\n",
        "ddf[\"short_name\"] = ddf.stocks.apply(lambda x: mapper.short_name.loc[x], meta=pd.Series(dtype=\"string\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTzB5osyPRnf"
      },
      "outputs": [],
      "source": [
        "name_function = lambda x: f\"data-{x}.parquet\"\n",
        "ddf.to_parquet(cwd+'/data/latest/', name_function=name_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F3dOc35YN8k"
      },
      "source": [
        "### Firmennamen-Nachrichtenkörper-Verifikation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w91ht_CvPZDx"
      },
      "outputs": [],
      "source": [
        "ddf = dd.read_parquet(cwd+'/data/latest/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeFzTaK77g0j"
      },
      "outputs": [],
      "source": [
        "# ddf[\"time\"] = ddf[\"time\"].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def f(df):\n",
        "    df[\"time\"] = df[\"time\"].astype(str)\n",
        "    return df\n",
        "# futures = client.submit(f, ddf)\n",
        "# ddf = ddf.map_partitions(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# results = client.gather(futures)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Weq1TOz3Yuvp"
      },
      "outputs": [],
      "source": [
        "mask = ddf.apply(lambda x: bool(re.search(x[\"short_name\"], x[\"body\"].replace(\"\\n\", \" \"), re.IGNORECASE)) or \\\n",
        "                 bool(re.search(x[\"short_name\"], x.title, re.IGNORECASE)),\n",
        "                 axis=1,\n",
        "                 meta=pd.Series(dtype=bool))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBv0rbEKYULN"
      },
      "outputs": [],
      "source": [
        "# Around 11k stocks before `filtering`\n",
        "# len(ddf.stocks.unique())\n",
        "\n",
        "# Around 10k stocks after `filtering`\n",
        "# len(ddf[mask].stocks.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-6OLBEJQYg_",
        "outputId": "1ff89d15-0394-48d1-cca5-ef3d72cf8b52"
      },
      "outputs": [],
      "source": [
        "ddf[mask].shape[0].compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtK_9YL-ORgB",
        "outputId": "27e6c0b8-bd03-4979-97af-ea40f9cde5ec"
      },
      "outputs": [],
      "source": [
        "# Case 1: Firmenname wird in abgekürzter Version benutzt, die wir nicht methodisch rekonstruieren können\n",
        "#   Z.B.: IBM -> International Business Machines,\n",
        "#   oder UPS -> United Parcel Service\n",
        "#   oder GE -> General Electric\n",
        "# Case 2: Ticker wurde recycled und zeigt inzwischen auf eine andere Firma.\n",
        "# Will hierzu nicht auch noch Daten kaufen, deswegen werden hier leider einige Nachrichten verworfen werden.\n",
        "\n",
        "ddf[~mask].shape[0].compute() # ~120k Nachrichten mit `FALSCHEM` Firmennamen -> Textinhalt ableichen mit Firmennamen des Kursdaten-Datensatzes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_17NxbxjXbZo"
      },
      "outputs": [],
      "source": [
        "# ddf[~mask].stocks.value_counts().compute().head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNAFvvVmmWm2"
      },
      "outputs": [],
      "source": [
        "# Filter for high conviction entries\n",
        "ddf = ddf[mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIKojMpEmyas"
      },
      "outputs": [],
      "source": [
        "ddf = ddf.repartition(npartitions=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ddf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xw1JSiI_0JH"
      },
      "source": [
        "### Duplikate Entfernen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHfiHqVTjZJ-"
      },
      "outputs": [],
      "source": [
        "# import cudf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8Yn_8iXjYl4"
      },
      "outputs": [],
      "source": [
        "# ddf = ddf.map_partitions(cudf.from_pandas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEzrJqEN_yuJ"
      },
      "outputs": [],
      "source": [
        "res = ddf.map_partitions(lambda x: x.drop_duplicates())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MM9AoaeT_48w"
      },
      "outputs": [],
      "source": [
        "# res.shape[0].compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5o-p2FkSACbs"
      },
      "outputs": [],
      "source": [
        "ddf = res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Cs74YSdZ9qEr",
        "outputId": "e5fe014e-73df-4157-ef7e-5ed837f9ba23"
      },
      "outputs": [],
      "source": [
        "ddf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvhpDVtFh6M6"
      },
      "outputs": [],
      "source": [
        "name_function = lambda x: f\"data-{x}.parquet\"\n",
        "ddf.to_parquet(cwd+'/data/latest1/', name_function=name_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HxMOTfgjpvs"
      },
      "source": [
        "#### Convert ddf to pd.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPF987k3H8tC"
      },
      "outputs": [],
      "source": [
        "#ddf = ddf.compute()\n",
        "ddf = ddf.sort_values(\"time\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SurJhZXHx0U"
      },
      "source": [
        "### Timedeltas zwischen Nachrichtenmeldungen\n",
        "\n",
        "\n",
        "\n",
        "Wir sehen, dass einige Nachrichten dupliziert vorkommen, d.h. mit einem Timedelta von 0 und mit derselben Überschrift etc. diese gilt es zu eliminieren."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDJqz8MW5_xW"
      },
      "outputs": [],
      "source": [
        "tmp = ddf[[\"time\", \"stocks\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "rZouQiwXHy0r",
        "outputId": "9b7f2880-cc0c-4213-d001-4301fa2451fa"
      },
      "outputs": [],
      "source": [
        "#### Adding timedeltas to the data frame\n",
        "news_timedeltas = tmp.groupby(\"stocks\").transform(lambda x: x.diff())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrcTtcDj-UjE",
        "outputId": "40234ebb-e422-4975-9960-e8387ac0b38b"
      },
      "outputs": [],
      "source": [
        "# ~3 minutes evaluates to true\n",
        "# (news_timedeltas.index == ddf.index).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P77WnvamNhf-",
        "outputId": "47792774-bb9d-446b-ae11-8a9faab88f29"
      },
      "outputs": [],
      "source": [
        "ddf.loc[:, \"timedelta\"] = news_timedeltas.time.fillna(pd.Timedelta(days=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ij4ENbFQw7b"
      },
      "outputs": [],
      "source": [
        "news_timedeltas = ddf.timedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbnz32U8RFCj",
        "outputId": "d123e27d-36c4-4777-b5a9-d6fb4356e38e"
      },
      "outputs": [],
      "source": [
        "news_timedeltas.iloc[0].components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcSEhkw6JLHO"
      },
      "outputs": [],
      "source": [
        "same_day_timedeltas = news_timedeltas.apply(lambda x: x.components.days == 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2wKSEQ4JT1E",
        "outputId": "1b80ea3e-072c-4c0f-a068-e52bb6dbcda7"
      },
      "outputs": [],
      "source": [
        "(same_day_timedeltas == 0).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEOr6c8qsI7G"
      },
      "outputs": [],
      "source": [
        "same_hour_timedeltas = news_timedeltas.apply(lambda x: (x.components.days == 0) & \\\n",
        "                                               (x.components.hours == 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0wfw1X8sMxy",
        "outputId": "37ed353e-674e-462f-d1f7-73331e5d037a"
      },
      "outputs": [],
      "source": [
        "print(same_hour_timedeltas.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-KxzooAoIrp"
      },
      "outputs": [],
      "source": [
        "same_minute_timedeltas = news_timedeltas.apply(lambda x: (x.components.days == 0) & \\\n",
        "                                               (x.components.hours == 0) & \\\n",
        "                                               (x.components.minutes == 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-eIKdg7ogxA",
        "outputId": "1cdc2aec-8102-4f7a-b373-8e48be498941"
      },
      "outputs": [],
      "source": [
        "print(same_minute_timedeltas.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cECO9zSxX6G"
      },
      "outputs": [],
      "source": [
        "same_day_ddf = ddf.loc[same_day_timedeltas]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcTg5C9lUyQk",
        "outputId": "5aa9db7e-ac62-47f3-adc3-512e5f16234a"
      },
      "outputs": [],
      "source": [
        "ddf.stocks.value_counts().describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bshj1secWQ3P"
      },
      "source": [
        "Bis zu 5k Nachrichten pro Firma, z.B. AT&T, was in 13 Jahren ca. einer Nachricht pro Tag entspricht. Wir wollen nicht das eine Firma mit vielen Junk-Nachrichten das Modell dominiert.\n",
        "\n",
        "Kategorisieren von Nachrichten (mit Text2Topic, wie Salbrechter?) und eliminieren von Business/Strategic etc.\n",
        "Im Falle von Text2Topic, versuche Estimates des Unternehmens von Dritten zu unterscheiden.\n",
        "\n",
        "Wichtig!!! Unterscheide zwischen LERN-Phase und PRODUKTIONS-Phase.\n",
        "Wir können z.B. CLS-Token in der Produktions-Phase vergleichen, in der Lern-Phase aber noch nicht.\n",
        "\n",
        "Text2Vec -> Business category evtl. entfernen-> Intrastock variance average"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRKgjNUn7Pb3"
      },
      "source": [
        "## Topic Modeling (Nach Aschluss Auslagern in Python file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scaling up cluster and installing required packages\n",
        "cluster.scale(5)\n",
        "client.wait_for_workers(5)\n",
        "client.run(worker_setup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ddf = dd.read_parquet(\"gcs://extreme-lore-398917-bzg/latest1/\",\n",
        "                      storage_options={'token': token})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-Vkpo3vnhlK"
      },
      "outputs": [],
      "source": [
        "# Remove numbers\n",
        "docs = ddf.body.map(lambda x: re.sub(r'\\d+', '', x), meta=pd.Series(dtype=\"string\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs.name=\"body\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZyQX5zE2HZZ"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.topic_modeling.helpers import tokenize, keyword_filter\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ufa0owo6be4L",
        "outputId": "050d7474-07e9-4262-af9f-b0bfe9d0a6f0"
      },
      "outputs": [],
      "source": [
        "keywords_list1 = [\"launch\", \"business\", \"strategy\", \"management\", \"product\", \"service\", \"app\", \"customer\", \"merge\"]\n",
        "keywords_list2 = [\"upgrade\", \"downgrade\", \"raise\", \"cut\", \"buy\", \"sell\", \"hold\", \"outperform\", \"underperform\", \"analyst\", \"estimate\"]\n",
        "keywords_list3 = [\"ebit\", \"eps\", \"earnings\", \"report\", \"financial\", \"quarter\", \"annual\", \"year\", \"ended\", \"net\", \"income\"]\n",
        "total_keywords = keywords_list1 + keywords_list2 + keywords_list3\n",
        "len(total_keywords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdsbibbHEXa2"
      },
      "outputs": [],
      "source": [
        "# Tokenization + rough filtering\n",
        "tfidf_docs = docs.map(lambda x: keyword_filter(tokenize(x), total_keywords), meta=pd.Series(dtype=\"object\"))\n",
        "tfidf_docs = tfidf_docs.reset_index().repartition(npartitions=5)\n",
        "tfidf_docs.columns = [\"index\", \"body\"]\n",
        "tfidf_docs.to_parquet(\"gcs://extreme-lore-398917-bzg/tfidf-tokens\",\n",
        "                                                    storage_options={'token': token})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_docs = dd.read_parquet(\"gcs://extreme-lore-398917-bzg/tfidf-tokens/\",\n",
        "                                                    storage_options={'token': token},\n",
        "                                                    dtype_backend=\"pyarrow\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_docs = tfidf_docs.compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Numpy representation is without commas making us unable to convert strings to list via eval\n",
        "tfidf_docs.loc[:, \"body\"] = tfidf_docs.body.map(lambda x: x.replace(\" \", \", \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def f(tokens):\n",
        "    if len(tokens) == 0:\n",
        "        return np.array([\"wordtopreventemptydocumentswhencalculatingtfidf\"])\n",
        "    else:\n",
        "        return np.array(tokens)\n",
        "        \n",
        "tfidf_docs.loc[:, \"body\"] = tfidf_docs.body.map(lambda x: f(eval(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_docs.iloc[0].body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_tfidfs_list = []\n",
        "for topic_tokenizer in [keywords1, keywords2, keywords3]:\n",
        "    lazy_result = dask.delayed(TfidfVectorizer(tokenizer=topic_tokenizer, lowercase=False).fit_transform)(tfidf_docs.body)\n",
        "    lazy_sums = dask.delayed(np.apply_along_axis)(np.sum, 1, lazy_result.todense())\n",
        "    topic_tfidfs_list.append(lazy_sums)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_indicator_list = dask.compute(*topic_tfidfs_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame(topic_indicator_list)\n",
        "df=df.transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_docs.head().iloc[4].body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__BEnYJj1-1Z"
      },
      "source": [
        "### LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsdTIt0xOZ0k"
      },
      "outputs": [],
      "source": [
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import Phrases\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhGqa-Kbk0T_"
      },
      "outputs": [],
      "source": [
        "# Split the documents into tokens.\n",
        "lda_docs = docs.apply(lambda x: word_tokenize(str.lower(x)), meta=pd.Series(dtype=\"object\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyuoeiVWiKBp"
      },
      "outputs": [],
      "source": [
        "# Remove words that are only one character.\n",
        "# Lemmatize the documents.\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lda_docs = lda_docs.apply(lambda x: [lemmatizer.lemmatize(token) for token in x if len(token) >1], meta=pd.Series(dtype=\"object\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwqu5jn_B12x",
        "outputId": "b0e65f6c-9800-4466-897c-c1f5fd025a1e"
      },
      "outputs": [],
      "source": [
        "lda_docs.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7XOh3dLmi0X",
        "outputId": "9adca3c4-9b4d-463d-f222-4d58446a1418"
      },
      "outputs": [],
      "source": [
        "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
        "bigram = Phrases(docs, min_count=20)\n",
        "def f(doc):\n",
        "  doc = doc\n",
        "  for token in bigram[doc]:\n",
        "      if '_' in token:\n",
        "          # Token is a bigram, add to document.\n",
        "          doc.append(token)\n",
        "  return doc\n",
        "lda_docs = lda_docs.apply(lambda doc: f(doc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5GbbTglNiFI"
      },
      "outputs": [],
      "source": [
        "# lda_docs.name = \"body\"\n",
        "# lda_docs = lda_docs.to_frame().reset_index()\n",
        "# lda_docs = lda_docs.reset_index().repartition()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N60eZ9T-OD49"
      },
      "outputs": [],
      "source": [
        "# lda_docs.to_parquet(cwd + \"/data/lda_docs\", schema={\"body\": pa.string(),  \"index\":pa.int32()})\n",
        "# lda_docs = dd.read_parquet(cwd + \"/data/lda_docs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGD2ZOf0m4Kp"
      },
      "outputs": [],
      "source": [
        "# Remove rare and common tokens.\n",
        "\n",
        "# Create a dictionary representation of the documents.\n",
        "dictionary = Dictionary(docs)\n",
        "\n",
        "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
        "dictionary.filter_extremes(no_below=1000, no_above=0.6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlJ5ZyFknNi_",
        "outputId": "e52cdad5-cd53-4f90-dd6b-1fc5ecc3ffa2"
      },
      "outputs": [],
      "source": [
        "# Bag-of-words representation of the documents.\n",
        "corpus = [dictionary.doc2bow(doc) for doc in docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xfj1is7XnO27"
      },
      "outputs": [],
      "source": [
        "print('Number of unique tokens: %d' % len(dictionary))\n",
        "print('Number of documents: %d' % len(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJEDyIf-nTv6"
      },
      "outputs": [],
      "source": [
        "# Train LDA model.\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "# Set training parameters.\n",
        "num_topics = 10\n",
        "chunksize = 2000\n",
        "passes = 20\n",
        "iterations = 400\n",
        "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
        "\n",
        "# Make an index to word dictionary.\n",
        "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
        "id2word = dictionary.id2token\n",
        "\n",
        "model = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    chunksize=chunksize,\n",
        "    alpha='auto',\n",
        "    eta='auto',\n",
        "    iterations=iterations,\n",
        "    num_topics=num_topics,\n",
        "    passes=passes,\n",
        "    eval_every=eval_every\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74ZPDdqSnVoN"
      },
      "outputs": [],
      "source": [
        "top_topics = model.top_topics(corpus)\n",
        "\n",
        "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
        "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
        "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
        "\n",
        "from pprint import pprint\n",
        "pprint(top_topics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRABrPVOcaa9"
      },
      "source": [
        "## Nachrichten-Parsing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster.scale(3)\n",
        "client.wait_for_workers(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ddf = dd.read_parquet(\"gcs://extreme-lore-398917-bzg/latest1/\",\n",
        "                      storage_options={'token': token})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Beispiel/ Untersuchung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keF_8QNAkM36"
      },
      "outputs": [],
      "source": [
        "sample_partition = ddf.get_partition(20)\n",
        "y = sample_partition.head(5)\n",
        "y.loc[:, \"time\"] = pd.to_datetime(y.time).dt.tz_convert(\"UTC\")\n",
        "x = y.iloc[2]\n",
        "x.body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSAGuM_Acrwr"
      },
      "outputs": [],
      "source": [
        "filter_body(x, logging=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj6mofzeqcqB"
      },
      "source": [
        "### Anwenden der filter_body-Funktion auf alle Reihen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def handle_timezone(x):\n",
        "    try:\n",
        "        return x.tz_convert(\"US/Eastern\")\n",
        "    except Exception as e:\n",
        "        return x.tz_localize(\"US/Eastern\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Still need to parse time correctly...\n",
        "ddf[\"time\"] = ddf[\"time\"].map(lambda x: handle_timezone(pd.to_datetime(x)), meta=pd.Series(dtype=\"datetime64[ns, US/Eastern]\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGlOJPEvFxuP"
      },
      "outputs": [],
      "source": [
        "# part_n = 1\n",
        "# par = ddf.get_partition(part_n)\n",
        "par = ddf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gPPJDRKPv72"
      },
      "outputs": [],
      "source": [
        "par[\"parsed_body\"] = par.apply(lambda x: filter_body(x),\n",
        "                                axis=1,\n",
        "                                meta=pd.Series(dtype=\"string\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "par.persist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# par.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vlCN3tKDfnw8"
      },
      "outputs": [],
      "source": [
        "par.to_parquet(\"gcs://extreme-lore-398917-bzg/processed_news\",\n",
        "                storage_options={'token': token})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uPo1Ruo0N0q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaK5pLz4mRHF"
      },
      "source": [
        "## Analyse der durschnittlichen Tokenlänge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y192tnIfpIXj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0bW6kYmpI1W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VG5rnSiNpWVN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMBWNMfIpebc"
      },
      "source": [
        "## Generating timestamp-stock pairs in order to know for which days we need stock data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mc6lpRCpfVT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
