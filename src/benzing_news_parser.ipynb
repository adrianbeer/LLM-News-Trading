{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkovyr-oc4-K"
      },
      "source": [
        "# Benzinga-Nachrichten-Verarbeitung"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A8GOctQmMfj"
      },
      "source": [
        "## Setup up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxfEWak8MNGE"
      },
      "source": [
        "### Cluster spin up"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "cwd=\"/content/drive/MyDrive/NewsTrading/trading_bot\"\n",
        "%cd /content/drive/MyDrive/NewsTrading/trading_bot\n",
        "%pip install -r requirements_clean.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pmVviAjMTaQ",
        "outputId": "09531355-3d92-4489-fcdc-636d80a51074"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/NewsTrading/trading_bot\n",
            "Requirement already satisfied: html2text in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 4)) (2020.1.16)\n",
            "Requirement already satisfied: datefinder in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 5)) (0.7.3)\n",
            "Requirement already satisfied: dask[complete] in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 6)) (2023.8.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 7)) (3.8.1)\n",
            "Requirement already satisfied: dateparser in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 8)) (1.2.0)\n",
            "Requirement already satisfied: sutime in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 9)) (1.0.1)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 10)) (0.2.33)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 11)) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 12)) (1.2.2)\n",
            "Requirement already satisfied: jupyter-server-proxy in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 15)) (4.1.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 16)) (7.0.3)\n",
            "Requirement already satisfied: fsspec==2023.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 19)) (2023.4.0)\n",
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 20)) (2023.4.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements_clean.txt (line 24)) (4.35.2)\n",
            "Requirement already satisfied: regex>=2017.02.08 in /usr/local/lib/python3.10/dist-packages (from datefinder->-r requirements_clean.txt (line 5)) (2023.6.3)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from datefinder->-r requirements_clean.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from datefinder->-r requirements_clean.txt (line 5)) (2023.3.post1)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]->-r requirements_clean.txt (line 6)) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]->-r requirements_clean.txt (line 6)) (2.2.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]->-r requirements_clean.txt (line 6)) (23.2)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]->-r requirements_clean.txt (line 6)) (1.4.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask[complete]->-r requirements_clean.txt (line 6)) (6.0.1)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]->-r requirements_clean.txt (line 6)) (0.12.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]->-r requirements_clean.txt (line 6)) (7.0.0)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]->-r requirements_clean.txt (line 6)) (10.0.1)\n",
            "Requirement already satisfied: lz4>=4.3.2 in /usr/local/lib/python3.10/dist-packages (from dask[complete]->-r requirements_clean.txt (line 6)) (4.3.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements_clean.txt (line 7)) (1.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements_clean.txt (line 7)) (4.66.1)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser->-r requirements_clean.txt (line 8)) (5.2)\n",
            "Requirement already satisfied: JPype1<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from sutime->-r requirements_clean.txt (line 9)) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from yfinance->-r requirements_clean.txt (line 10)) (1.23.5)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from yfinance->-r requirements_clean.txt (line 10)) (2.31.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance->-r requirements_clean.txt (line 10)) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance->-r requirements_clean.txt (line 10)) (4.9.3)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from yfinance->-r requirements_clean.txt (line 10)) (1.4.4)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance->-r requirements_clean.txt (line 10)) (2.3.10)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.10/dist-packages (from yfinance->-r requirements_clean.txt (line 10)) (3.17.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance->-r requirements_clean.txt (line 10)) (4.11.2)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance->-r requirements_clean.txt (line 10)) (1.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements_clean.txt (line 12)) (1.11.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements_clean.txt (line 12)) (3.2.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from jupyter-server-proxy->-r requirements_clean.txt (line 15)) (3.9.1)\n",
            "Requirement already satisfied: jupyter-server>=1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server-proxy->-r requirements_clean.txt (line 15)) (1.24.0)\n",
            "Requirement already satisfied: simpervisor>=1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server-proxy->-r requirements_clean.txt (line 15)) (1.0.0)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs->-r requirements_clean.txt (line 20)) (4.4.2)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs->-r requirements_clean.txt (line 20)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (from gcsfs->-r requirements_clean.txt (line 20)) (1.2.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (from gcsfs->-r requirements_clean.txt (line 20)) (2.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements_clean.txt (line 24)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements_clean.txt (line 24)) (0.17.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements_clean.txt (line 24)) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements_clean.txt (line 24)) (0.4.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance->-r requirements_clean.txt (line 10)) (2.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs->-r requirements_clean.txt (line 20)) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs->-r requirements_clean.txt (line 20)) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs->-r requirements_clean.txt (line 20)) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs->-r requirements_clean.txt (line 20)) (4.9)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance->-r requirements_clean.txt (line 10)) (0.5.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers->-r requirements_clean.txt (line 24)) (4.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask[complete]->-r requirements_clean.txt (line 6)) (3.17.0)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (3.7.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (23.1.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (3.1.2)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (6.1.12)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (5.5.1)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (6.5.4)\n",
            "Requirement already satisfied: nbformat>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (5.9.2)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (0.19.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (23.2.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (0.18.0)\n",
            "Requirement already satisfied: tornado>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (6.3.2)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (5.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (1.7.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.2.0->dask[complete]->-r requirements_clean.txt (line 6)) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance->-r requirements_clean.txt (line 10)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance->-r requirements_clean.txt (line 10)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance->-r requirements_clean.txt (line 10)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance->-r requirements_clean.txt (line 10)) (2023.11.17)\n",
            "Requirement already satisfied: bokeh>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from dask[complete]->-r requirements_clean.txt (line 6)) (3.3.2)\n",
            "Requirement already satisfied: distributed==2023.8.1 in /usr/local/lib/python3.10/dist-packages (from dask[complete]->-r requirements_clean.txt (line 6)) (2023.8.1)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1->dask[complete]->-r requirements_clean.txt (line 6)) (1.0.7)\n",
            "Requirement already satisfied: psutil>=5.7.2 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1->dask[complete]->-r requirements_clean.txt (line 6)) (5.9.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1->dask[complete]->-r requirements_clean.txt (line 6)) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1->dask[complete]->-r requirements_clean.txt (line 6)) (3.0.0)\n",
            "Requirement already satisfied: zict>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1->dask[complete]->-r requirements_clean.txt (line 6)) (3.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib->gcsfs->-r requirements_clean.txt (line 20)) (1.3.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs->-r requirements_clean.txt (line 20)) (2.11.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs->-r requirements_clean.txt (line 20)) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs->-r requirements_clean.txt (line 20)) (2.7.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1 in /usr/local/lib/python3.10/dist-packages (from bokeh>=2.4.2->dask[complete]->-r requirements_clean.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh>=2.4.2->dask[complete]->-r requirements_clean.txt (line 6)) (9.4.0)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh>=2.4.2->dask[complete]->-r requirements_clean.txt (line 6)) (2023.10.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs->-r requirements_clean.txt (line 20)) (1.62.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs->-r requirements_clean.txt (line 20)) (3.20.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage->gcsfs->-r requirements_clean.txt (line 20)) (1.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (2.1.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (4.1.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (0.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (0.9.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (1.5.0)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (2.16.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.2.0->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (2.19.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.2.0->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (4.19.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs->-r requirements_clean.txt (line 20)) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs->-r requirements_clean.txt (line 20)) (3.2.2)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (21.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (0.15.2)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server>=1.0->jupyter-server-proxy->-r requirements_clean.txt (line 15)) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zj0QodzRMNGE"
      },
      "outputs": [],
      "source": [
        "import sys, io, tarfile, os\n",
        "import re\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "import dask\n",
        "import dask.dataframe as dd\n",
        "from dask.distributed import Client\n",
        "from distributed.diagnostics.plugin import WorkerPlugin\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "from src.preprocessing.news_parser import get_company_abbreviation, yahoo_get_wrapper, \\\n",
        "                                          infer_author, filter_body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w1-9R-RMNGH",
        "outputId": "59af494f-22ad-4f3f-bb9c-a7a4f0c2721c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.scheduler:State start\n",
            "INFO:distributed.diskutils:Found stale lock file and directory '/tmp/dask-scratch-space/scheduler-uso0ra8n', purging\n",
            "INFO:distributed.diskutils:Found stale lock file and directory '/tmp/dask-scratch-space/worker-705zofuk', purging\n",
            "INFO:distributed.diskutils:Found stale lock file and directory '/tmp/dask-scratch-space/worker-_yh46wb_', purging\n",
            "INFO:distributed.diskutils:Found stale lock file and directory '/tmp/dask-scratch-space/worker-ko56lffv', purging\n",
            "INFO:distributed.diskutils:Found stale lock file and directory '/tmp/dask-scratch-space/worker-mb66l63q', purging\n",
            "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:35423\n",
            "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:8787/status\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:45799'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:42839'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:39987'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:35883'\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:36499', name: 2, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:36499\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:55396\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:46259', name: 0, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:46259\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:55376\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:46127', name: 3, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:46127\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:55392\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:38671', name: 1, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:38671\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:55406\n",
            "INFO:distributed.scheduler:Receive client connection: Client-bb74ca5b-a3d9-11ee-810f-0242ac1c000c\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:55420\n"
          ]
        }
      ],
      "source": [
        "from dask.distributed import Client, LocalCluster\n",
        "cluster = LocalCluster()\n",
        "client = Client(cluster)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "hqoPHxpNMNGH",
        "outputId": "1c646639-eb4e-4481-d0b2-0cb9d1084e76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-12-26T10:30:04+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n",
            "ERROR:pyngrok.process.ngrok:t=2023-12-26T10:30:04+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session obj=csess id=abecdc728234 err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent session.\\nYou can run multiple tunnels on a single agent session using a configuration file.\\nTo learn more, see https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config/\\n\\nActive ngrok agent sessions in region 'us':\\n  - ts_2a4Y2aAFqESy92mqZjwL0bYHb8w (104.196.32.137)\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2023-12-26T10:30:04+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent session.\\nYou can run multiple tunnels on a single agent session using a configuration file.\\nTo learn more, see https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config/\\n\\nActive ngrok agent sessions in region 'us':\\n  - ts_2a4Y2aAFqESy92mqZjwL0bYHb8w (104.196.32.137)\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-98ce03b91a9b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2WntwErWDt9LxQ2Jfp6C8OxDAMK_7iZVdC1utyZET1PE8cuUg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8787\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublic_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" * ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpublic_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8787\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"auth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating tunnel with options: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartup_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             raise PyngrokNgrokError(\"The ngrok process errored on start: {}.\".format(ngrok_process.startup_error),\n\u001b[0m\u001b[1;32m    396\u001b[0m                                     \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                                     ngrok_process.startup_error)\n",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Your account is limited to 1 simultaneous ngrok agent session.\\nYou can run multiple tunnels on a single agent session using a configuration file.\\nTo learn more, see https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config/\\n\\nActive ngrok agent sessions in region 'us':\\n  - ts_2a4Y2aAFqESy92mqZjwL0bYHb8w (104.196.32.137)\\r\\n\\r\\nERR_NGROK_108\\r\\n."
          ]
        }
      ],
      "source": [
        "## Set Up NGROK-Tunnel\n",
        "conf.get_default().auth_token = \"2WntwErWDt9LxQ2Jfp6C8OxDAMK_7iZVdC1utyZET1PE8cuUg\"\n",
        "\n",
        "public_url = ngrok.connect(8787).public_url\n",
        "print(\" * ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, 8787))\n",
        "\n",
        "if 'client' not in globals():\n",
        "  !python -m pip install jupyter-server-proxy\n",
        "  client = Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CymECzDhh6Mn"
      },
      "source": [
        "### System settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4lfXyeQvfCn"
      },
      "outputs": [],
      "source": [
        "cwd = os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCMMsTZyC0BE"
      },
      "source": [
        "### CUDF für hardware acceleration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqFqwisUh6Mn"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "# !python rapidsai-csp-utils/colab/pip-install.py\n",
        "# import cudf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRqa0EPy_lpx"
      },
      "source": [
        "## Grobes HTML-Parsing\n",
        "Als erstes müssen wir die HTML-Dokumente zu normalem Text umwandeln, ansonsten sind die Text-Zellen zu groß und führen zu Problemen mit PyArrow/Dask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4rQifxX6ygR"
      },
      "outputs": [],
      "source": [
        "input_dir = \"data/raw_bzg/\"\n",
        "output_dir = 'data/unraw1_bzg/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wy5-WZ4B_A2"
      },
      "outputs": [],
      "source": [
        "# for year in range(2019, 2020):\n",
        "#     print(year)\n",
        "#     df = pd.read_parquet(f\"{input_dir}story_df_raw_{year}.parquet\")\n",
        "#     df = dd.from_pandas(df, npartitions=12)\n",
        "#     df[\"html_body\"] = df[\"html_body\"].apply(body_formatter, meta=pd.Series(dtype=\"str\"))\n",
        "#     df = df.rename(columns={\"html_body\":\"body\"})\n",
        "#     name_function = lambda x: f\"data-{year}-{x}.parquet\"\n",
        "#     df.to_parquet(output_dir, name_function=name_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa6QpqfR_t97"
      },
      "source": [
        "## Neu-Partitionierug\n",
        "Sodass alle Partitionen etwa die gleiche Größe haben."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHFpynyiq3O4"
      },
      "outputs": [],
      "source": [
        "input_dir = 'data/unraw1_bzg/'\n",
        "output_dir = 'data/unraw2_bzg/'\n",
        "\n",
        "# ddf = dd.read_parquet(input_dir+\"*.parquet\")\n",
        "# ddf2 = ddf.repartition(npartitions=50)\n",
        "# name_function = lambda x: f\"data-{x}.parquet\"\n",
        "# ddf2.to_parquet(output_dir, name_function=name_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFnIFQW2Nag7"
      },
      "source": [
        "## Author-Inferenz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__EYo7CaFlVj"
      },
      "source": [
        "Ein bisschen die Daten säubern..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-jzA2AbFptW"
      },
      "outputs": [],
      "source": [
        "input_dir = cwd+'/data/unraw2_bzg/'\n",
        "output_dir = cwd+'/data/unraw3_bzg/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef28p8AOnrFD"
      },
      "outputs": [],
      "source": [
        "ddf = dd.read_parquet(input_dir+\"*.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylwarwpXLegE"
      },
      "outputs": [],
      "source": [
        "# Remove rows for which noo stock ticker is recorded\n",
        "ddf = ddf[ddf.stocks != '']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7OPUatcMS7C"
      },
      "outputs": [],
      "source": [
        "# Convert `channels`  datatype from string to list\n",
        "ddf[\"channels\"] = ddf[\"channels\"].apply(eval, meta=pd.Series(dtype='object'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu1EhTqrVpBG"
      },
      "source": [
        "Untersuche als nächstes die Behauptung, dass **PRNewswire** und **Businesswire** den gesamten Markt für Pressemeldungen in den USA kontrollieren. Wenn dem so ist, und sie nicht noch weitere, unwichtige Meldungen veröffentlichen, dann können wir einfach die Newsartikel nach diesen Autoren filtern und uns viel Arbeit ersparen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5mO091MfO6e"
      },
      "outputs": [],
      "source": [
        "dask.config.set(scheduler=\"processes\")\n",
        "ddf[\"inferred_author\"] = None\n",
        "ddf[\"inferred_author\"] = ddf.body.apply(infer_author, meta=pd.Series(dtype=\"string\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTcPjnU0VZZu"
      },
      "outputs": [],
      "source": [
        "# value_counts for authors\n",
        "auhtor_value_counts = pd.concat([ddf.author.value_counts().head(10), ddf.inferred_author.value_counts().head(10)], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNesN9jX1IP0"
      },
      "outputs": [],
      "source": [
        "auhtor_value_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKT6tusBp13y"
      },
      "outputs": [],
      "source": [
        "auhtor_value_counts.sum().diff()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk6p6woeqJqP"
      },
      "source": [
        "Ungefähr 650k Nachrichten werden ausgelassen, wenn nur die vier Hauptvertreiber von Pressemeldungen berücksichtigt werden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X8rHo-CsDCQ"
      },
      "outputs": [],
      "source": [
        "ddf = ddf[~ddf.inferred_author.isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHvzym8sFcX_"
      },
      "outputs": [],
      "source": [
        "ddf[\"inferred_author\"] = ddf[\"inferred_author\"].astype(\"string\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufk8_XUjFwdk"
      },
      "outputs": [],
      "source": [
        "ddf[\"channels\"] = ddf.channels.apply(lambda x: str(x), meta=pd.Series(dtype=\"string\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsEmViTzNvgs"
      },
      "outputs": [],
      "source": [
        "ddf.inferred_author.value_counts().compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfNNyD_31sly"
      },
      "outputs": [],
      "source": [
        "ddf.inferred_author.value_counts().sum().compute()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ddf = ddf.drop(columns=[\"author\"]).rename(columns={\"inferred_author\":\"author\"})"
      ],
      "metadata": {
        "id": "pmMI9DsEhrn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v04Wy5Y4iudn"
      },
      "outputs": [],
      "source": [
        "name_function = lambda x: f\"data-{x}.parquet\"\n",
        "ddf.to_parquet(output_dir, name_function=name_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLdnwdyxNFiH"
      },
      "outputs": [],
      "source": [
        "# Contains 100k rows\n",
        "earnings_ddf = ddf[ddf.channels.apply(lambda x: \"Earnings\" in x, meta=pd.Series(dtype=bool))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-zs5SSiVbyp"
      },
      "outputs": [],
      "source": [
        "# value counts for authors of earnings reports (contrast to value counts of all news articles)\n",
        "earnings_ddf.inferred_author.value_counts().head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dVD6j-G9nie"
      },
      "source": [
        "Hier sehen wir, dass es keine einzige Pressemeldung von **Business Wire** gibt, die mit *Earnings* gekennzeichnet sind. Trotzdem gibt es relevante *Earnings* reports von Business Wire. Dies habe ich kurz verifiziert..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jafqZ87tXDKn"
      },
      "source": [
        "## Vorgehen\n",
        "\n",
        "Benötigt:\n",
        "---------\n",
        "- data_shared/corporation_endings.txt \\\\\n",
        "- input_dir = data/unraw3_bzg/ \\\\\n",
        "\n",
        "Produziert:\n",
        "-----------\n",
        "- data_shared/all_ticker_name_mapper.parquet\n",
        "\n",
        "--------------\n",
        "\n",
        "Wie viele Nachrichten bleiben, wenn wir auf relevante Ticker filtern? Wir wollen nicht(!) - so ist es momentan - auf die momentane Russell 3k-Zusammensetzung filtern, denn wir wollen auch ungelistete bzw. ehemalige Russell-Aktien beachten.\n",
        "\n",
        "\n",
        "**1. Full-Name-Discovery:**\n",
        "\n",
        "Herausfinden des vollen Namens des Unternehmens für jeden Ticker, damit 1. der Text richtig geparst werden kann und 2. damit wir einen Anhaltspunkt für das Ticker-Grouping haben.\n",
        "\n",
        "\n",
        "**2. Ticker-Filtering:**\n",
        "\n",
        "Alle Ticker herausfiltern, die wir nicht brauchen. Wenn wir aber ein großes Aktienuniversum (mit inzwischen ungelisteten Aktien) benutzen, werden wir fast alle Nachrichten behalten können. Allerdings lassen sich so Fehlerhafte Nachrichten/Ticker etc. herausfiltern.\n",
        "\n",
        "\n",
        "**3. Ticker-Grouping:**\n",
        "\n",
        "Was machen wir, wenn wir mehrerer Aktiengattungen für ein Unternehmen haben? Z.B. Vorzugs- und Stammaktien. Wir können i.A. die Stammaktie nehmen, da diese normalerweise ein höheres Handelsvolumen aufweist. D.h. wir bilden alle Ticker der Benzinga-Nachrichten auf den Ticker der Stammaktie ab.\n",
        "\n",
        "\n",
        "**4. Firmennamen-Nachrichtenkörper-Verifikation:**\n",
        "\n",
        "Da Ticker wiederverwendet werden können bzw. sich verändern können wollen wir sicherstellen, dass der Unternehmensname im Nachrichtenkörper vorkommt! Bzw. generell ist das eine gute Datensäuberungs-Maßnahme."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VaSble_1ku1t"
      },
      "outputs": [],
      "source": [
        "input_dir = cwd+'/data/unraw3_bzg/'\n",
        "ddf = dd.read_parquet(input_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JosrB0RfsUsy"
      },
      "outputs": [],
      "source": [
        "all_tickers = ddf.stocks.unique().compute()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tickers sometimes have a dollar sign in front of them.\n",
        "# This is common practice to indicate that the acronym refers to a stock ticker.\n",
        "# This results in ~ 4 duplicate entries.\n",
        "all_tickers = all_tickers.apply(lambda x: x.strip(\"$\"))"
      ],
      "metadata": {
        "id": "JQ3o-6Zimwhh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some stock tickers are in lowercase.\n",
        "# Altough yfinance can handle lowercase tickers we uppercase them\n",
        "# in order to avoid inconsistencies and avoid duplicates.\n",
        "all_tickers = all_tickers.str.upper()\n",
        "all_tickers = all_tickers.drop_duplicates()"
      ],
      "metadata": {
        "id": "BglCX_xQnjNw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colon_tickers = [x for x in all_tickers.values if \":\" in x]\n",
        "print(f\"Around {len(colon_tickers)} stock tickers are from foreign exchanges. \\n\"\n",
        "      f\"The list of foreign exchanges is:\")\n",
        "set([x.split(\":\")[0] for x in colon_tickers])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHKsMwElpmBr",
        "outputId": "b572e960-f464-44dd-85dd-a13d372f4204"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Around 2741 stock tickers are from foreign exchanges. \n",
            "The list of foreign exchanges is:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'BMV', 'CSE', 'PARIS', 'TSX', 'TSXV', 'TYO'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We remove these foreign exchanges:\n",
        "all_tickers = all_tickers[all_tickers.apply(lambda x: \":\" not in x)]"
      ],
      "metadata": {
        "id": "KkJtizLXrOgR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do Whitespaces exit and prevent yfinance from finding company names?\n",
        "# Only one company, and its not even a ticker but the company name\n",
        "all_tickers[all_tickers.apply(lambda x: \" \" in x)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H--sFC-J8cF6",
        "outputId": "deadf50e-af9d-4c5a-bbce-f0ebb50ead8e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10892    SERA PROGNOSTICS, INC.\n",
              "Name: stocks, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdwKGnsPnkYS"
      },
      "source": [
        "### Full-Name-Discovery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8urd8wvsXza"
      },
      "outputs": [],
      "source": [
        "company_names = all_tickers.map(lambda x: yahoo_get_wrapper(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NUslIwYdsmU7"
      },
      "outputs": [],
      "source": [
        "all_mapper = pd.concat([all_tickers, company_names], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tW8AGmgfK_oV"
      },
      "outputs": [],
      "source": [
        "all_mapper.columns = [\"ticker\", \"company_name\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"For {all_mapper[all_mapper.isna().any(axis=1)].shape[0]} tickers, yfinance had no entry, or at least not entry for the `longName`\")\n",
        "# E.g. AIS, PTNR, BSI, LUFK, BFRM. Can't find these stocks on guidants as well and\n",
        "# Some are not headquartered in the US."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-yCIl7vEuUk",
        "outputId": "706c65bd-d30e-4ef3-f5cf-ee3acf77ef3b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For 8492 tickers, yfinance had no entry, or at least not entry for the `longName`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qa3sz1yBMkjw"
      },
      "outputs": [],
      "source": [
        "all_mapper = all_mapper.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FEP_5x3P9F2",
        "outputId": "eec5f5a7-cedf-4c66-b582-ec4cb5062de1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:distributed.utils_perf:full garbage collections took 11% CPU time recently (threshold: 10%)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "556"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Multiple tickers for the same company exist for ~556/2 companies\n",
        "vcs = all_mapper.company_name.value_counts()\n",
        "vcs = vcs[vcs >= 2]\n",
        "all_mapper[all_mapper.company_name.isin(vcs.index)].sort_values(\"company_name\").shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0vWtCfFSoox"
      },
      "source": [
        "Es ist nicht leicht zu sagen, welchen von den Tickern wir bevorzugen sollten. Abgleichen mit den Aktientickern des Kursdatensatzes notwendig, um zu sehen, ob überhaupt nur ein Ticker übereinstimmt. Wenn es für beide Ticker eine Kurszeitreihe gibt, dann sollten wir die nehmen, die ein höheres historisches Volumen hat. Dies ist allerdings etwas, was wir später machen und nicht jetzt. Hier wollen wir zunächst nur die Nachrichten verarbeiten, weswegen wir nur die NaN-Unternehmen rausnehmen und den Rest - *ohne Ticker-Filtering* - weiterverarbeiten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Y6J-M2i4WeW2"
      },
      "outputs": [],
      "source": [
        "mapper = all_mapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dvJpTNzVFmLZ"
      },
      "outputs": [],
      "source": [
        "mapper.columns = [\"ticker\", \"company_names\"]\n",
        "mapper.set_index(\"ticker\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-t2omABfXZRK"
      },
      "outputs": [],
      "source": [
        "company_endings = pd.read_table(\"data_shared/corporation_endings.txt\").iloc[:, 0]\n",
        "# Apply `get_company_abbreviation` twice in order to get rid of Enterprise, Ltd.\n",
        "# Otherwise , Ltd. remains. If no acronym, name stays as is.\n",
        "mapper[\"short_name\"] = mapper.company_names.apply(lambda x: get_company_abbreviation(x, company_endings=company_endings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "z3d5wM_qWhvx"
      },
      "outputs": [],
      "source": [
        "mapper = mapper.applymap(lambda x: x.strip(\" \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "voNHNOrXSIKL"
      },
      "outputs": [],
      "source": [
        "mapper.to_parquet(cwd + \"/data_shared/ticker_name_mapper.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "yrLjdrfwlmRm"
      },
      "outputs": [],
      "source": [
        "mapper = pd.read_parquet(cwd + \"/data_shared/ticker_name_mapper.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "70POMUC-PN-n"
      },
      "outputs": [],
      "source": [
        "filt_ddf = ddf[ddf.stocks.isin(mapper.index.to_list())]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZszkSxWPBK9",
        "outputId": "7c1f8c19-0a4d-4a84-b1d7-2f9113402a59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1436519"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "n = ddf.map_partitions(lambda x: x.shape[0]).compute()\n",
        "print(f\"Es verbleiben {n.sum()} Nachrichten, für die wir den Ticker zu einem Firmennamen auflösen können.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLbhWydOPaaK",
        "outputId": "67783c42-0623-42c3-e7d4-a5b235e2b0fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1002293"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "n = filt_ddf.map_partitions(lambda x: x.shape[0]).compute()\n",
        "n.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "xSH8cQgL5tZu"
      },
      "outputs": [],
      "source": [
        "ddf = filt_ddf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "safdLC-H6GR5"
      },
      "outputs": [],
      "source": [
        "ddf[\"company_name\"] = ddf.stocks.map(lambda x: mapper.company_names.loc[x], meta=pd.Series(dtype=\"string\"))\n",
        "ddf[\"short_name\"] = ddf.stocks.map(lambda x: mapper.short_name.loc[x], meta=pd.Series(dtype=\"string\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xw1JSiI_0JH"
      },
      "source": [
        "### Duplikate Entfernen"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = ddf.map_partitions(lambda x: x.shape[0]).compute().sum()\n",
        "n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92hxVN_efR9f",
        "outputId": "4b1a6b84-a7cc-4049-b36a-c6904fe7256b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1002293"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "cEzrJqEN_yuJ"
      },
      "outputs": [],
      "source": [
        "ddf = ddf.map_partitions(lambda df: df.drop_duplicates())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = ddf.map_partitions(lambda x: x.shape[0]).compute().sum()\n",
        "n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0qVBh55fwez",
        "outputId": "c130b590-6e5c-4d68-db4e-f8ebe0546e1a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "999448"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name_function = lambda x: f\"data-{x}.parquet\"\n",
        "ddf.to_parquet(cwd+'/data/latest/', name_function=name_function)"
      ],
      "metadata": {
        "id": "wQlXGkufSq-w"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F3dOc35YN8k"
      },
      "source": [
        "### Firmennamen-Nachrichtenkörper-Verifikation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "w91ht_CvPZDx"
      },
      "outputs": [],
      "source": [
        "ddf = dd.read_parquet(cwd+'/data/latest/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ddf.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CN9L3axUh6f7",
        "outputId": "317960f6-63dc-4b16-8976-44cb50bbdaf2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "time            datetime64[ns, pytz.FixedOffset(-240)]\n",
              "stocks                                          string\n",
              "title                                           string\n",
              "channels                                        object\n",
              "body                                            string\n",
              "author                                          string\n",
              "company_name                                    object\n",
              "short_name                                      object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If short_name doesn't occurr in the title or in the body, then article seems to be faulty\n",
        "mask = ddf.apply(lambda x:\n",
        "                 bool(re.search(x[\"short_name\"],\n",
        "                                x.title + x[\"body\"].replace(\"( )*\\n( )*\", \" \"),\n",
        "                                re.IGNORECASE)),\n",
        "                 axis=1,\n",
        "                 meta=pd.Series(dtype=bool))"
      ],
      "metadata": {
        "id": "d46wUvcQTzB8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FBv0rbEKYULN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ec535ad-1ce7-4931-e2ff-f5ed619f5709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Around 11158 stocks before filtering\n",
            "Around 9670 stocks after filtering\n"
          ]
        }
      ],
      "source": [
        "print(f\"Around {len(ddf.stocks.unique())} stocks before filtering\")\n",
        "print(f\"Around {len(ddf[mask].stocks.unique())} stocks after filtering\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oNAFvvVmmWm2"
      },
      "outputs": [],
      "source": [
        "# Filter out faulty news\n",
        "ddf = ddf[mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GvhpDVtFh6M6"
      },
      "outputs": [],
      "source": [
        "name_function = lambda x: f\"data-{x}.parquet\"\n",
        "ddf.to_parquet(cwd+'/data/latest2/', name_function=name_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SurJhZXHx0U"
      },
      "source": [
        "### Timedeltas zwischen Nachrichtenmeldungen\n",
        "\n",
        "\n",
        "\n",
        "Wir sehen, dass einige Nachrichten dupliziert vorkommen, d.h. mit einem Timedelta von 0 und mit derselben Überschrift etc. diese gilt es zu eliminieren."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HxMOTfgjpvs"
      },
      "source": [
        "#### Convert ddf to pd.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ddf = dd.read_parquet(cwd+'/data/latest2/')"
      ],
      "metadata": {
        "id": "QpskPuAUkXXu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sPF987k3H8tC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3ef1768-e8c5-41e1-bc83-3ccddc5baf27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Scheduler for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 4.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n"
          ]
        }
      ],
      "source": [
        "ddf = ddf.compute()\n",
        "ddf = ddf.sort_values(\"time\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NDJqz8MW5_xW"
      },
      "outputs": [],
      "source": [
        "tmp = ddf[[\"time\", \"stocks\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rZouQiwXHy0r"
      },
      "outputs": [],
      "source": [
        "#### Adding timedeltas to the data frame\n",
        "news_timedeltas = tmp.groupby(\"stocks\").transform(lambda x: x.diff())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrcTtcDj-UjE"
      },
      "outputs": [],
      "source": [
        "# ~3 minutes evaluates to true\n",
        "# (news_timedeltas.index == ddf.index).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "P77WnvamNhf-"
      },
      "outputs": [],
      "source": [
        "ddf.loc[:, \"timedelta\"] = news_timedeltas.time.fillna(pd.Timedelta(days=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3Ij4ENbFQw7b"
      },
      "outputs": [],
      "source": [
        "news_timedeltas = ddf.timedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbnz32U8RFCj",
        "outputId": "d66ef334-9084-4d9e-c628-84e0234a3846"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Components(days=100, hours=0, minutes=0, seconds=0, milliseconds=0, microseconds=0, nanoseconds=0)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "news_timedeltas.iloc[0].components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zcSEhkw6JLHO"
      },
      "outputs": [],
      "source": [
        "same_day_timedeltas = news_timedeltas.apply(lambda x: x.components.days == 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2wKSEQ4JT1E",
        "outputId": "39774545-8237-49a0-dae1-1cafbbbc3d5f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "699538"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "(same_day_timedeltas == 0).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xEOr6c8qsI7G"
      },
      "outputs": [],
      "source": [
        "same_hour_timedeltas = news_timedeltas.apply(lambda x: (x.components.days == 0) & \\\n",
        "                                               (x.components.hours == 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0wfw1X8sMxy",
        "outputId": "be2f4947-7119-442c-d4db-2e2c18c4a9ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43633\n"
          ]
        }
      ],
      "source": [
        "print(same_hour_timedeltas.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "L-KxzooAoIrp"
      },
      "outputs": [],
      "source": [
        "same_minute_timedeltas = news_timedeltas.apply(lambda x: (x.components.days == 0) & \\\n",
        "                                               (x.components.hours == 0) & \\\n",
        "                                               (x.components.minutes == 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-eIKdg7ogxA",
        "outputId": "a07fd68c-cde7-4465-d379-49fa0349b443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16328\n"
          ]
        }
      ],
      "source": [
        "print(same_minute_timedeltas.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "6cECO9zSxX6G"
      },
      "outputs": [],
      "source": [
        "same_day_ddf = ddf.loc[same_day_timedeltas]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcTg5C9lUyQk",
        "outputId": "5b92366d-1884-4df7-e7a4-f598e0e48a86"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    9670.000000\n",
              "mean       90.503413\n",
              "std       164.442156\n",
              "min         1.000000\n",
              "25%         4.000000\n",
              "50%        31.000000\n",
              "75%       117.000000\n",
              "max      5025.000000\n",
              "Name: stocks, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "ddf.stocks.value_counts().describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bshj1secWQ3P"
      },
      "source": [
        "Bis zu 5k Nachrichten pro Firma, z.B. AT&T, was in 13 Jahren ca. einer Nachricht pro Tag entspricht. Wir wollen nicht das eine Firma mit vielen Junk-Nachrichten das Modell dominiert. Wobei das Modell hoffentlich dann auch die Junk-Nachrichten als solche erkennt und ignoriert. Eher wichtig noch einen `staleness`-Faktor, also wie ähnlich die Nachricht zu Vorhergegangenen ist (i.e. Nachrichten desselben Tages oder derselben Woche).\n",
        "\n",
        "Kategorisieren von Nachrichten (mit Text2Topic, wie Salbrechter?) und eliminieren von Business/Strategic etc.\n",
        "Im Falle von Text2Topic, versuche Estimates des Unternehmens von Dritten zu unterscheiden.\n",
        "\n",
        "Wichtig!!! Unterscheide zwischen LERN-Phase und PRODUKTIONS-Phase.\n",
        "Wir können z.B. CLS-Token in der Produktions-Phase vergleichen, in der Lern-Phase aber noch nicht.\n",
        "\n",
        "Text2Vec -> Business category evtl. entfernen-> Intrastock variance average"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRABrPVOcaa9"
      },
      "source": [
        "## Nachrichten-Parsing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sBkgXJoMNGq"
      },
      "outputs": [],
      "source": [
        "ddf = dd.read_parquet(\"gcs://extreme-lore-398917-bzg/latest2/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUy_FfbYMNGq"
      },
      "outputs": [],
      "source": [
        "# TODO: Compare to ticker_name_mapper\n",
        "ticker_name_mapper_reduced = ddf[[\"stocks\", \"company_name\", \"short_name\"]].drop_duplicates(keep=\"first\").compute()\n",
        "ticker_name_mapper_reduced.to_parquet(\"data_shared/ticker_name_mapper_reduced.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqQBHH3QMNGr"
      },
      "source": [
        "### Beispiel/ Untersuchung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keF_8QNAkM36"
      },
      "outputs": [],
      "source": [
        "sample_partition = ddf.get_partition(20)\n",
        "y = sample_partition.head(5)\n",
        "y.loc[:, \"time\"] = pd.to_datetime(y.time).dt.tz_convert(\"UTC\")\n",
        "x = y.iloc[2]\n",
        "x.body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSAGuM_Acrwr"
      },
      "outputs": [],
      "source": [
        "res = client.submit(filter_body, row=x, logging=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LieS1minMNGs"
      },
      "outputs": [],
      "source": [
        "res.result()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj6mofzeqcqB"
      },
      "source": [
        "### Anwenden der filter_body-Funktion auf alle Reihen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tqzGy3bMNGs"
      },
      "outputs": [],
      "source": [
        "def handle_timezone(x):\n",
        "    try:\n",
        "        return x.tz_convert(\"US/Eastern\")\n",
        "    except Exception as e:\n",
        "        return x.tz_localize(\"US/Eastern\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qubEijBxMNGt"
      },
      "outputs": [],
      "source": [
        "# Still need to parse time correctly...\n",
        "ddf[\"time\"] = ddf[\"time\"].map(lambda x: handle_timezone(pd.to_datetime(x)), meta=pd.Series(dtype=\"datetime64[ns, US/Eastern]\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGlOJPEvFxuP"
      },
      "outputs": [],
      "source": [
        "par = ddf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gPPJDRKPv72"
      },
      "outputs": [],
      "source": [
        "par[\"parsed_body\"] = par.map_partitions(lambda y: y.apply(lambda x: filter_body(x),\n",
        "                                                axis=1),\n",
        "                                                meta=pd.Series(dtype=\"string\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IU0NTpsVMNGu"
      },
      "outputs": [],
      "source": [
        "par = client.persist(par)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vlCN3tKDfnw8"
      },
      "outputs": [],
      "source": [
        "# Do this only after having persisted, because this blocks the console making us unable to interact witth client/cluster\n",
        "# and can interfere with communication between client and scheduler.\n",
        "\n",
        "# VMManager needs to be redone... do os.system(kubectl delete dsk example), its a cleaner shutdown and also deletes nodes\n",
        "# with VMManager(cluster) as _:\n",
        "par.to_parquet(\"gcs://extreme-lore-398917-bzg/processed_news\",\n",
        "        storage_options={'token': token})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uPo1Ruo0N0q"
      },
      "outputs": [],
      "source": [
        "client # no connection, can we connect via ip??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6x3b8dPMNGv"
      },
      "outputs": [],
      "source": [
        "cluster # connection still here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihzi516YMNGv"
      },
      "source": [
        "## Filtern von Newstiteln"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdbc2yRqMNGv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1DIhhqZMNGv"
      },
      "source": [
        "## Voranstellen von gefilterten Newstiteln an Nachrichtenkörper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFEujkJ-MNGw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaK5pLz4mRHF"
      },
      "source": [
        "## Analyse der durschnittlichen Tokenlänge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y192tnIfpIXj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0bW6kYmpI1W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VG5rnSiNpWVN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mc6lpRCpfVT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}