{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "from dotmap import DotMap\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizerFast, get_linear_schedule_with_warmup\n",
    "from src.model.data_loading import create_dataloaders, get_text_and_labels\n",
    "\n",
    "from src.model.neural_network import (\n",
    "    TRANSFORMER_HF_ID,\n",
    "    MyBertModel,\n",
    "    WeightedSquaredLoss,\n",
    "    embed_inputs,\n",
    "    train,\n",
    ")\n",
    "\n",
    "config = DotMap(yaml.safe_load(open(\"src/config.yaml\")), _dynamic=False)\n",
    "\n",
    "FROM_SCRATCH = True\n",
    "batch_size = 4\n",
    "loss_confidence_parameter = 1 # Je höher, desto größer ist die Aussagekraft einer hohen Prognose\n",
    "\n",
    "input_col_name = config.model.input_col_name\n",
    "target_col_name = config.model.target_col_name\n",
    "\n",
    "\n",
    "# Download dataset\n",
    "dataset = pd.read_parquet(config.data.merged, columns=[input_col_name, target_col_name, \"section\"])\n",
    "train_texts, train_labels = get_text_and_labels(dataset, \"training\")\n",
    "test_texts, test_labels = get_text_and_labels(dataset, \"validation\")\n",
    "N_train = len(train_texts)\n",
    "print(f\"train_dat size: {N_train}\")\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(TRANSFORMER_HF_ID)\n",
    "train_inputs, train_masks = embed_inputs(train_texts, tokenizer)\n",
    "test_inputs, test_masks = embed_inputs(test_texts, tokenizer)\n",
    "\n",
    "train_dataloader = create_dataloaders(train_inputs, train_masks, \n",
    "                                      train_labels, batch_size)\n",
    "validation_dataloader = create_dataloaders(test_inputs, test_masks, \n",
    "                                     test_labels, batch_size)\n",
    "\n",
    "model: nn.Module = MyBertModel()\n",
    "if not FROM_SCRATCH: \n",
    "    model.load_state_dict(torch.load(\"data/model\")) # Use latest iteration of the model for training\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    if torch.cuda.is_available():       \n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using GPU.\")\n",
    "    else:\n",
    "        print(\"No GPU available, using the CPU instead.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Optimizer, scheduler and loss function\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
    "\n",
    "    epochs = 1\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps=0, \n",
    "                                                num_training_steps=total_steps)\n",
    "    loss_function = WeightedSquaredLoss(gamma=loss_confidence_parameter)\n",
    "\n",
    "    # Training\n",
    "    model, training_stats = train(model, \n",
    "                                  optimizer, \n",
    "                                  scheduler, \n",
    "                                  loss_function, \n",
    "                                  epochs, \n",
    "                                  train_dataloader, \n",
    "                                  validation_dataloader, \n",
    "                                  device, \n",
    "                                  clip_value=2,\n",
    "                                  N_train=N_train)\n",
    "\n",
    "    df_stats = pd.DataFrame(data=training_stats)\n",
    "    print(df_stats)\n",
    "\n",
    "    # Store Model\n",
    "    torch.save(model.state_dict(), \"data/model\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
