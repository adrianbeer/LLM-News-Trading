{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(\"gcs://extreme-lore-398917-bzg/latest2/\",\n",
    "                      storage_options={'token': token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove numbers\n",
    "docs = ddf.body.map(lambda x: re.sub(r'\\d+', '', x), meta=pd.Series(dtype=\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.name=\"body\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.topic_modeling.helpers import tokenize, keyword_filter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_list1 = [\"launch\", \"business\", \"strategy\", \"management\", \"product\", \"service\", \"app\", \"customer\", \"merge\"]\n",
    "keywords_list2 = [\"upgrade\", \"downgrade\", \"raise\", \"cut\", \"buy\", \"sell\", \"hold\", \"outperform\", \"underperform\", \"analyst\", \"estimate\"]\n",
    "keywords_list3 = [\"ebit\", \"eps\", \"earnings\", \"report\", \"financial\", \"quarter\", \"annual\", \"year\", \"ended\", \"net\", \"income\"]\n",
    "total_keywords = keywords_list1 + keywords_list2 + keywords_list3\n",
    "len(total_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization + rough filtering\n",
    "tfidf_docs = docs.map(lambda x: keyword_filter(tokenize(x), total_keywords), meta=pd.Series(dtype=\"object\"))\n",
    "tfidf_docs = tfidf_docs.reset_index().repartition(npartitions=5)\n",
    "tfidf_docs.columns = [\"index\", \"body\"]\n",
    "tfidf_docs.to_parquet(\"gcs://extreme-lore-398917-bzg/tfidf-tokens\",\n",
    "                                                    storage_options={'token': token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_docs = dd.read_parquet(\"gcs://extreme-lore-398917-bzg/tfidf-tokens/\",\n",
    "                                                    storage_options={'token': token},\n",
    "                                                    dtype_backend=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_docs = tfidf_docs.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy representation is without commas making us unable to convert strings to list via eval\n",
    "tfidf_docs.loc[:, \"body\"] = tfidf_docs.body.map(lambda x: x.replace(\" \", \", \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(tokens):\n",
    "    if len(tokens) == 0:\n",
    "        return np.array([\"wordtopreventemptydocumentswhencalculatingtfidf\"])\n",
    "    else:\n",
    "        return np.array(tokens)\n",
    "        \n",
    "tfidf_docs.loc[:, \"body\"] = tfidf_docs.body.map(lambda x: f(eval(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_docs.iloc[0].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_tfidfs_list = []\n",
    "for topic_tokenizer in [keywords1, keywords2, keywords3]:\n",
    "    lazy_result = dask.delayed(TfidfVectorizer(tokenizer=topic_tokenizer, lowercase=False).fit_transform)(tfidf_docs.body)\n",
    "    lazy_sums = dask.delayed(np.apply_along_axis)(np.sum, 1, lazy_result.todense())\n",
    "    topic_tfidfs_list.append(lazy_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_indicator_list = dask.compute(*topic_tfidfs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(topic_indicator_list)\n",
    "df=df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_docs.head().iloc[4].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Phrases\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the documents into tokens.\n",
    "lda_docs = docs.apply(lambda x: word_tokenize(str.lower(x)), meta=pd.Series(dtype=\"object\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove words that are only one character.\n",
    "# Lemmatize the documents.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lda_docs = lda_docs.apply(lambda x: [lemmatizer.lemmatize(token) for token in x if len(token) >1], meta=pd.Series(dtype=\"object\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "def f(doc):\n",
    "  doc = doc\n",
    "  for token in bigram[doc]:\n",
    "      if '_' in token:\n",
    "          # Token is a bigram, add to document.\n",
    "          doc.append(token)\n",
    "  return doc\n",
    "lda_docs = lda_docs.apply(lambda doc: f(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_docs.name = \"body\"\n",
    "# lda_docs = lda_docs.to_frame().reset_index()\n",
    "# lda_docs = lda_docs.reset_index().repartition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_docs.to_parquet(cwd + \"/data/lda_docs\", schema={\"body\": pa.string(),  \"index\":pa.int32()})\n",
    "# lda_docs = dd.read_parquet(cwd + \"/data/lda_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare and common tokens.\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=1000, no_above=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = model.top_topics(corpus)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
